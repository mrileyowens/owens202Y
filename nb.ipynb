{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8da6247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors: M. Riley Owens (GitHub: mrileyowens) and Jane R. Rigby (GitHub: janerigby)\n",
    "\n",
    "# This file is a total conversion of a previous file (Sunburst_NB_continuum_subtraction_v2.ipynb)\n",
    "# written by Jane R. Rigby, which created narrowband maps of multiple emission lines in the Sunburst Arc,\n",
    "# based upon various HST filter exposures. Unlike the previous file, the functionality of this file is \n",
    "# reduced and only creates Lya maps, and does not convolve copies of them to the F160W observations.\n",
    "# Contact M. Riley Owens (m.riley.owens@gmail.com) to request additional functionality for other emission \n",
    "# lines or to restore the convolution procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6755dd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas \n",
    "import stsynphot\n",
    "import synphot\n",
    "from synphot.models import Empirical1D\n",
    "import os\n",
    "import warnings\n",
    "import glob\n",
    "import copy\n",
    "import numpy as np\n",
    "from astropy.convolution import convolve\n",
    "from astropy.io import fits\n",
    "from astropy import units as u\n",
    "from astropy import wcs\n",
    "import extinction\n",
    "\n",
    "from regions import Regions\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pandas.options.display.width = 0\n",
    "pandas.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3c414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make():\n",
    "\n",
    "    # Establish common directories\n",
    "    home = os.getcwd()\n",
    "    data = f'{home}/data'\n",
    "    results = f'{home}/results'\n",
    "\n",
    "    # The filepaths of the SEDs fitted to complete images of the entire Sunburst Arc galaxy\n",
    "    SEDs = np.array([i for i in sorted(glob.glob(f'{data}/sed_fits/bestfit_model_image*_spec_filter6_cont.txt'))], dtype=str)\n",
    "\n",
    "    # Create a blank dictionary\n",
    "    df = {}\n",
    "    \n",
    "    # For each SED fit filepath\n",
    "    for i, sed in enumerate(SEDs) :\n",
    "\n",
    "        # Add the SED as a Pandas DataFrame to a dictionary with the filepath as its key\n",
    "        df[sed] = pandas.DataFrame(np.transpose(np.loadtxt(sed)), columns=('Observed wavelength (Ã…)', 'Flux density (maggies)'))\n",
    "\n",
    "    # ID of the SED to normalize the others against\n",
    "    image = 3\n",
    "\n",
    "    # Make an array that will store the normalization factors of the SEDs\n",
    "    normfactors = np.zeros(shape=len(SEDs))\n",
    "\n",
    "    # For each SED fit filepath\n",
    "    for i, sed in enumerate(SEDs):\n",
    "\n",
    "        # Determine the mean flux density (in Jy) between 1 and 1.05 microns\n",
    "        fluxhere = (df[sed]['Flux density (maggies)'].loc[df[sed]['Observed wavelength (Ã…)'].between(1E4,1.05E4)]).mean() * 3631   \n",
    "        normfactors[i] =  fluxhere\n",
    "\n",
    "    # The scaling factor of each SED\n",
    "    scaleby = normfactors / normfactors[image - 1]\n",
    "\n",
    "    # Make an empty list that we will append spectra to\n",
    "    arcspec = []\n",
    "\n",
    "    # For each SED fit filepath\n",
    "    for i, sed in enumerate(SEDs) :\n",
    "\n",
    "        # Get the observed wavelength (Ã…) and flux density (maggies)\n",
    "        w, f =  np.loadtxt(sed)\n",
    "\n",
    "        # Convert the flux density to Janskys and scale the spectrum to match the given complete image.\n",
    "        # This accounts for the different magnifications of the complete images. What is the purpose of\n",
    "        # making the units Janskys?\n",
    "        f_Jy = f * 3631 / scaleby[i]\n",
    "\n",
    "        # Determine the Milky Way extinction according to Cardelli et al. (1989),\n",
    "        # where E(B-V) = 0.09427 and R_v = 3.1. Also need to confirm how the E(B-V) \n",
    "        # is calculated. Original notebook states it follows Green et al. (2015) \n",
    "        # (ApJ, 810, 25), in mage_calc_MW_reddening.py.\n",
    "        MW_extinction = extinction.ccm89(w, -1 * 3.1 * 0.09427, 3.1, unit='aa')\n",
    "\n",
    "        # Deredden the flux densities\n",
    "        f_Jy = f_Jy * 10**(-0.4 * MW_extinction)\n",
    "\n",
    "        # Instantiate SourceSpectrum objects in synphot of the scaled and unscaled data and append them to separate lists\n",
    "        arcspec.append(synphot.SourceSpectrum(Empirical1D, points=w * u.Angstrom, lookup_table=f_Jy * u.Jy))\n",
    "\n",
    "\n",
    "    # Dictionary of the mean redshifts of the composite spectra of each stacked spectrum\n",
    "    z = {\n",
    "        'nonleaker' : 2.37060,\n",
    "        'leaker'  : 2.37029\n",
    "    }\n",
    "\n",
    "    # Dictionary of rest wavelength segments (angstroms) free from prominent lines\n",
    "    segments = np.array([\n",
    "        [1250, 1260],\n",
    "        [1265, 1275],\n",
    "        [1310, 1320],\n",
    "        [1350, 1360],\n",
    "        [1420, 1430],\n",
    "        [1450, 1460],\n",
    "        [1470, 1480],\n",
    "        [1490, 1500],\n",
    "        [1510, 1520],\n",
    "        [1560, 1630],\n",
    "        [1680, 1715],\n",
    "        [1720, 1740],\n",
    "        [1760, 1810],\n",
    "        [1820, 1850],\n",
    "        [1970, 2000]\n",
    "    ])\n",
    "\n",
    "    # For the identifying label of each stacked MagE spectrum\n",
    "    for i, label in enumerate(['nonleaker', 'leaker']):\n",
    "\n",
    "        # Get the file path\n",
    "        file = glob.glob(f'{data}/mage/sunburst_arc_{label}_stack_mage.txt')[0]\n",
    "\n",
    "        # Get the wavelength bins and flux densities of the stacked spectrum\n",
    "        w, f = np.loadtxt(file, delimiter='\\t', comments='#', usecols=(0,1), unpack=True)\n",
    "\n",
    "        # Determine the dispersion of the wavelength bins in the rest frame\n",
    "        dispersion_rest = np.absolute(np.diff(w))\n",
    "\n",
    "        # Compute the boxcar kernel that will convolve the data\n",
    "        boxcar = int(np.ceil(100 / np.median(dispersion_rest)) // 2 * 2 + 1)\n",
    "\n",
    "        f_masked = f\n",
    "\n",
    "        # For each clean wavelength segment\n",
    "        for j, segment in enumerate(segments):\n",
    "\n",
    "            # Mask non-clean wavelength segments with NaNs\n",
    "            f_masked = np.where((w >= segment[0]) & (w <= segment[1]), f_masked, np.nan)\n",
    "\n",
    "        # Convolve the data with the boxcar kernel\n",
    "        c = convolve(f_masked, np.ones((boxcar,)) / boxcar, boundary='fill', fill_value=np.nan)\n",
    "\n",
    "        # Place the data in the observed frame\n",
    "        w = w * (1 + z[label])\n",
    "        f = f / (1 + z[label])\n",
    "\n",
    "        # Replace Lya with the boxcar-smoothed estimate of the continuum\n",
    "        f = np.where((w >= 4080) & (w <= 4125), c, f)\n",
    "        \n",
    "        # Replace any remaining NaNs with zeros\n",
    "        f = np.where(np.isnan(f), 0, f)\n",
    "\n",
    "        # Append the MagE spectra as synphot SourceSpectrum objects to the existing list of SourceSpectrum objects from\n",
    "        # the SED fits\n",
    "        arcspec.append(synphot.SourceSpectrum(Empirical1D, points=w * u.Angstrom, lookup_table=f * synphot.units.FLAM))\n",
    "\n",
    "    # Make arrays of the filter information of the on- and off-band filters for the continuum\n",
    "    # subtraction. These strings are in a specific format that synphot will recognize and use\n",
    "    # to get instrumental information.\n",
    "    on_setups  = np.array(['wfc3,uvis1,f410m'], dtype=str)\n",
    "    off_setups = np.array(['wfc3,uvis1,f390w', 'wfc3,uvis1,f555w'], dtype=str)\n",
    "    \n",
    "    # Combine the two arrays of on- and off-band setups\n",
    "    setups = np.append(on_setups, off_setups)\n",
    "\n",
    "    # Make containers to store the results (consider changing variable name to n)\n",
    "    n_spec = len(arcspec)# + len(magelabels)\n",
    "\n",
    "    # The filters for Lya get 6 spectra (4 Gourav's + MagE leaker + Mage non-leaker)\n",
    "    # The other filters get 4 spectra (Gourav's fits to each complete image)\n",
    "    scaleF390W_for_Lya = np.empty(n_spec)  # Scale factor for continuum filter, before subtracting from filter w emiss line\n",
    "    scaleF555W_for_Lya = np.empty(n_spec)  # Try F555W as cont filter instead of F390W\n",
    "    contF410M = np.empty(n_spec)   # Calculated continuum in the filter containing the emission line\n",
    "    contF410M_alt = np.empty(n_spec)   # Try F555W as cont filter instead of F390W\n",
    "\n",
    "    # Treat Lya differently than the other filters, bc we have MagE spectra, and b/c of addnl cont uncertainty from Lya Forest\n",
    "    allspec = copy.deepcopy(arcspec)\n",
    "\n",
    "    # What is this meant to do?\n",
    "    if len(allspec) != 6 : raise Exception(\"Copy of list of files was not deep\")\n",
    "\n",
    "    # For each synphot Source Spectrum object of the spectra\n",
    "    for i, thisspec in enumerate(allspec):\n",
    "\n",
    "        # Create a dictionary that will contain the count rates for each filter\n",
    "        countrate = {}\n",
    "\n",
    "        # For each filter\n",
    "        for setup in setups :  # only F410M, F390W, F555W needed here\n",
    "\n",
    "            # Get the bandpass and pass the spectrum through it as a synphot Observation object\n",
    "            bp = stsynphot.band(setup)\n",
    "            obs = synphot.Observation(thisspec, bp, force='taper')\n",
    "\n",
    "            # Calculate the count rate measured from the filter-transmitted spectrum\n",
    "            countrate[setup] = obs.countrate(stsynphot.conf.area).value\n",
    "\n",
    "        # Compute the scaling factor for the F390W continuum estimate\n",
    "        scaleF390W_for_Lya[i] = countrate['wfc3,uvis1,f410m'] / countrate['wfc3,uvis1,f390w']  # continuum for Lya\n",
    "        \n",
    "        # Why is the F410M continuum just the F410M count rate?\n",
    "        contF410M[i] = scaleF390W_for_Lya[i] * countrate['wfc3,uvis1,f390w']\n",
    "        \n",
    "        # Compute the scaling factor for the F555W continuum estimate\n",
    "        scaleF555W_for_Lya[i] = countrate['wfc3,uvis1,f410m'] / countrate['wfc3,uvis1,f555w']  # alt continuum for Lya\n",
    "\n",
    "        # Why is the F410M continuum just the F410M count rate?\n",
    "        contF410M_alt[i] = scaleF555W_for_Lya[i] * countrate['wfc3,uvis1,f555w']\n",
    "\n",
    "    # Make arrays of the scale factors measured from each background treatment, \n",
    "    # as well as the corresponding means and standard deviations\n",
    "    scalefactors = np.array([scaleF390W_for_Lya, scaleF555W_for_Lya])\n",
    "    cont_scale_mean = np.array([np.mean(x) for x in scalefactors])\n",
    "    cont_scale_std  = np.array([np.std(x) for x in scalefactors])\n",
    "\n",
    "    # We need to subtract the sky background from the HST images, so let's get a\n",
    "    # Region object from a .reg file representing a patch of the sky that is relatively\n",
    "    # empty.\n",
    "    region = Regions.read(f'{data}/box_for_median_imcoords_v4.reg', format='ds9')[0]\n",
    "\n",
    "    # For convenience, get the headers of the v4 and v5 data. Since the WCS should be the same across all data of the same\n",
    "    # version, it shouldn't (?) matter which filter we pick from either version, so just grab what we have laying around.\n",
    "    v4_header = fits.getheader(f'{data}/hst/V4.0_PSZ1G311.65-18.48_F275W_0.03g0.8_cr2.5_0.7_drc_sci.fits')\n",
    "    v5_header = fits.getheader(f'{data}/hst/V5.0_PSZ1G311.65-18.48_F410M_0.03g0.6_crsc1.2_0.7crsn3.5_3.0_drc_sci.fits')\n",
    "\n",
    "    # The .reg file was originally created (by Jane Rigby, I presume?) from the pixel coordinates of the v4 reduction of the Sunburst \n",
    "    # Arc HST data. Since then, v5 is now available and should be used instead of v4. Since v5 has a slightly different WCS and the \n",
    "    # .reg file is built from pixel coordinates of the old WCS, we need to convert it to sky coordinates based on the WCS of the v4\n",
    "    # data.\n",
    "    sky_region = region.to_sky(wcs.WCS(v4_header))\n",
    "\n",
    "    # For convenience for future users, make a copy of the region file in sky coordinates.\n",
    "    # I am dropping the 'vX' suffix since this file is not built from pixel coordinates of\n",
    "    # a certion reduction version of data.\n",
    "    sky_region.write(f'{home}/results/box_for_median_imcoords.reg', format='ds9', overwrite=True)\n",
    "\n",
    "    # Now get the region as pixel coordinates in the v5 WCS. We'll use this to create a mask of the region.\n",
    "    pixel_region = sky_region.to_pixel(wcs.WCS(v5_header))\n",
    "\n",
    "    # Make a mask of the region and save it\n",
    "    mask = pixel_region.to_mask().to_image((v5_header['NAXIS2'], v5_header['NAXIS1']))\n",
    "    fits.writeto(f'{home}/results/masks/box_for_median_imcoords_mask_v5.fits', mask.data, header=v5_header, overwrite=True)\n",
    "\n",
    "    # For each filter used as a continuum estimate\n",
    "    for i, filter in enumerate(['F390W', 'F555W']):\n",
    "\n",
    "        # Get the data and headers of the on- and off-band filters\n",
    "        data_off, header_off = fits.getdata(f'{data}/hst/V5.0_PSZ1G311.65-18.48_{filter}_0.03g0.6_crsc1.2_0.7crsn3.5_3.0_drc_sci.fits', header=True)\n",
    "        data_on, header_on   = fits.getdata(f'{data}/hst/V5.0_PSZ1G311.65-18.48_F410M_0.03g0.6_crsc1.2_0.7crsn3.5_3.0_drc_sci.fits', header=True)\n",
    "\n",
    "        # Compute the median values of the masked sky patch\n",
    "        med_val_off = np.nanmedian(data_off * mask.data)\n",
    "        med_val_on = np.nanmedian(data_on * mask.data)\n",
    "\n",
    "        # Continuum subtract the on-band data with the off-band continuum estimate\n",
    "        scale_this_offband = cont_scale_mean[i]\n",
    "        data_cont = (data_off - med_val_off) * scale_this_offband  # remove median sky bkg\n",
    "        data_cont_sub = (data_on - med_val_on) - data_cont # remove median sky bkg\n",
    "        cont_math = f'F410M - {filter} * (CNTSUBT)'\n",
    "\n",
    "        # Propagate the uncertainty to the final continuum-subtracted image\n",
    "        n = np.sqrt((np.nanstd(data_on * mask.data))**2 +\n",
    "            (np.nanstd(data_on * mask.data))**2 + \n",
    "            ( (data_off - med_val_off) * cont_scale_mean[i] * np.sqrt( (np.sqrt((np.nanstd(data_off * mask.data))**2 + (np.nanstd(data_off * mask.data))**2) / (data_off - med_val_off))**2 + (cont_scale_std[i] / cont_scale_mean[i])**2 ) )**2\n",
    "        )\n",
    "\n",
    "        # Initiate headers for the two extensions of the new file from the WCS\n",
    "        # it will use. We will then build on the headers from there.\n",
    "        header_f = wcs.WCS(header_on).to_header()\n",
    "        header_n = wcs.WCS(header_on).to_header()\n",
    "\n",
    "        # Construct the CD matrix of the WCS system from the on-band header\n",
    "        cd_matrix = wcs.WCS(header_on).wcs.cd\n",
    "\n",
    "        # For the headers of the data and uncertainty extensions\n",
    "        for j, header in enumerate([header_f, header_n]):\n",
    "\n",
    "            # Set the CD matrix keywords in the new headers of the saved Lya maps\n",
    "            header['CD1_1'] = cd_matrix[0,0]\n",
    "            header['CD1_2'] = cd_matrix[0,1]\n",
    "            header['CD2_1'] = cd_matrix[1,0]\n",
    "            header['CD2_2'] = cd_matrix[1,1]\n",
    "\n",
    "            # Add various informational keywords to the header\n",
    "            header.insert(1, ('CREATOR', 'M. Riley Owens', 'Creator of this file'))\n",
    "            header.insert(2, ('CREATOR_EMAIL', 'm.riley.owens@gmail.com', 'Email of the creator of this file'))\n",
    "            header['GENFILE'] = ('nb.ipynb', 'The file used to create this file')\n",
    "            header['FILENAME'] = (f'Lya_cont_sub_{filter}.fits', 'Name of file')\n",
    "            header['ON_FILE'] = ('V5.0_PSZ1G311.65-18.48_F410M_0.03g0.6_crsc1.2_0.7crsn3.5_3.0_drc_sci.fits', 'The name of the on-band file')\n",
    "            header['OFF_FILE'] = (f'V5.0_PSZ1G311.65-18.48_{filter}_0.03g0.6_crsc1.2_0.7crsn3.5_3.0_drc_sci.fits', 'The name of the off-band file')\n",
    "            header['CNTMATH'] = (f'F410M - {filter} * SCALE', 'Formula used to calculate the Lyman-alpha emission in the narrowband image')\n",
    "            header['SCALE'] = (cont_scale_mean[i], 'The mean continuum scaling factor, as inferred from the distribution of scaling factors derived from the 4 SED fits and 2 stacked MagE spectra')\n",
    "            header['SCALEU'] = (cont_scale_std[i], 'The standard deviation of the continuum scaling factor, as inferred from the distribution of scaling factors derived from the 4 SED fits and 2 stacked MagE spectra')\n",
    "            \n",
    "            # For each keyword in an array of useful keywords that the observations share\n",
    "            for k, keyword in enumerate(np.array(['FILETYPE', 'TELESCOP', 'INSTRUME', 'EQUINOX', 'IMAGETYP',\n",
    "                'PRIMESI', 'OBSTYPE', 'BUNIT', ], dtype=str)):\n",
    "\n",
    "                # Add the keyword to the header\n",
    "                header[keyword] = (header_on[keyword], header_on.comments[keyword])\n",
    "\n",
    "        # Create a HDU list from the data and uncertainties and the constructed headers\n",
    "        hdu_f = fits.PrimaryHDU(data_cont_sub, header_f, 'DATA')\n",
    "        hdu_n = fits.ImageHDU(n, header_n, 'UNCERTAINTY')\n",
    "        hdul = fits.HDUList([hdu_f, hdu_n])\n",
    "\n",
    "        # Save the Lya map\n",
    "        hdul.writeto(f'{results}/lya_maps/Lya_cont_sub_{filter}.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7314114",
   "metadata": {},
   "outputs": [],
   "source": [
    "make()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
