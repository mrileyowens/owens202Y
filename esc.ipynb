{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51994da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: M. Riley Owens (GitHub: mrileyowens)\n",
    "\n",
    "# This file computes LyC escape fractions of the MagE spectra \n",
    "# according to Rivera-Thorsen et al. (2019) (arXiv:1904.08186)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9173d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import sigfig\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.convolution import convolve_fft, Gaussian2DKernel\n",
    "\n",
    "from astropy.stats import SigmaClip\n",
    "from photutils.background import Background2D, MedianBackground, SExtractorBackground, ModeEstimatorBackground, MeanBackground, MMMBackground, BiweightLocationBackground\n",
    "\n",
    "from reproject import reproject_interp\n",
    "\n",
    "import stsynphot as STS\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70947271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure():\n",
    "\n",
    "    '''\n",
    "    Measure the LyC escape fractions of the MagE apertures\n",
    "    '''\n",
    "\n",
    "    def convolve_uncertainties(uncertainty, stdv, shape):\n",
    "\n",
    "        '''\n",
    "        Convolve the uncertainties of an image\n",
    "\n",
    "        Parameters:\n",
    "            uncertainty : numpy.float64\n",
    "                The estimated uncertainty of the pixels in the image\n",
    "            stdv : numpy.float64\n",
    "                The standard deviation of the Gaussian convolution kernel in pixels\n",
    "            shape : tuple\n",
    "                The dimensions of the image\n",
    "\n",
    "        Returns:\n",
    "            uncertainty_convolved : numpy.ndarray\n",
    "                The convolved estimated uncertainty of the pixels in the image, \n",
    "                broadcasted to the shape of the image\n",
    "        '''\n",
    "    \n",
    "        # Set the kernel size to 3 standard deviations of the \n",
    "        # width of the time-averaged seeing conditions\n",
    "        kernel_size = int(3 * stdv)\n",
    "\n",
    "        # Make a dummy array of the convolved uncertainties with the same shape as the image\n",
    "        uncertainty_convolved = np.ones(shape)\n",
    "\n",
    "        # Set the squared sum of the weighted uncertainties to zero, to be added to in the double loop below\n",
    "        squared_sum = 0.0\n",
    "\n",
    "        # The double loop below calculates the convolved uncertainty of the pixels in the image. \n",
    "        # It relies on some assumptions. First, since the original uncertainty is assumed true \n",
    "        # for the entire image, the loop does not calculate the convolved uncertainty at each \n",
    "        # pixel in the image. It just calculates one instance (since they will all be the same, \n",
    "        # ignoring edge effects), and then broadcasts that value to the entire image\n",
    "\n",
    "        # For each pixel within the kernel size\n",
    "        for i in range(-kernel_size, kernel_size + 1):\n",
    "            for j in range(-kernel_size, kernel_size + 1):\n",
    "\n",
    "                # Calculate the weight of the pixel\n",
    "                weight = np.exp(-(i**2 + j**2) / (2 * stdv**2))\n",
    "                weight /= 2 * np.pi * stdv**2\n",
    "\n",
    "                # Add the weighted uncertainty to the total squared sum of uncertainties\n",
    "                squared_sum += (weight * uncertainty)**2\n",
    "\n",
    "        # Multiply the dummy array of the convolved uncertainties by the convolved uncertainty\n",
    "        uncertainty_convolved = uncertainty_convolved * np.sqrt(squared_sum)\n",
    "\n",
    "        return uncertainty_convolved\n",
    "\n",
    "    # Dictionary of the slit IDs, containing, from left to right: the \n",
    "    # file name of the data, redshift, and FWHM of the time-averaged \n",
    "    # seeing conditions of the observation\n",
    "    slits = {\n",
    "        'M5' : ['sunburst_M-5-comb1_MWdr.txt', 2.37086, 0.97],\n",
    "        'M4' : ['sunburst_M-4-comb1_MWdr.txt', 2.37073, 0.71],\n",
    "        'M6' : ['sunburst_M-6-comb1_MWdr.txt', 2.37021, 0.76],\n",
    "        'M3' : ['sunburst_M-3-comb1_MWdr.txt', 2.37025, 0.70],\n",
    "        'M0' : ['sunburst_M-0-comb1_MWdr.txt', 2.37014, 1.34],\n",
    "        'M2' : ['sunburst_M-2-comb1_MWdr.txt', 2.37017, 0.77],\n",
    "        'M7' : ['sunburst_M-7-comb1_MWdr.txt', 2.37044, 0.73],\n",
    "        'M8' : ['sunburst_M-8-comb1_MWdr.txt', 2.37024, 0.7],\n",
    "        'M9' : ['sunburst_M-9-comb1_MWdr.txt', 2.37030, 0.68]\n",
    "    }\n",
    "\n",
    "    # Establish common directories\n",
    "    home = os.getcwd()\n",
    "    data = f'{home}/data'\n",
    "    results = f'{home}/results'\n",
    "\n",
    "    # The IGM transmission according to Rivera-Thorsen et al. (2019) (Science, 366, 738)\n",
    "    t_IGM = 0.69\n",
    "\n",
    "    # File paths to masks of the arc and MagE apertures\n",
    "    arc_mask_file = f'{results}/arc_mask_v5.fits'\n",
    "\n",
    "    # The v5 reductions of the HST imaging in the F275W and F814W filters, with\n",
    "    # an intervening galaxy removed by Alex Navarre with GALFIT, and cut to a \n",
    "    # smaller footprint containing just the two largest arcs\n",
    "    f275w_file = f'{data}/hst/galfit_cutouts/f275w_result_residual_v5_wcs.fits'\n",
    "    f814w_file = f'{data}/hst/galfit_cutouts/f814w_result_residual_v5_wcs.fits'\n",
    "\n",
    "    # Retrieve the data from the .fits files of the filters \n",
    "    f275w = fits.getdata(f275w_file)\n",
    "    f814w = fits.getdata(f814w_file)\n",
    "\n",
    "    # The GALFIT outputs are in units of total counts, so divide the data by the\n",
    "    # total exposure time listed in the .fits header (which is the quantity\n",
    "    # Alex used to rescale the data into units of total counts). For the F275W\n",
    "    # imaging, this does not seem to be the same quantity as the total \n",
    "    # exposure time that Mike listed in the HST observation log of the paper.\n",
    "    # I don't know if this was a clerical error made by Mike when tabulating the\n",
    "    # exposure times actually used, or if the exposure time in the header is\n",
    "    # the incorrect source. Either way, I don't think it should make a difference\n",
    "    # when computing the LyC escape fraction as long as I use the scaling factor\n",
    "    # that Alex did.\n",
    "    f275w = f275w / 87733\n",
    "    f814w = f814w / 5280\n",
    "\n",
    "    # Set the sigma clipping properties\n",
    "    sigma_clip = SigmaClip(sigma=3.0, maxiters=10)\n",
    "\n",
    "    # Set the background estimation method\n",
    "    bkg_estimator = MedianBackground()\n",
    "\n",
    "    # Create masks of sources in the images\n",
    "    f275w_source_mask = (f275w >= np.abs(np.median(f275w)))\n",
    "    f814w_source_mask = (f814w >= 8 * np.abs(np.median(f814w)))\n",
    "    \n",
    "    # Create background models of the two images\n",
    "    f275w_background = Background2D(f275w, (15, 15), filter_size=(3,3), sigma_clip=sigma_clip, \n",
    "        bkg_estimator=bkg_estimator, mask=f275w_source_mask, exclude_percentile=80.0, fill_value=0.0)\n",
    "    f814w_background = Background2D(f814w, (20, 20), filter_size=(3,3), sigma_clip=sigma_clip, \n",
    "        bkg_estimator=bkg_estimator, mask=f814w_source_mask, exclude_percentile=20.0, fill_value=0.0)\n",
    "\n",
    "    # Create a figure that will show the background-subtracted HST images\n",
    "    fig, ax = plt.subplots(1,2, sharex=True, sharey=True, figsize=(20,10))\n",
    "\n",
    "    # Plot the background-subtracted HST images, using the source mask to remove sources\n",
    "    ax[0].imshow(np.ma.masked_where(f275w_source_mask, f275w - f275w_background.background), origin='lower', cmap='viridis', vmin=-0.000001, vmax=0.000001)\n",
    "    ax[1].imshow(np.ma.masked_where(f814w_source_mask, f814w - f814w_background.background), origin='lower', cmap='viridis', vmin=-0.001, vmax=0.001)\n",
    "\n",
    "    # Reassign NaNs in the data as zeros\n",
    "    f275w = np.where(np.isnan(f275w), 0.0, f275w)\n",
    "    f814w = np.where(np.isnan(f814w), 0.0, f814w)\n",
    "\n",
    "    # Convert background-subtracted HST images from counts/s to erg/s/cm^2/Å\n",
    "    f275w = (f275w - f275w_background.background) * fits.open(f'{data}/hst/V5.0_PSZ1G311.65-18.48_F275W_0.03g0.6_crsc1.2_0.7crsn3.5_3.0_drc_sci.fits')[0].header['PHOTFLAM']\n",
    "    f814w = (f814w - f814w_background.background) * fits.open(f'{data}/hst/V5.0_PSZ1G311.65-18.48_F814W_0.03g0.6_crsc1.2_0.7crsn3.5_3.0_drc_sci.fits')[0].header['PHOTFLAM']\n",
    "\n",
    "    # Retrieve transmission curve data for the HST/WFC3 F275W and HST/ACS F814W filters\n",
    "    bp_f275w = STS.band('wfc3,uvis1,f275w,mjd#58216')\n",
    "    bp_f814w = STS.band('acs,wfc1,f814w,mjd#58170')\n",
    "    w_f275w = bp_f275w.binset\n",
    "    w_f814w = bp_f814w.binset\n",
    "    tpt_f275w = bp_f275w(w_f275w)\n",
    "    tpt_f814w = bp_f814w(w_f814w)\n",
    "    w_f275w = w_f275w.value\n",
    "    w_f814w = w_f814w.value\n",
    "\n",
    "    # Integrate the throughput curves of the HST filters\n",
    "    int_tpt_f275w = np.trapz(tpt_f275w, x=w_f275w)\n",
    "    int_tpt_f814w = np.trapz(tpt_f814w, x=w_f814w)\n",
    "\n",
    "    # Reproject the mask of the arc to the footprint of the image cutouts, since the original arc mask\n",
    "    # has a much larger footprint\n",
    "    arc_mask, _ = reproject_interp(fits.open(arc_mask_file)[0], fits.getheader(f275w_file), hdu_in=0)\n",
    "\n",
    "    # Convert the arc mask to binary values\n",
    "    arc_mask = np.where(arc_mask > 0, 1, 0)\n",
    "\n",
    "    # Make empty lists to append the measurement results to\n",
    "    f275w_fluxes, n_f275w_fluxes, f814w_fluxes, n_f814w_fluxes, f_escs, n_escs = [], [], [], [], [], []\n",
    "\n",
    "    # For each slit ID\n",
    "    for i, slit_id in enumerate(slits):\n",
    "    \n",
    "        # Get the file name of the MagE data, redshift, and FWHM of the seeing conditions\n",
    "        file, z, fwhm = slits[slit_id][0], slits[slit_id][1], slits[slit_id][2]\n",
    "        \n",
    "        # Reproject the slit mask to the cutout of the HST image, which is much smaller than the footprint of the image with the slit mask\n",
    "        slit_mask, _ = reproject_interp(fits.open(f'{results}/masks/{slit_id}_mask_v5.fits')[0], fits.getheader(f275w_file))\n",
    "        \n",
    "        # Convert the slit mask to binary\n",
    "        slit_mask = np.where(slit_mask > 0, 1, 0)\n",
    "    \n",
    "        # Compute the intersection of the arc and slit masks; preparing both masks in binary format\n",
    "        # preserves the binary structure in the intersecting mask\n",
    "        intr = arc_mask * slit_mask\n",
    "\n",
    "        # Estimate the uncertainties in the two images as the standard deviation of the pixels inside\n",
    "        # the slit ID's aperture mask but outside the mask of the arc\n",
    "        n_f275w = np.nanstd(np.where((slit_mask > 0) & (arc_mask < 1), f275w, np.nan))\n",
    "        n_f814w = np.nanstd(np.where((slit_mask > 0) & (arc_mask < 1), f814w, np.nan))\n",
    "\n",
    "        # Convert the (assumed) Gaussian FWHM of the time-averaged seeing conditions, expressed \n",
    "        # in pixels (in this drizzling, 1 pixel is 0.03 arcseconds), to a standard deviation\n",
    "        stdv = (fwhm / (2 * np.sqrt(2 * np.log(2)))) / 0.03\n",
    "    \n",
    "        # Create a symmetric Gaussian kernel from the standard deviation of the seeing conditions\n",
    "        kernel = Gaussian2DKernel(x_stddev=stdv, y_stddev=stdv)\n",
    "\n",
    "        # Convolve the F275W and F814W images with the Gaussian kernel using the fast Fourier \n",
    "        # transform method. This saves significant time because of the size of the images\n",
    "        f275w_convolved = convolve_fft(f275w, kernel)\n",
    "        f814w_convolved = convolve_fft(f814w, kernel)\n",
    "\n",
    "        # Convolve the uncertainties, assuming the uncertainty of each pixel \n",
    "        # is independent and normally distributed\n",
    "        n_f275w = convolve_uncertainties(n_f275w, stdv, np.shape(f275w))\n",
    "        n_f814w = convolve_uncertainties(n_f814w, stdv, np.shape(f814w))\n",
    "\n",
    "        # Calculate the total flux of the masked slit aperture in each filter\n",
    "        f275w_flux = np.sum(intr * f275w_convolved * int_tpt_f275w)\n",
    "        f814w_flux = np.sum(intr * f814w_convolved * int_tpt_f814w)\n",
    "\n",
    "        # Add the measured fluxes to the corresponding result lists\n",
    "        f275w_fluxes.append(f275w_flux)\n",
    "        f814w_fluxes.append(f814w_flux)\n",
    "\n",
    "        # Calculate the propagated uncertainty of the flux measurements\n",
    "        n_f275w_flux = (abs(f275w_flux) / np.sum(intr * f275w_convolved)) * np.sqrt(np.sum((abs(intr * f275w_convolved) * n_f275w / f275w_convolved)**2))\n",
    "        n_f814w_flux = (abs(f814w_flux) / np.sum(intr * f814w_convolved)) * np.sqrt(np.sum((abs(intr * f814w_convolved) * n_f814w / f814w_convolved)**2))\n",
    "\n",
    "        # Add the measured flux uncertainties to the corresponding result lists\n",
    "        n_f275w_fluxes.append(n_f275w_flux)\n",
    "        n_f814w_fluxes.append(n_f814w_flux)\n",
    "\n",
    "        # Get the Starburst99 fit's wavelength bins and flux densities\n",
    "        w_sb99, f_sb99 = np.loadtxt(f'{data}/mage/stellar_continuum_fits/planckarc-{slit_id}-sb99-fit.txt', comments='#', usecols=(0,3), unpack=True)\n",
    "\n",
    "        # Get the wavelength bins and flux densities and associated uncertainties of the MagE spectrum\n",
    "        w, f, n = np.loadtxt(f'{data}/mage/{file}', delimiter='\\t', comments=('#','w'), usecols=(0,1,2), unpack=True)\n",
    "    \n",
    "        # Create a mask excluding extreme flux density outliers\n",
    "        f_mask = f < 1e-20    \n",
    "\n",
    "        # Apply the mask to the data to remove data points with extreme\n",
    "        # flux density outliers\n",
    "        w, f, n = w[f_mask], f[f_mask], n[f_mask]\n",
    "\n",
    "        # Convert the flux densities and associated uncertainties to \n",
    "        # wavelength space (from erg/s/cm^2/Hz to erg/s/cm^2/Å)\n",
    "        f, n = f * 2.998e18 / np.square(w), n * 2.998e18 / np.square(w)\n",
    "    \n",
    "        # Convert the MagE spectrum data to the rest frame\n",
    "        w, f, n = w / (1 + z), f * (1 + z), n * (1 + z)\n",
    "    \n",
    "        # Scale the Starburst99 fit to match the MagE spectrum (the Starburst99 fits were \n",
    "        # normalized by the median flux density between 1267-1276 Å in the rest frame)\n",
    "        f_sb99 = f_sb99 * np.median(f[(w >= 1267) & (w <= 1276)])\n",
    "\n",
    "        # Calculate the uncertainty of the Starburst99 flux densities, \n",
    "        # conservatively assuming a constant 10% uncertainty\n",
    "        n_sb99 = 0.1 * f_sb99\n",
    "    \n",
    "        # Convert the Starburst99 fit to the observed frame\n",
    "        w_sb99, f_sb99, n_sb99 = w_sb99 * (1 + z), f_sb99 / (1 + z), n_sb99 / (1 + z)\n",
    "\n",
    "        # Create masks of the wavelength bounds of the two HST filters\n",
    "        w_sb99_f275w_mask = (w_sb99 >= 2100) & (w_sb99 <= 3250)\n",
    "        w_sb99_f814w_mask = (w_sb99 >= 6500) & (w_sb99 <= 10000)\n",
    "\n",
    "        # Mask the Starburst99 fits to the wavelength bounds of the two HST filters\n",
    "        w_sb99_f275w, f_sb99_f275w, n_sb99_f275w = w_sb99[w_sb99_f275w_mask], f_sb99[w_sb99_f275w_mask], n_sb99[w_sb99_f275w_mask]\n",
    "        w_sb99_f814w, f_sb99_f814w, n_sb99_f814w = w_sb99[w_sb99_f814w_mask], f_sb99[w_sb99_f814w_mask], n_sb99[w_sb99_f814w_mask]\n",
    "    \n",
    "        # Interpolate the throughput curves of the HST filters to the Starburst99 observed-frame wavelength bins\n",
    "        interpT275 = interp1d(w_f275w, tpt_f275w)\n",
    "        tpt275interp = interpT275(w_sb99_f275w)\n",
    "        interpT814 = interp1d(w_f814w, tpt_f814w)\n",
    "        tpt814interp = interpT814(w_sb99_f814w)\n",
    "    \n",
    "        # Integrate the Starburst99 flux densities as transmitted through the filters\n",
    "        f_sb99_f275w_trnsmtd = np.trapz(tpt275interp * f_sb99_f275w, x=w_sb99_f275w)\n",
    "        f_sb99_f814w_trnsmtd = np.trapz(tpt814interp * f_sb99_f814w, x=w_sb99_f814w)\n",
    "    \n",
    "        # Make an empty list that will contain the uncertainty of each trapezoidal area of the \n",
    "        # integration of the transmitted, F275W-masked Starburst99 fit\n",
    "        n_areas = []\n",
    "    \n",
    "        # For each wavelength bin in the Starburst99 fit masked by the F275W filter wavelengths\n",
    "        for j, wav in enumerate(w_sb99_f275w[:-1]):\n",
    "        \n",
    "            # Calculate the width of the wavelength bin\n",
    "            w_bin_width = w_sb99_f275w[j+1] - w_sb99_f275w[j]\n",
    "        \n",
    "            # Following the trapezoidal interpretation of integration, determine the area and \n",
    "            # associated uncertainty of the trapezoid starting at the jth wavelength bin \n",
    "            n_area = abs((w_bin_width * (tpt275interp[j] * f_sb99_f275w[j] + tpt275interp[j+1] * f_sb99_f275w[j+1]) / 2.0)) / (tpt275interp[j] * f_sb99_f275w[j] + tpt275interp[j+1] * f_sb99_f275w[j+1]) \\\n",
    "                * np.sqrt((abs(tpt275interp[j] * f_sb99_f275w[j]) * n_sb99_f275w[j] / f_sb99_f275w[j])**2 + (abs(tpt275interp[j+1] * f_sb99_f275w[j+1]) * n_sb99_f275w[j+1] / f_sb99_f275w[j+1])**2)\n",
    "        \n",
    "            # Add the measured uncertainty to the uncertainty list\n",
    "            n_areas.append(n_area)\n",
    "        \n",
    "        # Calculate the uncertainty of the integration of the transmitted, F275W-masked Starburst99 fit\n",
    "        n_int3 = np.sqrt(np.sum(np.array(n_areas, dtype=np.float64) ** 2))\n",
    "\n",
    "        # Make an empty list that will contain the uncertainty of each trapezoidal area of the \n",
    "        # integration of the transmitted, F814W-masked Starburst99 fit\n",
    "        n_areas = []\n",
    "    \n",
    "        # For each wavelength bin in the Starburst99 fit masked by the F814W filter wavelengths\n",
    "        for j, wav in enumerate(w_sb99_f814w[:-1]):\n",
    "        \n",
    "            # Calculate the width of the wavelength bin\n",
    "            w_bin_width = w_sb99_f814w[j+1] - w_sb99_f814w[j]\n",
    "\n",
    "            # Following the trapezoidal interpretation of integration, determine the area and \n",
    "            # associated uncertainty of the trapezoid starting at the jth wavelength bin \n",
    "            n_area = abs((w_bin_width * (tpt814interp[j] * f_sb99_f814w[j] + tpt814interp[j+1] * f_sb99_f814w[j+1]) / 2.0)) / (tpt814interp[j] * f_sb99_f814w[j] + tpt814interp[j+1] * f_sb99_f814w[j+1]) \\\n",
    "                * np.sqrt((abs(tpt814interp[j] * f_sb99_f814w[j]) * n_sb99_f814w[j] / f_sb99_f814w[j])**2 + (abs(tpt814interp[j+1] * f_sb99_f814w[j+1]) * n_sb99_f814w[j+1] / f_sb99_f814w[j+1])**2)\n",
    "\n",
    "            # Add the measured uncertainty to the uncertainty list\n",
    "            n_areas.append(n_area)\n",
    "        \n",
    "        # Calculate the uncertainty of the integration of the transmitted, F814W-masked Starburst99 fit\n",
    "        n_int4 = np.sqrt(np.sum(np.array(n_areas, dtype=np.float64) ** 2))\n",
    "    \n",
    "        # Calculate the LyC escape fraction and append it to the measurement results list\n",
    "        f_esc = (f275w_flux * int_tpt_f275w * f_sb99_f814w_trnsmtd) / (t_IGM * f814w_flux * int_tpt_f814w * f_sb99_f275w_trnsmtd)\n",
    "        f_escs.append(f_esc)\n",
    "    \n",
    "        # Calculate the propagated uncertainty of the LyC escape fraction and append it to the uncertainty list\n",
    "        n_esc = abs(f_esc) * np.sqrt((n_f275w_flux / f275w_flux)**2 + (n_f814w_flux / f814w_flux)**2 + (n_int3 / f_sb99_f275w_trnsmtd)**2 + (n_int4 / f_sb99_f814w_trnsmtd)**2)\n",
    "        n_escs.append(n_esc)\n",
    "    \n",
    "    # Create a header for the LyC escape fraction measurements output file\n",
    "    header = 'Measurements of the LyC escape fraction of each MagE slit aperture\\n' \\\n",
    "        + f'Created by esc.ipynb on {datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + 'Columns, from left to right: HST/WFC3 F275W flux and uncertainty (erg/s/cm^2), HST/ACS F814W flux and uncertainty (erg/s/cm^2), and LyC escape fraction and uncertainty (%)'\n",
    "\n",
    "    # Save the results to an output file\n",
    "    np.savetxt(f'{results}/f_esc_lyc_measurements.txt', np.array([f275w_fluxes, n_f275w_fluxes, f814w_fluxes, n_f814w_fluxes, f_escs, n_escs], dtype=np.float64).T, delimiter=' ', header=header, comments='# ')\n",
    "\n",
    "def tabulate(): \n",
    "\n",
    "    '''\n",
    "    Make a table of the LyC escape fraction measurements\n",
    "    '''    \n",
    "\n",
    "    def round_to_uncertainties(measurement, uncertainty):\n",
    "\n",
    "        '''\n",
    "        Round the uncertainty to one significant figure and the measurement to\n",
    "        the same significant digit as the uncertainty\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            measurement : numpy.float64\n",
    "                Value of the measurement\n",
    "\n",
    "            uncertainty : numpy.float64\n",
    "                Estimated uncertainty of the measurement\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            measurement : str\n",
    "                Value of the measurement, rounded to the same significant digit as\n",
    "                the 1 significant figure-rounded uncertainty\n",
    "\n",
    "            uncertainty : str\n",
    "                Estimated uncertainty of the measurement, rounded to 1 significant figure\n",
    "        '''\n",
    "\n",
    "        # Round the uncertainty to one significant figure\n",
    "        uncertainty = sigfig.round(uncertainty, sigfigs=1, type=str)\n",
    "\n",
    "        # If the uncertainty is less than 1\n",
    "        if '.' in uncertainty:\n",
    "\n",
    "            # Round the measurement to the same digit as the uncertainty\n",
    "            measurement = sigfig.round(measurement, decimals=len(uncertainty.split('.')[1]), type=str)\n",
    "\n",
    "        # If the uncertainty is greater than 1\n",
    "        elif '.' not in uncertainty:\n",
    "\n",
    "            # Round the measurement to the same digit as the uncertainty\n",
    "            measurement = sigfig.round(measurement, len(str(measurement).replace('-','').split('.')[0]) - len(uncertainty) + 1, type=str)\n",
    "\n",
    "        return measurement, uncertainty\n",
    "\n",
    "    # Dictionary of the slit IDs\n",
    "    slits = {\n",
    "        'M5' : [],\n",
    "        'M4' : [],\n",
    "        'M6' : [],\n",
    "        'M3' : [],\n",
    "        'M0' : [],\n",
    "        'M2' : [],\n",
    "        'M7' : [],\n",
    "        'M8' : [],\n",
    "        'M9' : []\n",
    "    }\n",
    "\n",
    "    # Establish common directories\n",
    "    home = os.getcwd()\n",
    "    results = f'{home}/results'\n",
    "\n",
    "    # Create the table header\n",
    "    table = '\\\\begin{deluxetable}{cccc}[t]\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\tablecaption{HST-based properties of the MagE spectra \\label{tab:f_esc}}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\tablehead{\\n' \\\n",
    "        + '\\t\\colhead{Slit} & \\colhead{$F_{275}$} & \\colhead{$F_{814}$} & \\colhead{$f_{\\\\rm{esc}}^{\\\\rm{LyC}}$}\\n' \\\n",
    "        + '}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\startdata\\n\\n'\n",
    "\n",
    "    # Open the LyC escape fraction measurements of the slits\n",
    "    f_esc_lyc_measurements = np.loadtxt(f'{results}/f_esc_lyc_measurements.txt', delimiter=' ', comments='#')\n",
    "\n",
    "    for i in range(4):\n",
    "        f_esc_lyc_measurements[:,i] = f_esc_lyc_measurements[:,i] * 1e19\n",
    "\n",
    "    # For each slit ID\n",
    "    for i, slit_id in enumerate(slits):\n",
    "\n",
    "        # Get the LyC escape fraction measurements for the slit ID\n",
    "        slit_f_esc_lyc_measurements = f_esc_lyc_measurements[i]\n",
    "\n",
    "        # Add the slit ID to the row\n",
    "        table = table + f'{slit_id} '\n",
    "\n",
    "        # For each measurement type to list in the table\n",
    "        for j in range(3):\n",
    "\n",
    "            # Get the measurement and its uncertainty for the slit ID\n",
    "            measurement = slit_f_esc_lyc_measurements[2 * j]\n",
    "            uncertainty = slit_f_esc_lyc_measurements[2 * j + 1]\n",
    "\n",
    "            # Round the uncertainty to 1 significant figure and the measurement to the \n",
    "            # corresponding significant digit\n",
    "            measurement, uncertainty = round_to_uncertainties(measurement, uncertainty)\n",
    "\n",
    "            # Add the formatted measurement / uncertainty to the table row\n",
    "            table = table + f'& ${measurement}\\pm{uncertainty}$ '\n",
    "\n",
    "        # Add a LaTeX row break character, unless the row is the last row in the table\n",
    "        table = table + '\\\\\\\\\\n' if i != len(slits) - 1 else table + '\\n'\n",
    "\n",
    "    # Add a footer to the table\n",
    "    table = table + '\\n\\enddata\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\tablecomments{From left to right: slit label, flux in the HST/WFC3 F275W and HST/ACS F814W filters ($10^{-20}$ erg s$^{-1}$ cm$^{-2}$), and $f_{\\\\text{esc}}^{\\\\text{LyC}}$ (\\%), all computed according to \\S\\,\\\\ref{ssec:methods_fesc}.}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\end{deluxetable}'\n",
    "\n",
    "    # Save the table\n",
    "    f = open(f'{results}/tables/f_esc_lyc_measurements_table.txt', 'w', encoding='utf-8')\n",
    "    f.write(table)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d43d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "measure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab65a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabulate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b2b404ef7cfffcb2d9e58206576e0220bed399f08fa92bc6fd125b02b641f3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
