{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: M. Riley Owens (GitHub: mrileyowens)\n",
    "\n",
    "# This file determines how the Lyman-alpha flux\n",
    "# in the MagE apertures changes due to seeing effects,\n",
    "# as estimated from the narrowband Lyman-alpha\n",
    "# images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import sigfig\n",
    "\n",
    "from numba import njit\n",
    "\n",
    "from sigfig import round as round_sigfig\n",
    "from decimal import Decimal, getcontext\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from math import floor, log10\n",
    "\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.convolution import convolve, convolve_fft, Gaussian2DKernel\n",
    "from astropy import uncertainty as unc\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func():\n",
    "\n",
    "    @njit\n",
    "    def convolve_uncertainties(uncertainty, mask, stdv, psf_stdv, shape):\n",
    "\n",
    "        '''\n",
    "        Convolve the uncertainties of an image\n",
    "\n",
    "        Parameters:\n",
    "            uncertainty : numpy.ndarray\n",
    "                The estimated uncertainty of the pixels in the image\n",
    "            mask : numpy.ndarray\n",
    "                The mask of the slit aperture\n",
    "            stdv : numpy.float64\n",
    "                The standard deviation of the Gaussian convolution kernel in pixels\n",
    "            psf_stdv : numpy.float64\n",
    "                The standard deviation of the image's PSF in pixels\n",
    "            shape : tuple\n",
    "                The dimensions of the image\n",
    "\n",
    "        Returns:\n",
    "            uncertainty_convolved : numpy.ndarray\n",
    "                The convolved estimated uncertainty of the pixels in the image, \n",
    "                broadcasted to the shape of the image\n",
    "        '''\n",
    "\n",
    "\n",
    "        # Set the kernel size to 3 standard deviations of the \n",
    "        # width of the time-averaged seeing conditions\n",
    "        kernel_size = int(3 * stdv)\n",
    "\n",
    "        # Make a dummy array of the convolved uncertainties with the same shape as the image\n",
    "        uncertainty_convolved = np.zeros_like(uncertainty, dtype=np.float64)\n",
    "\n",
    "        #image_pixels = np.stack(np.indices((shape[0], shape[1])), axis=-1).reshape(-1,2)\n",
    "\n",
    "        slit_pixels = np.vstack(np.where(mask > 0)).T\n",
    "\n",
    "        '''\n",
    "        # For each pixel column in the image\n",
    "        for i in range(shape[0]):\n",
    "\n",
    "            # For each pixel row in the image\n",
    "            for j in range(shape[1]):\n",
    "        '''\n",
    "\n",
    "        for pixel in slit_pixels:\n",
    "\n",
    "            '''\n",
    "            # Set the squared sum of the weighted uncertainties to zero, to be added to in the double loop below\n",
    "            squared_sum = 0.0\n",
    "\n",
    "            # For each pixel within the kernel size\n",
    "            #for k in range(-np.array([kernel_size,i]).min(), np.array([kernel_size+i,shape[0]-1]).min()):\n",
    "            #    for l in range(-np.array([kernel_size,j]).min(), np.array([kernel_size+j,shape[1]-1]).min()):\n",
    "            for k in range(np.array([0, pixel[0]-kernel_size], dtype=np.int64).max(), np.array([shape[0]-1, pixel[0]+kernel_size], dtype=np.int64).min() + 1):\n",
    "                for l in range(np.array([0, pixel[1]-kernel_size], dtype=np.int64).max(), np.array([shape[1]-1, pixel[1]+kernel_size], dtype=np.int64).min() + 1):\n",
    "\n",
    "                    # Calculate the weight of the pixel\n",
    "                    weight = np.exp(-(abs(pixel[0]-k)**2 + abs(pixel[1]-l)**2) / (2 * stdv**2))\n",
    "                    weight /= 2 * np.pi * stdv**2\n",
    "\n",
    "                    # Add the weighted uncertainty to the total squared sum of uncertainties\n",
    "                    squared_sum += (weight * uncertainty[k,l])**2\n",
    "\n",
    "            uncertainty_convolved[pixel[0],pixel[1]] = np.sqrt(squared_sum)\n",
    "\n",
    "            #print(uncertainty[i,j], uncertainty_convolved[i,j])\n",
    "            '''\n",
    "\n",
    "            #kernel_pixels = np.array([[x,y] for y in range(np.array([0, pixel[1]-kernel_size], dtype=np.int64).max(), np.array([shape[1]-1, pixel[1]+kernel_size], dtype=np.int64).min() + 1) \n",
    "            #    for x in range(np.array([0, pixel[0]-kernel_size], dtype=np.int64).max(), np.array([shape[0]-1, pixel[0]+kernel_size], dtype=np.int64).min() + 1)], dtype=np.int64)\n",
    "\n",
    "            squared_sum = 0.0\n",
    "\n",
    "            #for i in range(shape[0]):\n",
    "            #for j in range(shape[1]):\n",
    "\n",
    "            #for pixel_1 in image_pixels:\n",
    "            #    for pixel_2 in image_pixels:\n",
    "            for k in range(np.array([0, pixel[0]-kernel_size], dtype=np.int64).max(), np.array([shape[0]-1, pixel[0]+kernel_size], dtype=np.int64).min() + 1):\n",
    "                for l in range(np.array([0, pixel[1]-kernel_size], dtype=np.int64).max(), np.array([shape[1]-1, pixel[1]+kernel_size], dtype=np.int64).min() + 1):\n",
    "\n",
    "            #for pixel_1 in kernel_pixels:\n",
    "            #    for pixel_2 in kernel_pixels:\n",
    "\n",
    "                    #midpoint = (pixel_1 + pixel_2) / 2\n",
    "\n",
    "                    #squared_sum = np.exp(-np.dot(pixel - pixel_1, pixel - pixel_1) / (2 * stdv**2) - np.dot(pixel - pixel_2, pixel - pixel_2) / (2 * stdv**2) - np.dot(pixel_1 - pixel_2, pixel_1 - pixel_2) / (4 * psf_stdv**2)) * uncertainty_interp((pixel_1 + pixel_2) / 2)**2\n",
    "                    #squared_sum = np.exp(-np.dot((pixel - pixel_1).astype(np.float64), (pixel - pixel_1).astype(np.float64)) / (2 * stdv**2) - np.dot((pixel - pixel_2).astype(np.float64), (pixel - pixel_2).astype(np.float64)) / (2 * stdv**2) - np.dot((pixel_1 - pixel_2).astype(np.float64), (pixel_1 - pixel_2).astype(np.float64)) / (4 * psf_stdv**2)) * uncertainty[round(midpoint[0]), round(midpoint[1])]**2\n",
    "\n",
    "                    squared_sum += np.exp(-np.dot((np.array([k,l], dtype=np.int64) - pixel).astype(np.float64), (np.array([k,l], dtype=np.int64) - pixel).astype(np.float64)) / stdv**2) * uncertainty[k,l]**2\n",
    "\n",
    "            variance = squared_sum * (2 * np.sqrt(np.pi) * stdv * psf_stdv / np.sqrt(stdv**2 + psf_stdv**2))**2 / (2 * np.pi * stdv**2)**2\n",
    "\n",
    "            uncertainty_convolved[pixel[0],pixel[1]] = np.sqrt(variance)\n",
    "\n",
    "        # Multiply the dummy array of the convolved uncertainties by the convolved uncertainty\n",
    "        #uncertainty_convolved = uncertainty_convolved * np.sqrt(squared_sum)\n",
    "\n",
    "        return uncertainty_convolved\n",
    "\n",
    "    # Establish common directories\n",
    "    home = os.getcwd()\n",
    "    #data = f'{home}/data'\n",
    "    results = f'{home}/results'\n",
    "    \n",
    "    # Array of the slit IDs\n",
    "    #slits = np.array(['M5', 'M4', 'M6', 'M3', 'M0', 'M2', 'M7', 'M8', 'M9'], \n",
    "    #    dtype=str)\n",
    "    \n",
    "    # Dictionary of roughly-estimated time-weighted seeing conditions, \n",
    "    # written as the FWHM of the PSF in arcseconds\n",
    "    '''\n",
    "    seeing = {\n",
    "        'M5' : 0.97,\n",
    "        'M4' : 0.71,\n",
    "        'M6' : 0.76,\n",
    "        'M3' : 0.70,\n",
    "        'M0' : 1.34,\n",
    "        'M2' : 0.77,\n",
    "        'M7' : 0.73,\n",
    "        'M8' : 0.70,\n",
    "        'M9' : 0.68\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    # Dictionary of the MagE slit aperture IDs, listing, from left to right: the FWHM of the seeing conditions \n",
    "    # in arcseconds, airmasses of each exposure, written as sec(z), and the exposure times of each exposure\n",
    "    slits = {\n",
    "        'M5' : [0.97, [1.756, 1.668, 1.602, 1.553, 1.531], [2700, 2700, 2700, 2700, 2700]],\n",
    "        'M4' : [0.71, [1.529, 1.532, 1.549], [2400, 2400, 2400]],\n",
    "        'M6' : [0.76, [1.530, 1.549, 1.581, 1.531, 1.552], [2700, 2400, 2400, 2700, 2700]],\n",
    "        'M3' : [0.70, [1.775, 1.690, 1.627, 1.571, 1.542], [2400, 2400, 2400, 2400, 2400]],\n",
    "        'M0' : [1.34, [1.540, 1.528, 1.536, 1.565, 1.656], [2700, 2700, 2700, 2700, 2700]],\n",
    "        'M2' : [0.77, [1.584, 1.639, 1.592], [2700, 2700, 2700]],\n",
    "        'M7' : [0.73, [1.563, 1.600, 1.665, 1.569, 1.610, 1.672], [2400, 2400, 2400, 2400, 2400, 2400]],\n",
    "        'M8' : [0.70, [1.528, 1.537, 1.747, 1.529, 1.539, 1.757], [2400, 2400, 2400, 2400, 2400, 2400]],\n",
    "        'M9' : [0.68, [1.738, 1.654, 1.592, 1.552, 1.531], [2700, 2700, 2700, 2700, 2700]]\n",
    "    }\n",
    "\n",
    "    '''\n",
    "    # Dictionary of the airmasses of the individual slit pointings,\n",
    "    # written as sec(z)\n",
    "    airmass = {\n",
    "        'M5' : [1.756, 1.668, 1.602, 1.553, 1.531],\n",
    "        'M4' : [1.529, 1.532, 1.549],\n",
    "        'M6' : [1.530, 1.549, 1.581, 1.531, 1.552],\n",
    "        'M3' : [1.775, 1.690, 1.627, 1.571, 1.542],\n",
    "        'M0' : [1.540, 1.528, 1.536, 1.565, 1.656],\n",
    "        'M2' : [1.584, 1.639, 1.592],\n",
    "        'M7' : [1.563, 1.600, 1.665, 1.569, 1.610, 1.672],\n",
    "        'M8' : [1.528, 1.537, 1.747, 1.529, 1.539, 1.757],\n",
    "        'M9' : [1.738, 1.654, 1.592, 1.552, 1.531]\n",
    "    }\n",
    "\n",
    "    # Dictionary of the exposure times of the individual slit pointings,\n",
    "    # written in seconds\n",
    "    exposures = {\n",
    "        'M5' : [2700, 2700, 2700, 2700, 2700],\n",
    "        'M4' : [2400, 2400, 2400],\n",
    "        'M6' : [2700, 2400, 2400, 2700, 2700],\n",
    "        'M3' : [2400, 2400, 2400, 2400, 2400],\n",
    "        'M0' : [2700, 2700, 2700, 2700, 2700],\n",
    "        'M2' : [2700, 2700, 2700],\n",
    "        'M7' : [2400, 2400, 2400, 2400, 2400, 2400],\n",
    "        'M8' : [2400, 2400, 2400, 2400, 2400, 2400],\n",
    "        'M9' : [2700, 2700, 2700, 2700, 2700]       \n",
    "    }\n",
    "    '''\n",
    "\n",
    "    #lya_files = glob.glob(f'{data}/hst/Lya_contsub*.fits')\n",
    "    \n",
    "    # HST filter IDs used as continuum estimates for the narrowband\n",
    "    # Lya maps\n",
    "    filters = np.array(['F390W', 'F555W'], dtype=str)\n",
    "\n",
    "    # File path to the mask of the two largest arcs of the Sunburst Arc\n",
    "    # in the v5 HST data reduction WCS\n",
    "    arc_mask_file = f'{results}/masks/arc_mask_v5.fits'\n",
    "\n",
    "    # Get a cutout of the arc mask. This is necessary because we will\n",
    "    # convolve the Lya maps later, which is computationally expensive for\n",
    "    # a very large footprint.\n",
    "    arc_mask = fits.getdata(arc_mask_file)[4000:5600, 4400:5700]\n",
    "\n",
    "    # Invert the arc mask so that it is 0 where the arc is and\n",
    "    # 1 elsewhere. This will make it easier to create an effective\n",
    "    # mask of a slit, except where the arc is.\n",
    "    inverted_arc_mask = np.where(arc_mask > 0, 0, 1)\n",
    "\n",
    "    #with open('seeing_simulation.txt', 'a') as table:\n",
    "\n",
    "    #table = ''\n",
    "\n",
    "    # Make empty lists to append the results of the simulation to\n",
    "    fluxes, n_fluxes, fluxes_convolved, n_fluxes_convolved, ratios, n_ratios = [], [], [], [], [], []\n",
    "\n",
    "    # For each slit ID\n",
    "    for i, slit_id in enumerate(slits):\n",
    "\n",
    "        # Get the FWHM of the seeing conditions and the airmasses and exposure times of each exposure\n",
    "        seeing, airmass, exposures = slits[slit_id][0], slits[slit_id][1], slits[slit_id][2]\n",
    "\n",
    "        #table = table + f'{slit_id} & '\n",
    "    \n",
    "        # File path to the slit mask\n",
    "        slit_mask_file = f'{results}/masks/{slit_id}_mask_v5.fits'\n",
    "\n",
    "        # Get a cutout of the mask\n",
    "        slit_mask = fits.getdata(slit_mask_file)[4000:5600, 4400:5700]\n",
    "\n",
    "        # For each filter used to estimate the continuum\n",
    "        for j, filter in enumerate(filters):\n",
    "\n",
    "            # File path to the corresponding Lya map\n",
    "            file = f'{results}/lya_maps/Lya_cont_sub_{filter}.fits'\n",
    "\n",
    "            # Get cutouts of the map and the uncertainty map\n",
    "            lya = fits.open(file)[0].data[4000:5600, 4400:5700]\n",
    "            #mask = fits.open(f'{results}/box_for_median_imcoords_mask_v5.fits')[0].data[4300:5300, 4700:5400]\n",
    "            n_lya = fits.open(file)[1].data[4000:5600, 4400:5700]\n",
    "            \n",
    "            #n_lya_interp = RegularGridInterpolator((range(np.shape(lya)[0]), range(np.shape(lya)[1])), n_lya)\n",
    "            \n",
    "            # We are going to sum the flux in the Lya map\n",
    "            # in the slit, so determine the associated noise\n",
    "            #n_lya = np.sqrt(np.sum(np.square(n_lya * slit_mask)))\n",
    "\n",
    "            #stddev = ((0.70 * (1.641)**0.6) / 2.355) / 0.03\n",
    "\n",
    "            # The standard deviation of the exposure time-weighted PSF in pixels\n",
    "            stdv = ((seeing * (np.sum(np.multiply(airmass, exposures) / np.sum(exposures)))**0.6) / 2.355) / 0.03\n",
    "\n",
    "            # A Gaussian kernel with the above standard deviation\n",
    "            kernel = Gaussian2DKernel(x_stddev=stdv, y_stddev=stdv)\n",
    "\n",
    "            # Convolve the Lya map with the Gaussian kernel representing the exposure time-weighted seeing conditions\n",
    "            lya_convolved = convolve_fft(lya, kernel)\n",
    "            \n",
    "            # Compute the error of each pixel in the convolved Lya map as the\n",
    "            # standard deviation of the pixel values outside of the arc mask but\n",
    "            # inside the slit mask\n",
    "            #n_convolved = np.std(lya_convolved * slit_mask * inverted_arc_mask)\n",
    "            #n_convolved = np.ones_like(lya_convolved) * n_convolved\n",
    "            #n_convolved = np.sqrt(np.sum(np.square(n_convolved * slit_mask)))\n",
    "\n",
    "            # Convolve the uncertainties of the Lya map with the Gaussian kernel \n",
    "            # representing the exposure time-weighted seeing conditions\n",
    "            n_lya_convolved = convolve_uncertainties(n_lya.astype(np.float64), (slit_mask * arc_mask).astype(np.int64), stdv.astype(np.float64), 0.067 / 0.03, np.array([np.shape(lya)[0], np.shape(lya)[1]], dtype=np.int64))\n",
    "\n",
    "            # Sum the unconvolved Lya flux in the slit\n",
    "            flux = np.sum(slit_mask * lya)\n",
    "            fluxes.append(flux)\n",
    "\n",
    "            # Calculate the propagated uncertainty of the unconvolved Lya flux in the slit\n",
    "            n_flux = np.sqrt(np.sum((slit_mask * n_lya)**2))\n",
    "            n_fluxes.append(n_flux)\n",
    "\n",
    "            # Sum the convolved Lya flux in the slit\n",
    "            flux_convolved = np.sum(slit_mask * lya_convolved)\n",
    "            fluxes_convolved.append(flux_convolved)\n",
    "\n",
    "            # Calculate the uncertainty of the convolved flux\n",
    "            n_flux_convolved = np.sqrt(np.sum((slit_mask * n_lya_convolved)**2))\n",
    "            n_fluxes_convolved.append(n_flux_convolved)\n",
    "\n",
    "            # Calculate the ratio between the convolved and unconvolved flux\n",
    "            ratio = flux_convolved / flux\n",
    "            ratios.append(ratio)\n",
    "\n",
    "            # Calculate the propagated uncertainty of the ratio between the convolved and unconvolved Lya flux\n",
    "            n_ratio = np.absolute(ratio) * np.sqrt((n_flux / flux)**2 + (n_flux_convolved / flux_convolved)**2)\n",
    "            n_ratios.append(n_ratio)\n",
    "\n",
    "            #n = str(round_sigfig(n, sigfigs=1))\n",
    "            #n_convolved = str(round_sigfig(n_convolved, sigfigs=1))\n",
    "            #ratio_uncertainty = str(round_sigfig(ratio_uncertainty, sigfigs=1))\n",
    "\n",
    "            #place = -int(floor(log10(abs(float(ratio_uncertainty)))) + 1) if float(ratio_uncertainty) >= 1 else -int(floor(log10(abs(float(ratio_uncertainty)))))\n",
    "\n",
    "            #ratio = round(ratio, -int(floor(log10(abs(float(ratio_uncertainty))))) + 1)\n",
    "\n",
    "            #ratio_uncertainty = ratio_uncertainty.split('.')[1].count('0', )\n",
    "\n",
    "            #ratio = round_sigfig(ratio, decimals=ratio_uncertainty.split('.')[1].count('0') + 1)\n",
    "\n",
    "            #line.join([line, f'${ratio}\\pm {ratio_uncertainty}$'])\n",
    "            #table = table +  f'${flux}\\pm {n} & {flux_convolved}\\pm {n_convolved} & {ratio}\\pm {ratio_uncertainty}$'\n",
    "\n",
    "            #table = table + ' & ' if j==0 else table\n",
    "\n",
    "            #np.savetxt('seeing_simulation.txt')\n",
    "\n",
    "        #table = table + ' \\\\\\\\ \\n' if i < 8 else ''\n",
    "\n",
    "        #table.write(line)\n",
    "    \n",
    "    header = 'The Lyα fluxes and ratios measured in the seeing effects simulation\\n' \\\n",
    "        + f'Created by seeing.ipynb on {datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + 'The columns below are organized as follows. There are three types of measurements: unconvolved and convolved Lyα fluxes in a slit aperture (e/s),\\n' \\\n",
    "        + 'and the ratio between the two fluxes. Each measurement type has a set of 4 columns, arranged consecutively in the order mentioned previously.\\n' \\\n",
    "        + 'Those sets of 4 columns each have 2 consecutive subsets of 2 columns. In each subset, the first two columns represent, respectively, the\\n' \\\n",
    "        + 'measurement and uncertainty from the F390W-based Lyα map, and the last two columns represent, respectively, the measurement and uncertainty\\n' \\\n",
    "        + 'from the F555W-based Lyα map. The rows corresponding to the different MagE slit apertures are arranged, from top to bottom, in the following\\n' \\\n",
    "        + 'order: M5, M4, M6, M3, M0, M2, M7, M8, and M9.\\n' \\\n",
    "    \n",
    "    np.savetxt(f'{results}/seeing_simulation_measurements_results.txt', \n",
    "        np.array([fluxes[::2], n_fluxes[::2], fluxes[1::2], n_fluxes[1::2], fluxes_convolved[::2], n_fluxes_convolved[::2], fluxes_convolved[1::2], n_fluxes_convolved[1::2], ratios[::2], n_ratios[::2], ratios[1::2], n_ratios[1::2]], dtype=np.float64).T, \n",
    "        delimiter=' ', header=header, encoding='utf-8')\n",
    "\n",
    "def tabulate():\n",
    "\n",
    "    def round_to_uncertainties(measurement, uncertainty):\n",
    "\n",
    "        '''\n",
    "        Round the uncertainty to one significant figure and the measurement to\n",
    "        the same significant digit as the uncertainty\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            measurement : numpy.float64\n",
    "                Value of the measurement\n",
    "\n",
    "            uncertainty : numpy.float64\n",
    "                Estimated uncertainty of the measurement\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            measurement : str\n",
    "                Value of the measurement, rounded to the same significant digit as\n",
    "                the 1 significant figure-rounded uncertainty\n",
    "\n",
    "            uncertainty : str\n",
    "                Estimated uncertainty of the measurement, rounded to 1 significant figure\n",
    "        '''\n",
    "\n",
    "        # Round the uncertainty to one significant figure\n",
    "        uncertainty = sigfig.round(uncertainty, sigfigs=1, type=str)\n",
    "\n",
    "        # If the uncertainty is less than 1\n",
    "        if '.' in uncertainty:\n",
    "\n",
    "            # Round the measurement to the same digit as the uncertainty\n",
    "            measurement = sigfig.round(measurement, decimals=len(uncertainty.split('.')[1]), type=str)\n",
    "\n",
    "        # If the uncertainty is greater than 1\n",
    "        elif '.' not in uncertainty:\n",
    "\n",
    "            # Round the measurement to the same digit as the uncertainty\n",
    "            measurement = sigfig.round(measurement, len(str(measurement).replace('-','').split('.')[0]) - len(uncertainty) + 1, type=str)\n",
    "\n",
    "        return measurement, uncertainty\n",
    "\n",
    "    slits = {\n",
    "        'M5' : [],\n",
    "        'M4' : [],\n",
    "        'M6' : [],\n",
    "        'M3' : [],\n",
    "        'M0' : [],\n",
    "        'M2' : [],\n",
    "        'M7' : [],\n",
    "        'M8' : [],\n",
    "        'M9' : []\n",
    "    }\n",
    "\n",
    "    home = os.getcwd()\n",
    "    results = f'{home}/results'\n",
    "\n",
    "    table = '\\\\begin{deluxetable*}{r|lll|lll}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\tablecaption{Simulated aperture flux changes due to atmospheric seeing \\label{tab:seeing_simulation}}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\tablehead{\\n' \\\n",
    "        + '\\t\\colhead{Slit} & \\n' \\\n",
    "        + '\\t\\colhead{} &\\n' \\\n",
    "        + '\\t\\colhead{F390W} &\\n' \\\n",
    "        + '\\t\\colhead{} &\\n' \\\n",
    "        + '\\t\\colhead{} &\\n' \\\n",
    "        + '\\t\\colhead{F555W} & \\n' \\\n",
    "        + '\\t\\colhead{}\\n' \\\n",
    "        + '\\t\\\\\\\\\\n' \\\n",
    "        + '\\t\\colhead{} &\\n' \\\n",
    "        + '\\t\\colhead{Unconvolved} &\\n' \\\n",
    "        + '\\t\\colhead{Convolved} &\\n' \\\n",
    "        + '\\t\\colhead{Ratio} & \\n' \\\n",
    "        + '\\t\\colhead{Unconvolved} &\\n' \\\n",
    "        + '\\t\\colhead{Convolved} &\\n' \\\n",
    "        + '\\t\\colhead{Ratio}\\n' \\\n",
    "        + '\\t\\\\\\\\\\n' \\\n",
    "        + '\\t\\colhead{} &\\n' \\\n",
    "        + '\\t\\colhead{[$e^-$ s$^{-1}$]} &\\n' \\\n",
    "        + '\\t\\colhead{[$e^-$ s$^{-1}$]} &\\n' \\\n",
    "        + '\\t\\colhead{} &\\n' \\\n",
    "        + '\\t\\colhead{[$e^-$ s$^{-1}$]} &\\n' \\\n",
    "        + '\\t\\colhead{[$e^-$ s$^{-1}$]} &\\n' \\\n",
    "        + '\\t\\colhead{}\\n' \\\n",
    "        + '}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\startdata\\n' \\\n",
    "        + '\\n'\n",
    "\n",
    "    lya_measurements = np.loadtxt(f'{results}/seeing_simulation_measurements_results.txt', delimiter=' ', comments='#')\n",
    "\n",
    "    for i, slit_id in enumerate(slits):\n",
    "\n",
    "        slit_lya_measurements = lya_measurements[i]\n",
    "\n",
    "        f390w_flux, n_f390w_flux = round_to_uncertainties(slit_lya_measurements[0], slit_lya_measurements[1])\n",
    "        f390w_flux_convolved, n_f390w_flux_convolved = round_to_uncertainties(slit_lya_measurements[4], slit_lya_measurements[5])\n",
    "        f390w_ratio, n_f390w_ratio = round_to_uncertainties(slit_lya_measurements[8], slit_lya_measurements[9])\n",
    "\n",
    "        f555w_flux, n_f555w_flux = round_to_uncertainties(slit_lya_measurements[2], slit_lya_measurements[3])\n",
    "        f555w_flux_convolved, n_f555w_flux_convolved = round_to_uncertainties(slit_lya_measurements[6], slit_lya_measurements[7])\n",
    "        f555w_ratio, n_f555w_ratio = round_to_uncertainties(slit_lya_measurements[10], slit_lya_measurements[11])\n",
    "\n",
    "        # Add a separating horizontal line between the LyC-leaking and non-LyC-leaking spectra\n",
    "        table = table + '\\hline\\n' if slit_id == 'M0' else table + ''\n",
    "\n",
    "        table = table + f'{slit_id} & ${f390w_flux}\\pm{n_f390w_flux}$ & ${f390w_flux_convolved}\\pm{n_f390w_flux_convolved}$ & ${f390w_ratio}\\pm{n_f390w_ratio}$ ' \\\n",
    "            + f'& ${f555w_flux}\\pm{n_f555w_flux}$ & ${f555w_flux_convolved}\\pm{n_f555w_flux_convolved}$ & ${f555w_ratio}\\pm{n_f555w_ratio}$'\n",
    "\n",
    "        table = table + ' \\\\\\\\\\n' if i != len(slits) - 1 else table + '\\n'\n",
    "\n",
    "    table = table + '\\n\\enddata\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\tablecomments{From left to right: slit, flux inside the slit of the unconvolved and simulated seeing-convolved F390W-based Ly$\\\\alpha$ map, the ratio between the two fluxes, and likewise for the F555W-based Ly$\\\\alpha$ map. Ratios $>$ 1 indicate the flux in the aperture increased after the convolution, and ratios $<$ 1 indicate the flux in the aperture decreased after the convolution. The convolution used a 2-dimensional Gaussian kernel of the combined effect of the time-weighted seeing conditions and airmasses (\\S\\,\\\\ref{sssec:disc_lya_seeing}).}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\end{deluxetable*}'\n",
    "\n",
    "    # Save the table\n",
    "    f = open(f'{results}/tables/seeing_simulation_measurements_table.txt', 'w', encoding='utf-8')\n",
    "    f.write(table)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabulate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
