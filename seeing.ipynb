{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: M. Riley Owens (GitHub: mrileyowens)\n",
    "\n",
    "# This file determines how the Lyman-alpha flux\n",
    "# in the MagE apertures changes due to seeing effects,\n",
    "# as estimated from the narrowband Lyman-alpha\n",
    "# images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from numba import njit\n",
    "\n",
    "from sigfig import round as round_sigfig\n",
    "from decimal import Decimal, getcontext\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from math import floor, log10\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy.convolution import convolve, convolve_fft, Gaussian2DKernel\n",
    "from astropy import uncertainty as unc\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func():\n",
    "\n",
    "    #@njit\n",
    "    def convolve_uncertainties(uncertainty, stdv, shape):\n",
    "\n",
    "        '''\n",
    "        Convolve the uncertainties of an image\n",
    "\n",
    "        Parameters:\n",
    "            uncertainty : numpy.float64\n",
    "                The estimated uncertainty of the pixels in the image\n",
    "            stdv : numpy.float64\n",
    "                The standard deviation of the Gaussian convolution kernel in pixels\n",
    "            shape : tuple\n",
    "                The dimensions of the image\n",
    "\n",
    "        Returns:\n",
    "            uncertainty_convolved : numpy.ndarray\n",
    "                The convolved estimated uncertainty of the pixels in the image, \n",
    "                broadcasted to the shape of the image\n",
    "        '''\n",
    "    \n",
    "        # Set the kernel size to 3 standard deviations of the \n",
    "        # width of the time-averaged seeing conditions\n",
    "        kernel_size = int(0.1 * stdv)\n",
    "\n",
    "        # Make a dummy array of the convolved uncertainties with the same shape as the image\n",
    "        uncertainty_convolved = np.zeros(shape)\n",
    "\n",
    "        # For each pixel column in the image\n",
    "        for i in range(shape[0]):\n",
    "\n",
    "            # For each pixel row in the image\n",
    "            for j in range(shape[1]):\n",
    "\n",
    "                # Set the squared sum of the weighted uncertainties to zero, to be added to in the double loop below\n",
    "                squared_sum = 0.0\n",
    "\n",
    "                # For each pixel within the kernel size\n",
    "                #for k in range(-np.array([kernel_size,i]).min(), np.array([kernel_size+i,shape[0]-1]).min()):\n",
    "                #    for l in range(-np.array([kernel_size,j]).min(), np.array([kernel_size+j,shape[1]-1]).min()):\n",
    "                for k in range(np.array([0, i-kernel_size]).max(), np.array([shape[0]-1, i+kernel_size]).min() + 1):\n",
    "                    for l in range(np.array([0, j-kernel_size]).max(), np.array([shape[1]-1, j+kernel_size]).min() + 1):\n",
    "\n",
    "                        # Calculate the weight of the pixel\n",
    "                        weight = np.exp(-(abs(i-k)**2 + abs(j-l)**2) / (2 * stdv**2))\n",
    "                        weight /= 2 * np.pi * stdv**2\n",
    "\n",
    "                        # Add the weighted uncertainty to the total squared sum of uncertainties\n",
    "                        squared_sum += (weight * uncertainty[k,l])**2\n",
    "\n",
    "                uncertainty_convolved[i,j] = np.sqrt(squared_sum)\n",
    "\n",
    "        # Multiply the dummy array of the convolved uncertainties by the convolved uncertainty\n",
    "        #uncertainty_convolved = uncertainty_convolved * np.sqrt(squared_sum)\n",
    "\n",
    "        return uncertainty_convolved\n",
    "\n",
    "    home = os.getcwd()\n",
    "    data = f'{home}/data'\n",
    "    results = f'{home}/results'\n",
    "    \n",
    "    # Array of the slit IDs\n",
    "    #slits = np.array(['M5', 'M4', 'M6', 'M3', 'M0', 'M2', 'M7', 'M8', 'M9'], \n",
    "    #    dtype=str)\n",
    "    \n",
    "    # Dictionary of roughly-estimated time-weighted seeing conditions, \n",
    "    # written as the FWHM of the PSF in arcseconds\n",
    "    '''\n",
    "    seeing = {\n",
    "        'M5' : 0.97,\n",
    "        'M4' : 0.71,\n",
    "        'M6' : 0.76,\n",
    "        'M3' : 0.70,\n",
    "        'M0' : 1.34,\n",
    "        'M2' : 0.77,\n",
    "        'M7' : 0.73,\n",
    "        'M8' : 0.70,\n",
    "        'M9' : 0.68\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    slits = {\n",
    "        'M5' : [0.97, [1.756, 1.668, 1.602, 1.553, 1.531], [2700, 2700, 2700, 2700, 2700]],\n",
    "        'M4' : [0.71, [1.529, 1.532, 1.549], [2400, 2400, 2400]],\n",
    "        'M6' : [0.76, [1.530, 1.549, 1.581, 1.531, 1.552], [2700, 2400, 2400, 2700, 2700]],\n",
    "        'M3' : [0.70, [1.775, 1.690, 1.627, 1.571, 1.542], [2400, 2400, 2400, 2400, 2400]],\n",
    "        'M0' : [1.34, [1.540, 1.528, 1.536, 1.565, 1.656], [2700, 2700, 2700, 2700, 2700]],\n",
    "        'M2' : [0.77, [1.584, 1.639, 1.592], [2700, 2700, 2700]],\n",
    "        'M7' : [0.73, [1.563, 1.600, 1.665, 1.569, 1.610, 1.672], [2400, 2400, 2400, 2400, 2400, 2400]],\n",
    "        'M8' : [0.70, [1.528, 1.537, 1.747, 1.529, 1.539, 1.757], [2400, 2400, 2400, 2400, 2400, 2400]],\n",
    "        'M9' : [0.68, [1.738, 1.654, 1.592, 1.552, 1.531], [2700, 2700, 2700, 2700, 2700]]\n",
    "    }\n",
    "\n",
    "    '''\n",
    "    # Dictionary of the airmasses of the individual slit pointings,\n",
    "    # written as sec(z)\n",
    "    airmass = {\n",
    "        'M5' : [1.756, 1.668, 1.602, 1.553, 1.531],\n",
    "        'M4' : [1.529, 1.532, 1.549],\n",
    "        'M6' : [1.530, 1.549, 1.581, 1.531, 1.552],\n",
    "        'M3' : [1.775, 1.690, 1.627, 1.571, 1.542],\n",
    "        'M0' : [1.540, 1.528, 1.536, 1.565, 1.656],\n",
    "        'M2' : [1.584, 1.639, 1.592],\n",
    "        'M7' : [1.563, 1.600, 1.665, 1.569, 1.610, 1.672],\n",
    "        'M8' : [1.528, 1.537, 1.747, 1.529, 1.539, 1.757],\n",
    "        'M9' : [1.738, 1.654, 1.592, 1.552, 1.531]\n",
    "    }\n",
    "\n",
    "    # Dictionary of the exposure times of the individual slit pointings,\n",
    "    # written in seconds\n",
    "    exposures = {\n",
    "        'M5' : [2700, 2700, 2700, 2700, 2700],\n",
    "        'M4' : [2400, 2400, 2400],\n",
    "        'M6' : [2700, 2400, 2400, 2700, 2700],\n",
    "        'M3' : [2400, 2400, 2400, 2400, 2400],\n",
    "        'M0' : [2700, 2700, 2700, 2700, 2700],\n",
    "        'M2' : [2700, 2700, 2700],\n",
    "        'M7' : [2400, 2400, 2400, 2400, 2400, 2400],\n",
    "        'M8' : [2400, 2400, 2400, 2400, 2400, 2400],\n",
    "        'M9' : [2700, 2700, 2700, 2700, 2700]       \n",
    "    }\n",
    "    '''\n",
    "\n",
    "    #lya_files = glob.glob(f'{data}/hst/Lya_contsub*.fits')\n",
    "    \n",
    "    # HST filter IDs used as continuum estimates for the narrowband\n",
    "    # Lya maps\n",
    "    filters = np.array(['F390W', 'F555W'], dtype=str)\n",
    "\n",
    "    # File path to the mask of the two largest arcs of the Sunburst Arc\n",
    "    # in the v5 HST data reduction WCS\n",
    "    arc_mask_file = f'{results}/arc_mask_v5.fits'\n",
    "\n",
    "    # Get a cutout of the arc mask. This is necessary because we will\n",
    "    # convolve the Lya maps later, which is computationally expensive for\n",
    "    # a very large footprint.\n",
    "    arc_mask = fits.getdata(arc_mask_file)[4000:5600, 4400:5700]\n",
    "\n",
    "    # Invert the arc mask so that it is 0 where the arc is and\n",
    "    # 1 elsewhere. This will make it easier to create an effective\n",
    "    # mask of a slit, except where the arc is.\n",
    "    inverted_arc_mask = np.where(arc_mask > 0, 0, 1)\n",
    "\n",
    "    #with open('seeing_simulation.txt', 'a') as table:\n",
    "\n",
    "    #table = ''\n",
    "\n",
    "    fluxes, n_fluxes, fluxes_convolved, n_fluxes_convolved, ratios, n_ratios = [], [], [], [], [], []\n",
    "\n",
    "    # For each slit\n",
    "    for i, slit_id in enumerate(slits):\n",
    "\n",
    "        seeing, airmass, exposures = slits[slit_id][0], slits[slit_id][1], slits[slit_id][2]\n",
    "\n",
    "        #table = table + f'{slit_id} & '\n",
    "    \n",
    "        # File path to the slit mask\n",
    "        slit_mask_file = f'{results}/{slit_id}_mask_v5.fits'\n",
    "\n",
    "        # Get a cutout of the mask\n",
    "        slit_mask = fits.getdata(slit_mask_file)[4000:5600, 4400:5700]\n",
    "\n",
    "        '''\n",
    "        \n",
    "        !!! ATTENTION !!!\n",
    "\n",
    "        NEED TO UPDATE ESC.IPYNB TO ALSO USE A CONVOLUTION KERNEL WITH THE EXPOSURE TIME-WEIGHTED SEEING CONDITIONS THAT INCLUDES THE\n",
    "        EFFECTS OF AIRMASS, NOT JUST THE ATMOSPHERIC SEEING\n",
    "\n",
    "        '''\n",
    "\n",
    "        # For each filter used to estimate the continuum\n",
    "        for j, filter in enumerate(filters):\n",
    "\n",
    "            # File path to the corresponding Lya map\n",
    "            file = f'{results}/Lya_cont_sub_{filter}.fits'\n",
    "\n",
    "            # Get cutouts of the map and the uncertainty map\n",
    "            lya = fits.open(file)[0].data[4000:5600, 4400:5700]\n",
    "            #mask = fits.open(f'{results}/box_for_median_imcoords_mask_v5.fits')[0].data[4300:5300, 4700:5400]\n",
    "            n_lya = fits.open(file)[1].data[4000:5600, 4400:5700]\n",
    "            \n",
    "            # We are going to sum the flux in the Lya map\n",
    "            # in the slit, so determine the associated noise\n",
    "            #n_lya = np.sqrt(np.sum(np.square(n_lya * slit_mask)))\n",
    "\n",
    "            #stddev = ((0.70 * (1.641)**0.6) / 2.355) / 0.03\n",
    "\n",
    "            # The standard deviation of the exposure time-weighted PSF in pixels\n",
    "            stdv = ((seeing * (np.sum(np.multiply(airmass, exposures) / np.sum(exposures)))**0.6) / 2.355) / 0.03\n",
    "\n",
    "            # A Gaussian kernel with the above standard deviation\n",
    "            kernel = Gaussian2DKernel(x_stddev=stdv, y_stddev=stdv)\n",
    "\n",
    "            # Convolve the Lya map with the Gaussian kernel representing the exposure time-weighted seeing conditions\n",
    "            lya_convolved = convolve_fft(lya, kernel)\n",
    "            \n",
    "            # Compute the error of each pixel in the convolved Lya map as the\n",
    "            # standard deviation of the pixel values outside of the arc mask but\n",
    "            # inside the slit mask\n",
    "            #n_convolved = np.std(lya_convolved * slit_mask * inverted_arc_mask)\n",
    "            #n_convolved = np.ones_like(lya_convolved) * n_convolved\n",
    "            #n_convolved = np.sqrt(np.sum(np.square(n_convolved * slit_mask)))\n",
    "\n",
    "            # Convolve the uncertainties of the Lya map with the Gaussian kernel \n",
    "            # representing the exposure time-weighted seeing conditions\n",
    "            n_lya_convolved = convolve_uncertainties(n_lya, stdv, np.shape(lya))\n",
    "\n",
    "            # Sum the unconvolved Lya flux in the slit\n",
    "            flux = np.sum(slit_mask * lya)\n",
    "            fluxes.append(flux)\n",
    "\n",
    "            # Calculate the propagated uncertainty of the unconvolved Lya flux in the slit\n",
    "            n_flux = np.sqrt(np.sum(slit_mask * n_lya)**2)\n",
    "            n_fluxes.append(n_flux)\n",
    "\n",
    "            # Sum the convolved Lya flux in the slit\n",
    "            flux_convolved = np.sum(slit_mask * lya_convolved)\n",
    "            fluxes_convolved.append(flux_convolved)\n",
    "\n",
    "            # Calculate the uncertainty of the convolved flux\n",
    "            n_flux_convolved = np.sqrt(np.sum(slit_mask * n_lya_convolved)**2)\n",
    "            n_fluxes_convolved.append(n_flux_convolved)\n",
    "\n",
    "            # Calculate the ratio between the convolved and unconvolved flux\n",
    "            ratio = flux_convolved / flux\n",
    "            ratios.append(ratio)\n",
    "\n",
    "            # Calculate the propagated uncertainty of the ratio between the convolved and unconvolved Lya flux\n",
    "            n_ratio = np.absolute(ratio) * np.sqrt((n_flux / flux)**2 + (n_flux_convolved / flux_convolved)**2)\n",
    "            n_ratios.append(n_ratio)\n",
    "\n",
    "            #n = str(round_sigfig(n, sigfigs=1))\n",
    "            #n_convolved = str(round_sigfig(n_convolved, sigfigs=1))\n",
    "            #ratio_uncertainty = str(round_sigfig(ratio_uncertainty, sigfigs=1))\n",
    "\n",
    "            #place = -int(floor(log10(abs(float(ratio_uncertainty)))) + 1) if float(ratio_uncertainty) >= 1 else -int(floor(log10(abs(float(ratio_uncertainty)))))\n",
    "\n",
    "            #ratio = round(ratio, -int(floor(log10(abs(float(ratio_uncertainty))))) + 1)\n",
    "\n",
    "            #ratio_uncertainty = ratio_uncertainty.split('.')[1].count('0', )\n",
    "\n",
    "            #ratio = round_sigfig(ratio, decimals=ratio_uncertainty.split('.')[1].count('0') + 1)\n",
    "\n",
    "            #line.join([line, f'${ratio}\\pm {ratio_uncertainty}$'])\n",
    "            #table = table +  f'${flux}\\pm {n} & {flux_convolved}\\pm {n_convolved} & {ratio}\\pm {ratio_uncertainty}$'\n",
    "\n",
    "            #table = table + ' & ' if j==0 else table\n",
    "\n",
    "            #np.savetxt('seeing_simulation.txt')\n",
    "\n",
    "        #table = table + ' \\\\\\\\ \\n' if i < 8 else ''\n",
    "\n",
    "        #table.write(line)\n",
    "    \n",
    "    np.savetxt(f'{results}/seeing_simulation_measurements_results.txt', [fluxes, n_fluxes, fluxes_convolved, n_fluxes_convolved, ratios, n_ratios], \n",
    "        delimiter=' ', header='')\n",
    "\n",
    "def tabulate():\n",
    "\n",
    "    table = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
