{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f1beb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Riley Owens (GitHub: mrileyowens)\n",
    "\n",
    "# This file measures values and errors of \n",
    "# various Lyα profile parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92eb6b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import erf\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "\n",
    "import sigfig\n",
    "from sigfig import round as sf_round\n",
    "#from to_precision import std_notation\n",
    "import decimal\n",
    "\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "\n",
    "from numba import njit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf4643e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "\n",
    "    def extract_data(file):\n",
    "\n",
    "        '''\n",
    "        Extract spectrum from the .txt file\n",
    "\n",
    "        Parameters:\n",
    "            file : str\n",
    "                Name of the file\n",
    "\n",
    "        Returns:\n",
    "            w : numpy.ndarray\n",
    "                Observed wavelength bins\n",
    "            f : numpy.ndarray\n",
    "                Observed flux densities\n",
    "            n : numpy.ndarray\n",
    "                Observed Gaussian standard deviation of observed flux densities\n",
    "        '''\n",
    "\n",
    "        # Retrieve the data columns\n",
    "        w, f, n = np.loadtxt(file, delimiter='\\t', comments=('#', 'w'), usecols=(0,1,2), unpack=True)\n",
    "    \n",
    "        # If the file is not the stacked leaker spectrum. This step is necessary because \n",
    "        if 'leaker' not in file:\n",
    "\n",
    "            # Remove bins of extreme outliers\n",
    "            w = w[f < 1e-20]\n",
    "            n = n[f < 1e-20]\n",
    "            f = f[f < 1e-20]\n",
    "\n",
    "            # Convert from units of erg/s/cm^2/Hz to erg/s/cm^2/Å\n",
    "            f = f * 2.998e18 / np.square(w)\n",
    "            n = n * 2.998e18 / np.square(w)\n",
    "\n",
    "        return w, f, n    \n",
    "\n",
    "    def compute_fwhm_and_peak(parameters, Rs):\n",
    "\n",
    "        '''\n",
    "        Returns:\n",
    "            fwhm : numpy.float64\n",
    "            loc : numpy.float64\n",
    "        '''\n",
    "        def lin_interp(x, y, i):\n",
    "            return x[i] + (x[i+1] - x[i]) * y[i] / (y[i+1] - y[i])\n",
    "\n",
    "        def skew_gaussian(x, amp, cen, width, skew):\n",
    "\n",
    "            fit = amp * np.exp(-((x - cen) / width)**2 / 2) * (1 + erf(skew * ((x - cen) / width) / np.sqrt(2)))\n",
    "\n",
    "            return fit\n",
    "\n",
    "        v = np.arange(-500,500,0.1)\n",
    "\n",
    "        fits = []\n",
    "\n",
    "        for i, fit_params in enumerate(parameters):\n",
    "\n",
    "            fit = skew_gaussian(v, *fit_params)\n",
    "\n",
    "            fits.append([*fit])\n",
    "\n",
    "        fits_max_indices = np.argmax(fits, axis=1)\n",
    "\n",
    "        locs = v[fits_max_indices]\n",
    "\n",
    "        fits_halved = fits - np.amax(fits, axis=1, keepdims=True) / 2\n",
    "\n",
    "        crossings = np.diff(np.sign(fits_halved), axis=1) != 0\n",
    "\n",
    "        indices = np.array(np.where(crossings))\n",
    "\n",
    "        indices = indices.T\n",
    "\n",
    "        fwhms = []\n",
    "\n",
    "        for i in range(1000):\n",
    "\n",
    "            fwhm = abs(lin_interp(v, fits_halved[i], indices[2 * i][1]) - lin_interp(v, fits_halved[i], indices[2 * i + 1][1]))\n",
    "\n",
    "            fwhms.append(fwhm)\n",
    "\n",
    "        #res = np.random.normal(R, R_error, len(parameters))\n",
    "\n",
    "        fwhms = np.sqrt((np.array(fwhms))**2 - (Rs)**2)\n",
    "\n",
    "        return fwhms, locs\n",
    "\n",
    "    def two_peaks(x, amp_r, cen_r, width_r, skew_r, amp_c, cen_c, width_c, cntm):\n",
    "\n",
    "        return amp_r * np.exp(-((x - cen_r) / width_r)**2 / 2) * (1 + erf(skew_r * ((x - cen_r) / width_r) / np.sqrt(2))) \\\n",
    "            + amp_c * np.exp(-((x - cen_c) / width_c)**2 / 2) \\\n",
    "            + cntm\n",
    "\n",
    "    def three_peaks(x, amp_b, cen_b, width_b, skew_b, amp_r, cen_r, width_r, skew_r, amp_c, cen_c, width_c, cntm):\n",
    "\n",
    "        return amp_b * np.exp(-((x - cen_b) / width_b)**2 / 2) * (1 + erf(skew_b * ((x - cen_b) / width_b) / np.sqrt(2)))  \\\n",
    "            + amp_r * np.exp(-((x - cen_r) / width_r)**2 / 2) * (1 + erf(skew_r * ((x - cen_r) / width_r) / np.sqrt(2))) \\\n",
    "            + amp_c * np.exp(-((x - cen_c) / width_c)**2 / 2) \\\n",
    "            + cntm\n",
    "\n",
    "    # Dictionary containing information about the MagE slit spectra, including, from left to right:\n",
    "    # the name of the file containing the data, redshift (taken from Mainali et al. (2022) (ApJ, 940, 160)),\n",
    "    # magnification of the slit, a tuple specifying a range of Lya peculiar velocities used to set \n",
    "    # initial parameter estimates of the central Lya peak, spectral resolution, and uncertainty in the spectral resolution\n",
    "    slits = {\n",
    "        'NL' : ['rest_sba-nonleaker-no_m3_MWdr.txt', 0, 1, [60,140], 5400, 200],\n",
    "        'L' : ['rest_sba-leaker-no_m0_MWdr.txt', 0, 1, [20,130], 5300, 200],\n",
    "        'M5' : ['psz-arcslit-m5-comb1_MWdr.txt', 2.37086, 51, [0,100], 5500, 400],\n",
    "        'M4' : ['psz-arcslit-m4-comb1_MWdr.txt', 2.37073, 14.6, [0,85], 5400, 300],\n",
    "        'M6' : ['psz-arcslit-m6-comb1_MWdr.txt', 2.37021, 147, [10,130], 5300, 300],\n",
    "        'M3' : ['psz-arcslit-m3-comb1_MWdr.txt', 2.37025, 36, [35,120], 5500, 400],\n",
    "        'M0' : ['planckarc_m0-comb1_MWdr.txt', 2.37014, 10, [10,130], 4700, 200],\n",
    "        'M2' : ['psz-arcslit-m2-comb1_MWdr.txt', 2.37017, 32, [45,125], 5300, 300],\n",
    "        'M7' : ['psz-arcslit-m7-comb1_MWdr.txt', 2.37044, 35, [15,100], 5200, 200],\n",
    "        'M8' : ['psz-arcslit-m8-comb1_MWdr.txt', 2.37024, 29, [25,125], 5200, 300],\n",
    "        'M9' : ['psz-arcslit-m9-comb1_MWdr.txt', 2.37030, 31, [15,125], 5500, 400]\n",
    "    }\n",
    "\n",
    "    # Establish directories\n",
    "    home = os.getcwd()\n",
    "    data = f'{home}/data'\n",
    "    results = f'{home}/results'\n",
    "\n",
    "    #f_esc = [2.3, -0.6, 3, 2.3, 17, 18, 12, 15, 14]\n",
    "    #ne_esc = [0.8, 0.2, 1, 0.8, 6, 7, 5, 6, 5]\n",
    "\n",
    "    #c_peak_range = np.array([[60,140],[20,130],[0,100],[0,85],[10,130],[35,120],[10,130],\n",
    "    #                         [45,125],[15,100],[25,125],[15,125]], dtype=np.float64)\n",
    "\n",
    "    #total_result = np.array([np.empty((9,1000))])\n",
    "\n",
    "    '''\n",
    "    R = np.array([\n",
    "        299792.458 / 5400,\n",
    "        299792.458 / 5300,\n",
    "        299792.458 / 5500,\n",
    "        299792.458 / 5400,\n",
    "        299792.458 / 5300,\n",
    "        299792.458 / 5500,\n",
    "        299792.458 / 4700,\n",
    "        299792.458 / 5300,\n",
    "        299792.458 / 5200,\n",
    "        299792.458 / 5200,\n",
    "        299792.458 / 5500\n",
    "    ])\n",
    "\n",
    "    R_error = np.array([\n",
    "        299792.458 / 5400**2 * 200,\n",
    "        299792.458 / 5300**2 * 200,\n",
    "        299792.458 / 5500**2 * 400,\n",
    "        299792.458 / 5400**2 * 300,\n",
    "        299792.458 / 5300**2 * 300,\n",
    "        299792.458 / 5500**2 * 400,\n",
    "        299792.458 / 4700**2 * 200,\n",
    "        299792.458 / 5300**2 * 300,\n",
    "        299792.458 / 5200**2 * 200,\n",
    "        299792.458 / 5200**2 * 300,\n",
    "        299792.458 / 5500**2 * 400\n",
    "    ])\n",
    "    '''\n",
    "\n",
    "    # Instantiate a cosmology with 30% matter-based energy density and an expansion rate of 70 km/s/Mpc\n",
    "    cosmology = FlatLambdaCDM(70,0.3)\n",
    "\n",
    "    # For each slit ID\n",
    "    for i, slit_id in enumerate(slits):\n",
    "\n",
    "        # Unpack the dictionary values for this slit\n",
    "        file_name, z, mag, c_peak_range, R, R_error = slits[slit_id]\n",
    "\n",
    "        # Convert the spectral resolution to units of km/s, which is \n",
    "        # necessary when correcting the FWHMs (which has units of km/s)\n",
    "        # for the instrumental resolution\n",
    "        R_error = 299792.458 / R**2 * R_error\n",
    "        R = 299792.458 / R\n",
    "\n",
    "        '''\n",
    "        mag = slits[slit_id][2]\n",
    "        c_peak_range = slits[slit_id][3]\n",
    "        R = slits[slit_id][4]\n",
    "        R_error = slits[slit_id][5]\n",
    "        '''\n",
    "\n",
    "        # Set a scaling factor which will be applied to the data. This is necesary\n",
    "        # because for the observed data (so not the stacked spectra), the fits are \n",
    "        # very poor (the fitting does not explore the parameter space) without rescaling \n",
    "        # them, perhaps related to float precision because their given flux density units, \n",
    "        # the flux density values are very small (~10^16 - 10^17 erg/s/cm^2/Å)\n",
    "        factor = 1e16 if 'M' in slit_id else 1\n",
    "\n",
    "        # Extract the data from the .txt file\n",
    "        w, f, n = extract_data(f'{data}/spectra/mage/{file_name}')\n",
    "\n",
    "        # Cut the data to just the relevant portion of the spectrum\n",
    "        f = f[(w >= 1195 * (1 + z)) & (w <= 1235 * (1 + z))]\n",
    "        n = n[(w >= 1195 * (1 + z)) & (w <= 1235 * (1 + z))]\n",
    "        w = w[(w >= 1195 * (1 + z)) & (w <= 1235 * (1 + z))]\n",
    "        \n",
    "        # Place the data in the rest frame, and apply the scaling factor\n",
    "        w = w / (1 + z)\n",
    "        f = f * (1 + z) * factor\n",
    "        n = n * (1 + z) * factor\n",
    "\n",
    "        # Calculate the Lya peculiar velocities\n",
    "        v = 299792.458 * (w / 1215.67 - 1)\n",
    "\n",
    "        # Instantiate a cosmology with 30% matter-based energy density and an expansion rate of 70 km/s/Mpc\n",
    "        #cosmology = FlatLambdaCDM(70,0.3)\n",
    "\n",
    "        # Determine the luminosity distance to the redshift in units of centimeters (to match the flux density unit)\n",
    "        l_dist = cosmology.luminosity_distance(z).value * 3.086e24\n",
    "\n",
    "        # Create a boolean mask designating the wavelength range used to compute the local continuum\n",
    "        cntm_mask = (w >= 1221) & (w <= 1225)\n",
    "\n",
    "        # Create a boolean mask designating the integration range when computing the equivalent width\n",
    "        #ew_mask = (w >= 1212) & (w <= 1221)\n",
    "\n",
    "        # Create a boolean mask designating the integration range when computing the Lya equivalent width and luminosity\n",
    "        lya_profile_mask = (w >= 1212) & (w <= 1221)\n",
    "        \n",
    "        # Create boolean masks for the integration ranges about the rest velocity of the Lya profile\n",
    "        # for computing the central escape fraction of the Lya profile\n",
    "        v100_mask = (v >= -100) & (v <= 100)\n",
    "        v1000_mask = (v >= -1000) & (v <= 1000)\n",
    "\n",
    "        # Mask for the observed-frame wavelength boundaries when computing the luminosity of the Lya profile\n",
    "        #l_mask = (w * (1 + z) >= 1212 * (1 + z)) & (w * (1 + z) <= 1221 * (1 + z))\n",
    "\n",
    "        # Set a string for the flux density unit of the data\n",
    "        flux_density_unit = 'erg/s/cm^2/Å' if 'M' in slit_id else 'normalized wavelength-space flux density'\n",
    "\n",
    "        # If there is no blue peak and the slit ID is not M5\n",
    "        if i in [0,3,4]:\n",
    "        \n",
    "            # Set the model function and the model parameter labels\n",
    "            model = two_peaks\n",
    "            model_parameter_labels = f'redshifted Lyα peak amplitude ({flux_density_unit}), centroid (km/s), width (km/s), and skew, \\n' \\\n",
    "                + f'central Lyα peak amplitude ({flux_density_unit}), centroid (km/s), and width (km/s), ' \\\n",
    "                + f'and local continuum ({flux_density_unit})'\n",
    "        \n",
    "            # Set the initial parameter estimates and bounds\n",
    "            p0 = (np.amax(f[(v >= 150) & (v <= 1000)]),200,20,1,np.amax(f[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]),80,40,np.median(f[cntm_mask]))\n",
    "            bounds = ([0,0,0,0,0,0,0,0],[np.amax(f),1000,np.inf,np.inf,np.amax(f),200,85,np.amax(f[cntm_mask])])\n",
    "    \n",
    "        # If the slit ID is M5; this elif statement is necessary for improved initial parameter estimates of the central Lya peak in slit M5. The previous\n",
    "        # if statement was not satisfactory\n",
    "        elif i in [2]:\n",
    "\n",
    "            # Set the model function and the model parameter labels\n",
    "            model = two_peaks\n",
    "            model_parameter_labels = f'redshifted Lyα peak amplitude ({flux_density_unit}), centroid (km/s), width (km/s), and skew, \\n' \\\n",
    "                + f'central Lyα peak amplitude ({flux_density_unit}), centroid (km/s), and width (km/s), ' \\\n",
    "                + f'and local continuum ({flux_density_unit})'\n",
    "\n",
    "            # Set the initial parameter estimates and bounds\n",
    "            p0 = (np.amax(f[(v >= 150) & (v <= 1000)]),200,20,1,np.amax(f[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]) - np.median(f[cntm_mask]),50,20,np.median(f[cntm_mask]))\n",
    "            bounds = ([0,0,0,0,0,0,0,0],[np.amax(f),500,np.inf,np.inf,np.amax(f[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]) - np.median(f[cntm_mask]),100,40,np.amax(f[cntm_mask])])\n",
    "\n",
    "        # Otherwise\n",
    "        else:\n",
    "\n",
    "            # Set the model function and the model parameter labels\n",
    "            model = three_peaks\n",
    "            model_parameter_labels = f'blueshifted Lyα peak amplitude ({flux_density_unit}), centroid (km/s), width (km/s), and skew, \\n' \\\n",
    "                + f'redshifted Lyα peak amplitude ({flux_density_unit}), centroid (km/s), width (km/s), and skew, \\n' \\\n",
    "                + f'central Lyα peak amplitude ({flux_density_unit}), centroid (km/s), and width (km/s), ' \\\n",
    "                + f'and local continuum ({flux_density_unit})'\n",
    "\n",
    "            # Set the initial parameter estimates and bounds\n",
    "            p0 = (np.amax(f[(v >= -1000) & (v <= 0)]),-150,20,-1,np.amax(f[(v >= 150) & (v <= 1000)]),200,20,1,np.amax(f[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]),80,40,np.median(f[cntm_mask]))\n",
    "            bounds = ([0,-1000,0,-np.inf,0,0,0,0,0,0,0,0],[np.amax(f[(v >= -1000) & (v <= 0)]),0,np.inf,0,np.amax(f),1000,np.inf,np.inf,np.amax(f),200,85,np.amax(f[cntm_mask])])\n",
    "\n",
    "        # Establish empty lists to append the results of the Monte Carlo simulation to\n",
    "        mc_spectra = []\n",
    "        parameters = []\n",
    "        measurements = []\n",
    "\n",
    "        # For each iteration in the Monte Carlo simulation\n",
    "        for j in range(1000):\n",
    "\n",
    "            # Draw a randomly sampled spectrum from the original observation, assuming \n",
    "            # that the observed flux densities and associated uncertainties correspond \n",
    "            # to the mean and standard deviation of Gaussian distributions, respectively\n",
    "            f_mc = np.random.normal(f, n)\n",
    "\n",
    "            # Compute the local continuum as the median flux density between 1221 - 1225 Å in the rest frame\n",
    "            c = np.median(f_mc[cntm_mask])\n",
    "\n",
    "            # Compute the rest-frame equivalent width of the Lya profile\n",
    "            ew = -1 * np.trapz(1 - (f_mc / c)[lya_profile_mask], w[lya_profile_mask])\n",
    "\n",
    "            # Compute the central escape fraction of the Lya profile\n",
    "            f_cen = np.trapz(f_mc[v100_mask], v[v100_mask]) / np.trapz(f_mc[v1000_mask], v[v1000_mask]) * 100\n",
    "\n",
    "            # Compute the luminosity of the Lya profile, placing the data back into the observed frame, removing the scaling factor, and \n",
    "            # correcting for the magnification\n",
    "            #l = 4 * np.pi * np.trapz((f_mc / (1 + z) / mag)[l_mask] - c / (1 + z) / mag, (w * (1 + z))[l_mask]) * l_dist**2\n",
    "            l = 4 * np.pi * np.trapz((f_mc[lya_profile_mask] - c) / (1 + z) / mag / factor, (w * (1 + z))[lya_profile_mask]) * l_dist**2\n",
    "\n",
    "            # Directly fit the Lya profile with the assigned model function and initial parameter estimates and bounds\n",
    "            p, _ = curve_fit(model, v, f_mc, p0=p0, bounds=bounds, maxfev=1e6) \n",
    "\n",
    "            # Compute the ratio between the 'minimum' flux density between the redshifted and blueshifted Lya peaks \n",
    "            # (really taken as the amplitude of the central Lya peak; see paper for justifying details) and the local continuum\n",
    "            ratio = p[-4] / p[-1]\n",
    "\n",
    "            # Append the randomly sampled spectrum, best-fit model parameters, and Lya measurements to the aggregate lists\n",
    "            mc_spectra.append([*f_mc])\n",
    "            parameters.append([*p])\n",
    "            measurements.append([ratio, ew, f_cen, l])\n",
    "\n",
    "        # Convert all of the result lists into arrays for easier handling\n",
    "        p0 = np.array(p0, dtype=np.float64)\n",
    "        mc_spectra = np.array(mc_spectra, dtype=np.float64)\n",
    "        parameters = np.array(parameters, dtype=np.float64)\n",
    "        measurements = np.array(measurements, dtype=np.float64)\n",
    "\n",
    "        # Unscale the affected best-fit model parameters by the artifical scaling factor\n",
    "        parameters[:,-1], parameters[:,-4], parameters[:,-8] = parameters[:,-1] / factor, parameters[:,-4] / factor, parameters[:,-8] / factor\n",
    "\n",
    "        # Also unscale the amplitude of the blueshifted Lya peak if the model function had 3 peaks\n",
    "        parameters[:,0] = parameters[:,0] / factor if model==three_peaks else parameters[:,0]\n",
    "\n",
    "        # Unscale the affected intial model parameter estimates by the artificial scale factor\n",
    "        p0[-1], p0[-4], p0[-8] = p0[-1] / factor, p0[-4] / factor, p0[-8] / factor\n",
    "\n",
    "        # Also unscale the initial estimate of the amplitude of the blueshifted Lya peak if the model function had 3 peaks\n",
    "        p0[0] = p0[0] / factor if model==three_peaks else p0[0]\n",
    "\n",
    "        measurements[:,-1] = np.nan if 'L' in slit_id else measurements[:,-1]\n",
    "\n",
    "        # Randomly sample spectral resolutions for the spectrum from a Gaussian distribution, \n",
    "        # matching the number of iterations of the Monte Carlo simulation, and assuming that \n",
    "        # the reported measurement and uncertainty correspond to the mean and standard deviation \n",
    "        # of the Gaussian distribution. This is necessary to fold in the uncertainty in the \n",
    "        # spectral resolution into the measured FWHMs of the Lya peaks, since they are corrected\n",
    "        # for instrumental resolution\n",
    "        Rs = np.random.normal(R, R_error, len(parameters))\n",
    "\n",
    "        # Calculate the FWHMs of the central Lya peak, correcting for the instrumental resolution\n",
    "        fwhms_c = 2 * np.sqrt(2 * np.log(2)) * parameters.T[-2]\n",
    "        fwhms_c = np.sqrt((np.array(fwhms_c))**2 - (Rs)**2)\n",
    "\n",
    "        # Calculate the FWHMs and locations of the maxima of the redshifted Lya peak\n",
    "        fwhms_r, locs_r = compute_fwhm_and_peak(parameters[:, -8:-4], Rs)\n",
    "\n",
    "        # Insert the central and redshifted Lya peak FWHMs into the Lya measurements array\n",
    "        measurements = np.insert(measurements, 0, fwhms_r, axis=1)\n",
    "        measurements = np.insert(measurements, 0, fwhms_c, axis=1)\n",
    "\n",
    "        # Instantiate the Lya peak separations and blueshifted Lya peak FWHMs as NaNs. This is\n",
    "        # done to keep the output Lya measurements the same shape for all the spectra, even if\n",
    "        # they do not have a blueshifted Lya peak necessary to measure the Lya peak separation\n",
    "        # and blueshifted Lya peak FWHM. If the spectrum does have a third peak, these NaNs\n",
    "        # will be replaced by actual measurements in the following if statement\n",
    "        v_sep, fwhms_b = np.empty(1000), np.empty(1000)\n",
    "        v_sep[:], fwhms_b[:] = np.nan, np.nan\n",
    "\n",
    "        # If the spectrum has three Lya peaks\n",
    "        if slit_id in ['L', 'M0', 'M2', 'M3', 'M7', 'M8', 'M9']:\n",
    "            \n",
    "            # Calculate the FWHMs and locations of the maxima of the blueshifted Lya peak\n",
    "            fwhms_b, locs_b = compute_fwhm_and_peak(parameters[:, 0:4], Rs)\n",
    "\n",
    "            # Calculate the Lya peak separations\n",
    "            v_sep = np.abs(locs_r - locs_b)\n",
    "\n",
    "        # Insert the Lya peak separations and blueshifted Lya peak FWHMs into the Lya measurements array\n",
    "        measurements = np.insert(measurements, 0, [v_sep, fwhms_b], axis=1)\n",
    "\n",
    "        # If the results folder for this slit ID doesn't exist yet, make it\n",
    "        if not os.path.isdir(f'{results}/lya_fits/{slit_id}'):\n",
    "            os.makedirs(f'{results}/lya_fits/{slit_id}')\n",
    "\n",
    "        # Set the header for the output file containing the randomly sampled Lya spectra\n",
    "        header = f'Randomly sampled Lyα spectra of the Monte Carlo simulation of {f\"slit {slit_id}\" if \"M\" in slit_id else slit_id}\\n' \\\n",
    "            + '\\n' \\\n",
    "            + f'Columns, from left to right: peculiar velocity relative to Lyα (km/s), flux density uncertainty ({flux_density_unit}), and randomly sampled flux densities of each iteration of the Monte Carlo simulation ({flux_density_unit})\\n'\n",
    "\n",
    "        # Save the output file containing the randomly sampled Lya spectra\n",
    "        np.savetxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_spectra.txt', np.array([v, n, *mc_spectra]).T, header=header, delimiter=' ', encoding='utf-8')    \n",
    "\n",
    "        # Set the header for the output file containing the best-fit model parameters\n",
    "        header = f'Best-fit parameters of the Lyα fits of {f\"slit {slit_id}\" if \"M\" in slit_id else slit_id}\\n' \\\n",
    "            + '\\n' \\\n",
    "            + f'Initial parameters: {p0}\\n' \\\n",
    "            + '\\n' \\\n",
    "            + f'Columns, from left to right: {model_parameter_labels}\\n'\n",
    "\n",
    "        # Save the output file containing the best-fit model parameters\n",
    "        np.savetxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_best_fit_model_parameters.txt', parameters, header=header, delimiter=' ', encoding='utf-8')\n",
    "\n",
    "        # Set the header for the output file containing the Lya measurements\n",
    "        header = f'Measurements of the Lyα profiles from the best-fit curves of the Monte Carlo simulation of {f\"slit {slit_id}\" if \"M\" in slit_id else slit_id}\\n' \\\n",
    "            + '\\n' \\\n",
    "            + 'Columns, from left to right: Lyα peak separation (km/s), blueshifted Lyα peak FWHM (km/s), central Lyα peak FWHM (km/s), redshifted Lyα peak FWHM (km/s),\\n' \\\n",
    "            + 'ratio between the \\'minimum\\' flux density between the redshifted and blueshifted Lyα peaks and the continuum flux density (consult the paper for more details),\\n' \\\n",
    "            + 'rest-frame Lyα equivalent width (Å), central fraction of Lyα flux (%), and Lyα luminosity (erg/s)\\n'\n",
    "\n",
    "        # Save the output file containing the Lya measurements\n",
    "        np.savetxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_measurements.txt', measurements, header=header, delimiter=' ', encoding='utf-8')\n",
    "\n",
    "def correlate():\n",
    "\n",
    "    slits = {\n",
    "        'NL' : ['rest_sba-nonleaker-no_m3_MWdr.txt', 0, 1, [60,140]],\n",
    "        'L' : ['rest_sba-leaker-no_m0_MWdr.txt', 0, 1, [20,130]],\n",
    "        'M5' : ['psz-arcslit-m5-comb1_MWdr.txt', 2.37086, 51, [0,100]],\n",
    "        'M4' : ['psz-arcslit-m4-comb1_MWdr.txt', 2.37073, 14.6, [0,85]],\n",
    "        'M6' : ['psz-arcslit-m6-comb1_MWdr.txt', 2.37021, 147, [10,130]],\n",
    "        'M3' : ['psz-arcslit-m3-comb1_MWdr.txt', 2.37025, 36, [35,120]],\n",
    "        'M0' : ['planckarc_m0-comb1_MWdr.txt', 2.37014, 10, [10,130]],\n",
    "        'M2' : ['psz-arcslit-m2-comb1_MWdr.txt', 2.37017, 32, [45,125]],\n",
    "        'M7' : ['psz-arcslit-m7-comb1_MWdr.txt', 2.37044, 35, [15,100]],\n",
    "        'M8' : ['psz-arcslit-m8-comb1_MWdr.txt', 2.37024, 29, [25,125]],\n",
    "        'M9' : ['psz-arcslit-m9-comb1_MWdr.txt', 2.37030, 31, [15,125]]\n",
    "    }\n",
    "\n",
    "    # Establish directories\n",
    "    home = os.getcwd()\n",
    "    results = f'{home}/results'\n",
    "\n",
    "    # The LyC escape fractions and associated uncertainties of each spectrum\n",
    "    f_esc = [np.nan, np.nan, 2.3, -0.6, 3, 2.3, 17, 18, 12, 15, 14]\n",
    "    ne_esc = [np.nan, np.nan, 0.8, 0.2, 1, 0.8, 6, 7, 5, 6, 5]\n",
    "\n",
    "    # Make an empty list that will contain the statistical correlation results\n",
    "    corr_coefs = []\n",
    "\n",
    "    # For each Lya parameter\n",
    "    for i in range(8):\n",
    "\n",
    "        # For each unique combination with another Lya parameter not covered by the loop yet\n",
    "        for j in range(8 - (i + 1)):\n",
    "\n",
    "            # Make an empty list that will store the aggregate measurements of the parameter pair from all the spectra\n",
    "            measurements = []\n",
    "\n",
    "            # For each slit ID\n",
    "            for k, slit_id in enumerate(slits):\n",
    "\n",
    "                # Append the measurements of the parameter pair to the measurement list\n",
    "                measurements.append(np.loadtxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_measurements.txt', usecols=(i, i + j + 1)))\n",
    "            \n",
    "            # Make the measurements list into an array for convenience and easier handling\n",
    "            measurements = np.array(measurements)\n",
    "\n",
    "            # Make an empty list that will store the aggregate statistical correlations between the measurement pairs\n",
    "            measurement_pair_corr_coefs = []\n",
    "            \n",
    "            #for k, _ in enumerate(measurements):\n",
    "            # For each iteration in the Monte Carlo simulation\n",
    "            for k in range(1000):\n",
    "\n",
    "                # Get all of the measurements of the given measurement pair for that iteration\n",
    "                pairs = measurements[:,k,:]\n",
    "\n",
    "                # Drop any pairs that contain NaNs (i.e., where that measurement wasn't applicable)\n",
    "                pairs = pairs[~np.any(np.isnan(pairs), axis=1)]\n",
    "\n",
    "                # Take the transpose of the pairs to separate the measurements into an 'x' and 'y' column\n",
    "                pairs = pairs.T\n",
    "\n",
    "                # Compute the Pearson and Kendall correlation coefficients\n",
    "                pearson_corr_coef = np.corrcoef(pairs[0], pairs[1])[0,1]\n",
    "                kendall_corr_coef = kendalltau(pairs[0], pairs[1], variant='b').statistic\n",
    "\n",
    "                # Append the correlation coefficients to the aggregate list\n",
    "                measurement_pair_corr_coefs.append([pearson_corr_coef, kendall_corr_coef])\n",
    "\n",
    "            # Append all the statistical correlation measurements for the parameter pair to the aggregate list\n",
    "            corr_coefs.append(measurement_pair_corr_coefs)        \n",
    "\n",
    "        # Make an empty list that will store the randomly sampled measurements of the LyC escape fractions\n",
    "        f_esc_lyc = []\n",
    "\n",
    "        # For each spectrum\n",
    "        for j, _ in enumerate(slits):\n",
    "\n",
    "            # Append the randomly sampled measurements (following a Gaussian distribution\n",
    "            # and assuming that the measured value and associated uncertainty correspond \n",
    "            # to the mean and standard deviation of the distribution) of the LyC escape\n",
    "            # fraction of the slit to the aggregate list\n",
    "            f_esc_lyc.append(np.random.normal(f_esc[j], ne_esc[j], 1000))\n",
    "\n",
    "        # Convert the list to an array and transpose it\n",
    "        f_esc_lyc = np.array(f_esc_lyc).T\n",
    "\n",
    "        # Make an empty list that will store the aggregate statistical correlations between the measurement pairs\n",
    "        measurement_pair_corr_coefs = []\n",
    "\n",
    "        # For each iteration of the Monte Carlo simulation \n",
    "        for j in range(1000):\n",
    "\n",
    "            # Make an array of the measurements organized into pairs by source spectrum\n",
    "            pairs = np.array([measurements[:,k,0], f_esc_lyc[j]]).T\n",
    "\n",
    "            # Remove any pairs that contain any NaNs (i.e., where that measurement wasn't applicable)\n",
    "            pairs = pairs[~np.any(np.isnan(pairs), axis=1)].T\n",
    "\n",
    "            # Calculate the Pearson and Kendall correlation coefficients\n",
    "            pearson_corr_coef = np.corrcoef(pairs[0], pairs[1])[0,1]\n",
    "            kendall_corr_coef = kendalltau(pairs[0], pairs[1], variant='b').statistic\n",
    "\n",
    "            # Append the correlation coefficients to the aggregate list\n",
    "            measurement_pair_corr_coefs.append([pearson_corr_coef, kendall_corr_coef])\n",
    " \n",
    "        # Append all the statistical correlation measurements for the parameter pair to the aggregate list\n",
    "        corr_coefs.append(measurement_pair_corr_coefs)\n",
    "\n",
    "    # Conver the list of correlation coefficient measurements to an array for easier handling\n",
    "    corr_coefs = np.array(corr_coefs)\n",
    "\n",
    "    # Flatten the array so that each set of measurements of a type of correlation coefficient\n",
    "    # for a parameter pair becomes its own column\n",
    "    corr_coefs = corr_coefs.swapaxes(1, 2)\n",
    "    corr_coefs = corr_coefs.reshape(-1, corr_coefs.shape[-1])\n",
    "\n",
    "    # Set the header of the output file of the statistical correlations between the measurement pairs\n",
    "    header = ''\n",
    "\n",
    "    # Save the output file containing the statistical correlations between the measurement pairs\n",
    "    np.savetxt(f'{results}/lya_fits/mc_sim_lya_measurements_statistical_correlations.txt', corr_coefs, header=header, delimiter='\\t', encoding='utf-8')\n",
    "\n",
    "    '''\n",
    "    fwhm_c_results = fwhm_c_results[~np.isnan(fwhm_c_results)]\n",
    "\n",
    "    # If the spectrum is one of the stacked spectra\n",
    "    if i < 2:\n",
    "\n",
    "        results = [vsep_results, fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results]\n",
    "        r = np.array([None,None,None,None,1,1,1])\n",
    "        tau = np.array([None,None,None,None,1,1,1])\n",
    "        rho = np.array([None,None,None,None,1,1,1])\n",
    "\n",
    "    # If the spectrum is not one of the stacked spectra\n",
    "    elif i > 1:\n",
    "\n",
    "        results = [vsep_results, fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results, l_results * 1e-42]\n",
    "        r = np.array([None,None,None,None,1,1,1,1])\n",
    "        tau = np.array([None,None,None,None,1,1,1,1])\n",
    "        rho = np.array([None,None,None,None,1,1,1,1])\n",
    "\n",
    "    # If the spectrum is not a stacked one\n",
    "    if i > 1:\n",
    "        # Create pseudo-measurements of the LyC escape fraction assuming the calculated value and standard deviation\n",
    "        # correspond to a Gaussian mean and standard deviation\n",
    "        fesc_results = np.random.normal(f_esc[i-2], ne_esc[i-2], 1000)\n",
    "\n",
    "    else:\n",
    "        fesc_results = np.empty(1000)\n",
    "        fesc_results[:] = np.nan\n",
    "\n",
    "    slit_result = np.array([np.empty(1000)])\n",
    "\n",
    "    for j, result in enumerate([vsep_results, fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results, l_results * 1e-42, fesc_results]):\n",
    "            \n",
    "        # Try to append any missing values as NaNs\n",
    "        try:\n",
    "            if len(result) < 1000:\n",
    "                a = np.empty(1000 - len(result))\n",
    "                a[:] = np.nan\n",
    "\n",
    "                result = np.append(result, a)\n",
    "\n",
    "        # Unless the array has not been instantiated\n",
    "        except TypeError:\n",
    "            result = np.empty(1000)\n",
    "            result[:] = np.nan\n",
    "            \n",
    "        if (i == 6) and (j == 7):\n",
    "            result = np.empty(1000)\n",
    "            result[:] = np.nan\n",
    "\n",
    "        result = np.where(result != 0.0, result, np.nan)\n",
    "\n",
    "        slit_result = np.append(slit_result, np.array([result]), axis=0)\n",
    "\n",
    "    # Drop the first row since it is empty\n",
    "    slit_result = slit_result[1:]\n",
    "\n",
    "    total_result = np.append(total_result, np.array([slit_result]), axis=0)\n",
    "\n",
    "    total_result = total_result[1:]\n",
    "\n",
    "    r_locs = [[3,0,0,0,0,0,0,0],\n",
    "        [2,4,0,0,0,0,0,0],\n",
    "        [1,1,2,0,0,0,0,0],\n",
    "        [3,3,2,7,0,0,0,0],\n",
    "        [3,3,2,7,2,0,0,0],\n",
    "        [3,3,2,7,2,2,1,1],\n",
    "        [3,3,2,1,2,2,2,1],\n",
    "        [2,'center left',2,4,2,2,2,2]]\n",
    "\n",
    "    # For each row in the corner plot\n",
    "    for i, row in enumerate(ax_c):\n",
    "\n",
    "        # For each column in the corner plot\n",
    "        for j, subplot in enumerate(row):\n",
    "\n",
    "            r_results = np.array([], dtype=np.float64)\n",
    "            tau_results = np.array([], dtype=np.float64)\n",
    "            rho_results = np.array([], dtype=np.float64)\n",
    "\n",
    "            # If the row / column pair is above the main diagonal\n",
    "            if j > i:\n",
    "                pass\n",
    "            else:\n",
    "\n",
    "                x_data = np.empty(1000)\n",
    "                x_data[:] = np.nan\n",
    "                x_data = np.array([x_data])\n",
    "                y_data = np.empty(1000)\n",
    "                y_data[:] = np.nan\n",
    "                y_data = np.array([y_data])\n",
    "\n",
    "                # For each spectrum\n",
    "                for k, slit in enumerate(total_result):\n",
    "                    \n",
    "                    if all(np.isnan(slit[j])) or all(np.isnan(slit[i + 1])):\n",
    "                        pass\n",
    "                    else:\n",
    "\n",
    "                        x_data = np.append(x_data, np.array([slit[j]]), axis=0)\n",
    "                        y_data = np.append(y_data, np.array([slit[i + 1]]), axis=0)\n",
    "\n",
    "                x_data = x_data[1:]\n",
    "                y_data = y_data[1:]\n",
    "\n",
    "\n",
    "                if (i==6) and (j==6):\n",
    "                    pass\n",
    "\n",
    "                for k, sample in enumerate(x_data[0]):\n",
    "\n",
    "                    x = x_data[:,k]\n",
    "                    y = y_data[:,k]\n",
    "\n",
    "                    if any(np.isnan(x)) or any(np.isnan(y)):\n",
    "                        pass\n",
    "                    else:\n",
    "                        x = x[x != 0]\n",
    "                        y = y[y != 0]\n",
    "\n",
    "                        r = np.corrcoef(x, y)[0,1]\n",
    "                        tau = kendalltau(x, y).statistic\n",
    "                        rho = spearmanr(x, y).statistic\n",
    "\n",
    "                        r_results = np.append(r_results, r)\n",
    "                        tau_results = np.append(tau_results, tau)\n",
    "                        rho_results = np.append(rho_results, rho)\n",
    "    '''\n",
    "\n",
    "def plot():\n",
    "\n",
    "    def extract_data(file):\n",
    "\n",
    "        '''\n",
    "        Extract spectrum from the .txt file\n",
    "\n",
    "        Parameters:\n",
    "            file : str\n",
    "                Name of the file\n",
    "\n",
    "        Returns:\n",
    "            w : numpy.ndarray\n",
    "                Observed wavelength bins\n",
    "            f : numpy.ndarray\n",
    "                Observed flux densities\n",
    "            n : numpy.ndarray\n",
    "                Observed Gaussian standard deviation of observed flux densities\n",
    "        '''\n",
    "\n",
    "        # Retrieve the data columns\n",
    "        w, f, n = np.loadtxt(file, delimiter='\\t', comments=('#', 'w'), usecols=(0,1,2), unpack=True)\n",
    "    \n",
    "        # If the file is not the stacked leaker spectrum. This step is necessary because \n",
    "        if 'leaker' not in file:\n",
    "\n",
    "            # Remove bins of extreme outliers\n",
    "            w = w[f < 1e-20]\n",
    "            n = n[f < 1e-20]\n",
    "            f = f[f < 1e-20]\n",
    "\n",
    "            # Convert from units of erg/s/cm^2/Hz to erg/s/cm^2/Å\n",
    "            f = f * 2.998e18 / np.square(w)\n",
    "            n = n * 2.998e18 / np.square(w)\n",
    "\n",
    "        return w, f, n\n",
    "\n",
    "    slits = {\n",
    "        'NL' : ['rest_sba-nonleaker-no_m3_MWdr.txt', 0, 1, [60,140]],\n",
    "        'L' : ['rest_sba-leaker-no_m0_MWdr.txt', 0, 1, [20,130]],\n",
    "        'M5' : ['psz-arcslit-m5-comb1_MWdr.txt', 2.37086, 51, [0,100]],\n",
    "        'M4' : ['psz-arcslit-m4-comb1_MWdr.txt', 2.37073, 14.6, [0,85]],\n",
    "        'M6' : ['psz-arcslit-m6-comb1_MWdr.txt', 2.37021, 147, [10,130]],\n",
    "        'M3' : ['psz-arcslit-m3-comb1_MWdr.txt', 2.37025, 36, [35,120]],\n",
    "        'M0' : ['planckarc_m0-comb1_MWdr.txt', 2.37014, 10, [10,130]],\n",
    "        'M2' : ['psz-arcslit-m2-comb1_MWdr.txt', 2.37017, 32, [45,125]],\n",
    "        'M7' : ['psz-arcslit-m7-comb1_MWdr.txt', 2.37044, 35, [15,100]],\n",
    "        'M8' : ['psz-arcslit-m8-comb1_MWdr.txt', 2.37024, 29, [25,125]],\n",
    "        'M9' : ['psz-arcslit-m9-comb1_MWdr.txt', 2.37030, 31, [15,125]]\n",
    "    }\n",
    "\n",
    "    home = os.getcwd()\n",
    "    data = f'{home}/data'\n",
    "    figs = f'{home}/figs'\n",
    "\n",
    "    fig_lya, ax_lya = plt.subplots(3,3, figsize=(12,12), sharex=True, constrained_layout=True)\n",
    "    ax_lya_array = np.array(ax_lya).reshape(-1)\n",
    "\n",
    "    for i, slit_id in enumerate(list(slits.keys())[2:]):\n",
    "\n",
    "        z = slits[slit_id][1]\n",
    "\n",
    "        w, f, n = extract_data(f'{data}/spectra/mage/{slits[slit_id][0]}')\n",
    "\n",
    "        w, f, n = w / (1 + z), f * (1 + z), n * (1 + z)\n",
    "\n",
    "        ax_lya_array[i].plot(w, f, ds='steps-mid')\n",
    "\n",
    "    fig_lya.savefig(f'{figs}/lya_fits.pdf', bbox_inches='tight')\n",
    "\n",
    "def tabulate():\n",
    "\n",
    "    slits = {\n",
    "        'NL' : [],\n",
    "        'L' : [],\n",
    "        'M5' : [],\n",
    "        'M4' : [],\n",
    "        'M6' : [],\n",
    "        'M3' : [],\n",
    "        'M0' : [],\n",
    "        'M2' : [],\n",
    "        'M7' : [],\n",
    "        'M8' : [],\n",
    "        'M9' : []\n",
    "    }\n",
    "\n",
    "    home = os.getcwd()\n",
    "    results = f'{home}/results'\n",
    "\n",
    "    table = '\\\\begin{deluxetable*}{cllllllll}[ht!]\\n\\n' \\\n",
    "        + '\\\\tablecaption{Ly$\\\\alpha$ measurements \\label{tab:lya_params}}\\n\\n' \\\n",
    "        + '\\\\tablehead{\\n' \\\n",
    "        + '\\t\\colhead{Slit} & \\colhead{$v_{\\\\rm{sep}}$} & \\colhead{FWHM (blue)} & \\colhead{FWHM (center)} & \\colhead{FWHM (red)} & \\colhead{$f_{\\\\rm{min}}/f_{\\\\rm{cont}}$} & \\colhead{EW} & \\colhead{$f_{\\\\rm{cen}}$} & \\colhead{Luminosity}\\n' \\\n",
    "        + '\\t\\\\\\\\\\n' \\\n",
    "        + '\\t\\colhead{} &\\n' \\\n",
    "        + '\\t\\colhead{[km s$^{-1}$]} &\\n' \\\n",
    "        + '\\t\\colhead{[km s$^{-1}$]} &\\n' \\\n",
    "        + '\\t\\colhead{[km s$^{-1}$]} &\\n' \\\n",
    "        + '\\t\\colhead{[km s$^{-1}$]} &\\n' \\\n",
    "        + '\\t\\colhead{} &\\n' \\\n",
    "        + '\\t\\colhead{[\\AA]} &\\n' \\\n",
    "        + '\\t\\colhead{[\\%]} &\\n' \\\n",
    "        + '\\t\\colhead{[$10^{41}$ erg s$^{-1}$]}\\n' \\\n",
    "        + '}\\n\\n' \\\n",
    "        + '\\startdata\\n\\n'\n",
    "\n",
    "    for i, slit_id in enumerate(slits):\n",
    "\n",
    "        slit_lya_measurements = np.loadtxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_measurements.txt', delimiter=' ', comments='#').T\n",
    "\n",
    "        table = table + f'{slit_id} '        \n",
    "\n",
    "        for j, measurements in enumerate(slit_lya_measurements):\n",
    "\n",
    "            median = np.percentile(measurements, 50)\n",
    "            lower = median - np.percentile(measurements, 16)\n",
    "            upper = np.percentile(measurements, 84) - median\n",
    "\n",
    "            median, lower, upper = (median / 1e41, lower / 1e41, upper / 1e41) if j+1 == len(slit_lya_measurements) else (median, lower, upper)\n",
    "\n",
    "            #lower = sigfig.round(lower, sigfigs=1, type=str)\n",
    "            #upper = sigfig.round(upper, sigfigs=1, type=str)\n",
    "\n",
    "            if not np.isnan(median):\n",
    "\n",
    "                lower = sigfig.round(lower, sigfigs=1, type=str)\n",
    "                upper = sigfig.round(upper, sigfigs=1, type=str)\n",
    "\n",
    "                # Get whichever bound is smaller, in order to round the median to\n",
    "                # the digit of that bound's significant figure\n",
    "                ref = min(np.array([lower, upper], dtype=str), key=float)\n",
    "\n",
    "                # If the smaller bound is less than 1\n",
    "                if '.' in ref:\n",
    "\n",
    "                    # Round the median to the same digit as the smaller bound's only significant figure\n",
    "                    median = sigfig.round(median, decimals=len(ref.split('.')[1]), type=str)\n",
    "\n",
    "                # If the smaller bound is greater than 1\n",
    "                elif '.' not in ref:\n",
    "\n",
    "                    # Round the median to the same digit as the smaller bound's only significant figure\n",
    "                    median = sigfig.round(median, len(str(median).split('.')[0]) - len(ref) + 1, type=str)\n",
    "\n",
    "                # If the median or either bound is less than 0.0001 (this is Python's default limit to begin \n",
    "                # printing numbers in scientific notation)\n",
    "                if any('0.0000' in i for i in np.array([median, lower, upper], dtype=str)):\n",
    "\n",
    "                    # Get the smallest quantity of the median and bounds\n",
    "                    ref = min(np.array([median, lower, upper], dtype=str), key=float)\n",
    "\n",
    "                    # Determine the necesessary scientific notation exponent so that the first significant \n",
    "                    # figure of the smallest quantity is the first digit to the left of the decimal place\n",
    "                    exp = -len(ref.split('.')[1])\n",
    "\n",
    "                    # Write the scientific notation factor as a string\n",
    "                    factor = f'\\\\times 10^{{{exp}}}'\n",
    "\n",
    "                    if '.' in median:\n",
    "                        if len(median.split('.')[1]) < abs(exp):\n",
    "                            median += '0' * (abs(exp) - len(median.split('.')[1]))\n",
    "                            median = median.split('.')[0] + median.split('.')[1]\n",
    "                            median = median.lstrip('0')\n",
    "                        elif len(median.split('.')[1]) == abs(exp):\n",
    "                            median = median.split('.')[0] + median.split('.')[1]\n",
    "                            median = median.lstrip('0')\n",
    "                        elif len(median.split('.')[1]) > abs(exp): \n",
    "                            median = median.split('.')[0] + median.split('.')[1][0:abs(exp)] + '.' + median.split('.')[1][abs(exp):]\n",
    "                    elif '.' not in median:\n",
    "                        median += '0' * abs(exp)\n",
    "\n",
    "                    if '.' in lower:\n",
    "                        if len(lower.split('.')[1]) < abs(exp):\n",
    "                            lower += '0' * (abs(exp) - len(lower.split('.')[1]))\n",
    "                            lower = lower.split('.')[0] + lower.split('.')[1]\n",
    "                            lower = lower.lstrip('0')\n",
    "                        elif len(lower.split('.')[1]) == abs(exp):\n",
    "                            lower = lower.split('.')[0] + lower.split('.')[1]\n",
    "                            lower = lower.lstrip('0')\n",
    "                        elif len(lower.split('.')[1]) > abs(exp): \n",
    "                            lower = lower.split('.')[0] + lower.split('.')[1][0:abs(exp)] + '.' + lower.split('.')[1][abs(exp):]\n",
    "                    elif '.' not in lower:\n",
    "                        lower += '0' * abs(exp)\n",
    "                \n",
    "                    if '.' in upper:\n",
    "                        if len(upper.split('.')[1]) < abs(exp):\n",
    "                            upper += '0' * (abs(exp) - len(upper.split('.')[1]))\n",
    "                            upper = upper.split('.')[0] + upper.split('.')[1]\n",
    "                            upper = upper.lstrip('0')\n",
    "                        elif len(upper.split('.')[1]) == abs(exp):\n",
    "                            upper = upper.split('.')[0] + upper.split('.')[1]\n",
    "                            upper = upper.lstrip('0')\n",
    "                        elif len(upper.split('.')[1]) > abs(exp): \n",
    "                            upper = upper.split('.')[0] + upper.split('.')[1][0:abs(exp)] + '.' + upper.split('.')[1][abs(exp):]\n",
    "                    elif '.' not in upper:\n",
    "                        lower += '0' * abs(exp)\n",
    "\n",
    "                table = table + f'& ${median}_{{-{lower}}}^{{+{upper}}}$ '\n",
    "            \n",
    "            else:\n",
    "\n",
    "                table = table + '& $---$ '\n",
    "        \n",
    "        table = table + '\\\\\\\\' + '\\n' if i != len(slits) - 1 else table + '\\n'\n",
    "\n",
    "    table = table + '\\n\\\\enddata\\n\\n' \\\n",
    "        + '\\\\tablecomments{From left to right: slit label, peak separation between the redshifted and blueshifted Ly$\\\\alpha$ peaks ' \\\n",
    "        + '(km s$^{-1}$), FWHM of the blueshifted, central, and redshifted Ly$\\\\alpha$ peaks (km s$^{-1}$), respectively, ratio between ' \\\n",
    "        + 'the `minimum\\' flux density between the redshifted and blueshifted Ly$\\\\alpha$ peaks and the local continuum flux density, ' \\\n",
    "        + 'rest-frame Ly$\\\\alpha$ equivalent width ({\\AA}), central escape fraction (\\%), and magnification-corrected Ly$\\\\alpha$ luminosity ' \\\n",
    "        + '(10$^{41}$ erg s$^{-1}$). Because the deconvolved FWHMs of the central Ly$\\\\alpha$ peaks of slits M4 and M5 were not significantly ' \\\n",
    "        + 'greater than the instrumental line spread function FWHM ($\\sim55$ km s$^{-1}$), we quote the 84th percentiles of those measurements ' \\\n",
    "        + 'as an upper bound on the intrinsic FWHM of their central Ly$\\\\alpha$ peaks.}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\tablenotetext{a}{Slit M0\\'s observation was taken through thin cloud cover that prevented an accurate fluxing, so its ' \\\n",
    "        + 'significantly larger luminosity is not an accurate estimate. We do not include this data point in any figures or when ' \\\n",
    "        + 'estimating any correlations involving the Ly$\\\\alpha$ luminosity. See Table \\\\ref{tab:mage_log} for more information about ' \\\n",
    "        + 'the observation.}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\end{deluxetable*}'\n",
    "\n",
    "    f = open(f'{results}/tables/lya_measurements_table.txt', 'w', encoding='utf-8')\n",
    "    f.write(table)\n",
    "    f.close()\n",
    "\n",
    "    correlations_table = ''\n",
    "\n",
    "    lya_model_parameters_table = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9887ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83b584ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9353af9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x864 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a888e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(file):\n",
    "\n",
    "    '''\n",
    "    Extract spectrum from the .txt file\n",
    "\n",
    "    Parameters:\n",
    "        file : str\n",
    "            Name of the file\n",
    "\n",
    "    Returns:\n",
    "        w : numpy.ndarray\n",
    "            Observed wavelength bins\n",
    "        f : numpy.ndarray\n",
    "            Observed flux densities\n",
    "        n : numpy.ndarray\n",
    "            Observed Gaussian standard deviation of observed flux densities\n",
    "    '''\n",
    "\n",
    "    # Retrieve the data columns\n",
    "    w, f, n = np.loadtxt(file, delimiter='\\t', comments=('#', 'w'), usecols=(0,1,2), unpack=True)\n",
    "    \n",
    "    # If the file is not the stacked leaker spectrum. This step is necessary because \n",
    "    if 'leaker' not in file:\n",
    "\n",
    "        # Remove bins of extreme outliers\n",
    "        w = w[f < 1e-20]\n",
    "        n = n[f < 1e-20]\n",
    "        f = f[f < 1e-20]\n",
    "\n",
    "        # Convert from units of erg/s/cm^2/Hz to erg/s/cm^2/Å\n",
    "        f = f * 2.998e18 / np.square(w)\n",
    "        n = n * 2.998e18 / np.square(w)\n",
    "\n",
    "    return w, f, n    \n",
    "\n",
    "@njit\n",
    "def rest_frame(w, f, n, z):\n",
    "\n",
    "    '''\n",
    "    Place the data in the rest frame\n",
    "\n",
    "    Parameters:\n",
    "        w : numpy.ndarray\n",
    "            Observed wavelength bins\n",
    "        f : numpy.ndarray\n",
    "            Observed flux densities\n",
    "        n : numpy.ndarray\n",
    "            Gaussian standard deviation of observed flux densities\n",
    "        z : numpy.float64\n",
    "            Redshift of the spectrum\n",
    "\n",
    "    Returns:\n",
    "        w : numpy.ndarray\n",
    "            Rest wavelength bins\n",
    "        f : numpy.ndarray\n",
    "            Rest flux densities\n",
    "        n : numpy.ndarray\n",
    "            Gaussian standard deviation of rest flux densities\n",
    "    '''\n",
    "\n",
    "    w = w / (1 + z)\n",
    "    f = f * (1 + z)\n",
    "    n = n * (1 + z)\n",
    "\n",
    "    return w, f, n\n",
    "\n",
    "def compute_continuum(w, f):\n",
    "\n",
    "    '''\n",
    "    Compute the local continuum\n",
    "\n",
    "    Parameters:\n",
    "        w : numpy.ndarray\n",
    "            Rest wavelength\n",
    "        f : numpy.ndarray\n",
    "            Rest flux density\n",
    "\n",
    "    Returns:\n",
    "        c : numpy.float64\n",
    "            Local continuum flux density\n",
    "    '''\n",
    "\n",
    "    # Compute the local continuum as the\n",
    "    # median flux density between 1221-1225 Å\n",
    "    f = f[(w >= 1221) & (w <= 1225)]\n",
    "    c = np.median(f)    \n",
    "\n",
    "    return c\n",
    "\n",
    "@njit\n",
    "def compute_ew(w, f, c, results):\n",
    "\n",
    "    '''\n",
    "    Compute the EW of the Lyα profile\n",
    "\n",
    "    Parameters:\n",
    "        w : numpy.ndarray\n",
    "            Rest wavelength\n",
    "        f : numpy.ndarray\n",
    "            Rest flux density\n",
    "        c : numpy.float64\n",
    "            Local continuum flux density\n",
    "        results : numpy.ndarray\n",
    "            Array containing measurements\n",
    "\n",
    "    Returns:\n",
    "        ew : numpy.float64\n",
    "            Equivalent width\n",
    "    '''\n",
    "\n",
    "    # Compute the EW of the Lyα profile\n",
    "    # between 1212-1221 Å\n",
    "    f = f[(w >= 1212) & (w <= 1221)]\n",
    "    w = w[(w >= 1212) & (w <= 1221)]\n",
    "    ew = -1 * np.trapz(1 - f / c, w)\n",
    "\n",
    "    results = np.append(results, ew)\n",
    "\n",
    "    return results\n",
    "\n",
    "@njit\n",
    "def compute_fcen(v, f, results):\n",
    "\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    # Compute the central escape fraction as a percent\n",
    "    f_cen = np.trapz(f[(v >= -100) & (v <= 100)], v[(v >= -100) & (v <= 100)]) / np.trapz(f[(v >= -1000) & (v <= 1000)], v[(v >= -1000) & (v <= 1000)]) * 100\n",
    "    results = np.append(results, f_cen)\n",
    "\n",
    "    return results\n",
    "\n",
    "def compute_l(w, f_mc, c, z, l_distance, l_results):\n",
    "\n",
    "    f_mc = f_mc[(w >= 1212 * (1 + z)) & ((w <= 1221 * (1 + z)))]\n",
    "    w = w[(w >= 1212 * (1 + z)) & ((w <= 1221 * (1 + z)))]\n",
    "\n",
    "    flux = np.trapz(f_mc - c, w)\n",
    "\n",
    "    l = 4 * np.pi * flux * l_distance**2\n",
    "    l_results = np.append(l_results, l)\n",
    "\n",
    "    return l_results\n",
    "\n",
    "\n",
    "def compute_ratio(v, f_mc, c, c_peak_range, ratio_results):\n",
    "\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    ratio = np.amax(f_mc[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]) / c\n",
    "    ratio_results = np.append(ratio_results, ratio)\n",
    "\n",
    "    return ratio_results\n",
    "\n",
    "def fit_peaks(v, f_mc, c, c_peak_range, model, i):\n",
    "\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    # If there is no blue peak\n",
    "    if i in [0,3,4]:\n",
    "        \n",
    "        # Assign the initial guess and parameter bounds; this must be done inside the Monte Carlo loop to get the best initial\n",
    "        # estimates so they can be built from the random sample\n",
    "        p0 = (np.amax(f_mc[(v >= 150) & (v <= 1000)]),200,20,1,np.amax(f_mc[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]),80,40,c)\n",
    "        bounds = ([0,0,0,0,0,0,0,0],[np.amax(f_mc),1000,np.inf,np.inf,np.amax(f_mc),200,85,np.mean(f_mc)])\n",
    "    \n",
    "    elif i in [2]:\n",
    "\n",
    "        # Assign the initial guess and parameter bounds; this must be done inside the Monte Carlo loop to get the best initial\n",
    "        # estimates so they can be built from the random sample\n",
    "        p0 = (np.amax(f_mc[(v >= 150) & (v <= 1000)]),200,20,1,np.amax(f_mc[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]) - c,50,20,c)\n",
    "        bounds = ([0,0,0,0,0,0,0,0],[np.amax(f_mc),500,np.inf,np.inf,np.amax(f_mc[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]) - c,100,40,np.amax(f_mc)])\n",
    "\n",
    "    # Otherwise\n",
    "    else:\n",
    "\n",
    "        # Assign the initial guess and parameter bounds; this must be done inside the Monte Carlo loop to get the best initial\n",
    "        # estimates so they can be built from the random sample\n",
    "        p0 = (np.amax(f_mc[(v >= -1000) & (v <= 0)]),-150,20,-1,np.amax(f_mc[(v >= 150) & (v <= 1000)]),200,20,1,np.amax(f_mc[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]),80,40,c)\n",
    "        bounds = ([0,-1000,0,-np.inf,0,0,0,0,0,0,0,0],[np.amax(f_mc[(v >= -1000) & (v <= 0)]),0,np.inf,0,np.amax(f_mc),1000,np.inf,np.inf,np.amax(f_mc),200,85,np.mean(f_mc)])\n",
    "\n",
    "    p, cov = curve_fit(model, v, f_mc, p0=p0, bounds=bounds, maxfev=2000)\n",
    "\n",
    "    return p, cov\n",
    "\n",
    "def mc(w, v, f, n, z, c_peak_range, i):\n",
    "\n",
    "    '''\n",
    "    Parameters:\n",
    "        w : numpy.ndarray\n",
    "        f : numpy.ndarray\n",
    "        n : numpy.ndarray\n",
    "    '''\n",
    "\n",
    "    def two_peaks(x, amp_r, cen_r, width_r, skew_r, amp_c, cen_c, width_c, cntm):\n",
    "\n",
    "        return amp_r * np.exp(-((x - cen_r) / width_r)**2 / 2) * (1 + erf(skew_r * ((x - cen_r) / width_r) / np.sqrt(2))) \\\n",
    "            + amp_c * np.exp(-((x - cen_c) / width_c)**2 / 2) \\\n",
    "            + cntm\n",
    "\n",
    "    def three_peaks(x, amp_b, cen_b, width_b, skew_b, amp_r, cen_r, width_r, skew_r, amp_c, cen_c, width_c, cntm):\n",
    "\n",
    "        return amp_b * np.exp(-((x - cen_b) / width_b)**2 / 2) * (1 + erf(skew_b * ((x - cen_b) / width_b) / np.sqrt(2)))  \\\n",
    "            + amp_r * np.exp(-((x - cen_r) / width_r)**2 / 2) * (1 + erf(skew_r * ((x - cen_r) / width_r) / np.sqrt(2))) \\\n",
    "            + amp_c * np.exp(-((x - cen_c) / width_c)**2 / 2) \\\n",
    "            + cntm\n",
    "\n",
    "    cosmology=FlatLambdaCDM(70,0.3)\n",
    "    l_distance = cosmology.luminosity_distance(z).value * 3.086e24\n",
    "\n",
    "    # If there is no blue peak\n",
    "    if i in [0,2,3,4]:\n",
    "        model = two_peaks\n",
    "        fit_results = np.empty((1,8), dtype=np.float64)\n",
    "    \n",
    "    # Otherwise\n",
    "    else:\n",
    "        model = three_peaks\n",
    "        fit_results = np.empty((1,12), dtype=np.float64)\n",
    "\n",
    "    if i > 1:\n",
    "        f = f * 1e17\n",
    "        n = n * 1e17\n",
    "\n",
    "    mag = np.array([1.0, 1.0, 50.7, 14.6, 147.0, 36.1, 10.4, 31.6, 34.6, 29.4, 30.9], dtype=np.float64)\n",
    "\n",
    "    e_results = np.array([], dtype=np.float64)\n",
    "    fcen_results = np.array([], dtype=np.float64)\n",
    "    l_results = np.array([], dtype=np.float64)\n",
    "    ratio_results = np.array([], dtype=np.float64)\n",
    "\n",
    "    for s in range(1000):\n",
    "\n",
    "        f_mc = np.random.normal(f, n)\n",
    "\n",
    "        c = compute_continuum(w, f_mc)\n",
    "\n",
    "        e_results = compute_ew(w, f_mc, c, e_results)\n",
    "\n",
    "        fcen_results = compute_fcen(v, f_mc, fcen_results)\n",
    "\n",
    "        l_results = compute_l(w * (1 + z), f_mc * 1e-17 / (1 + z) / mag[i], c * 1e-17 / (1 + z) / mag[i], z, l_distance, l_results)    \n",
    "\n",
    "        #ratio_results = compute_ratio(v, f_mc, c, c_peak_range, ratio_results)\n",
    "\n",
    "        try:\n",
    "\n",
    "            p, _ = fit_peaks(v[(v >= -1000) & (v <= 1000) & (f_mc >= c)], f_mc[(v >= -1000) & (v <= 1000) & (f_mc >= c)], c, c_peak_range, model, i)\n",
    "            \n",
    "            ratio_results = np.append(ratio_results, p[-4] / p[-1])\n",
    "            \n",
    "            fit_results = np.append(fit_results, np.array([p]), axis=0)\n",
    "\n",
    "            '''\n",
    "            if i==0:\n",
    "\n",
    "                fig, ax = plt.subplots()  \n",
    "\n",
    "                ax.plot(v, f_mc, ds='steps-mid', color='black')\n",
    "                ax.plot(v, model(v, *p), ls='dashed', color='red')\n",
    "                ax.set_xlim(-1000,1000)\n",
    "                plt.show()\n",
    "            '''\n",
    "\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "\n",
    "    # Remove the first empty row that was created\n",
    "    # when the array was initialized\n",
    "    fit_results = fit_results[1:]\n",
    "\n",
    "    for i, parameter in enumerate(fit_results.T):\n",
    "\n",
    "        #print(f'$np.median(parameter)_-{abs(np.median(parameter) - np.percentile(parameter, 16))}r}^r{+{abs(np.percentile(parameter, 84) - np.median(parameter))}r}$')\n",
    "        print('$' + str(np.median(parameter)) + \\\n",
    "            '_{-' + str(sf_round(abs(np.median(parameter) - np.percentile(parameter, 16)),1)) + '}' + \\\n",
    "            '^{+' + str(sf_round(abs(np.percentile(parameter, 84) - np.median(parameter)),1)) + '}' + \\\n",
    "            '$')\n",
    "\n",
    "    return fit_results, e_results, fcen_results, ratio_results, l_results\n",
    "\n",
    "def compute_fwhm_and_peak(amp, cen, width, skew, R, R_error):\n",
    "\n",
    "    '''\n",
    "    Returns:\n",
    "        fwhm : numpy.float64\n",
    "        loc : numpy.float64\n",
    "    '''\n",
    "    def lin_interp(x, y, i, half):\n",
    "        return x[i] + (x[i+1] - x[i]) * ((half - y[i]) / (y[i+1] - y[i]))\n",
    "\n",
    "    def model(x, amp, cen, width, skew):\n",
    "\n",
    "        return amp * np.exp(-((x - cen) / width)**2 / 2) * (1 + erf(skew * ((x - cen) / width) / np.sqrt(2)))\n",
    "\n",
    "    v = np.arange(-1000,1000,0.1)\n",
    "\n",
    "    peak = model(v, amp, cen, width, skew)    \n",
    "\n",
    "    loc = v[np.argmax(peak)]\n",
    "\n",
    "    half = np.amax(peak) / 2.0\n",
    "    signs = np.sign(np.add(peak, -half))\n",
    "    zero_crossings = (signs[0:-2] != signs[1:-1])\n",
    "    zero_crossings_i = np.where(zero_crossings)[0]\n",
    "\n",
    "    fwhm = abs(lin_interp(v, peak, zero_crossings_i[0], half) - lin_interp(v, peak, zero_crossings_i[1], half))\n",
    "\n",
    "    res = np.random.normal(R, R_error)\n",
    "\n",
    "    fwhm = np.sqrt((fwhm)**2 - (res)**2)\n",
    "\n",
    "    return fwhm, loc\n",
    "\n",
    "def label(fig, ax, fig_stack, ax_stack):\n",
    "\n",
    "    slits = np.array(['M5','M4','M6','M3','M0','M2','M7','M8','M9'], dtype=str)    \n",
    "\n",
    "    for i, row in enumerate(ax):\n",
    "\n",
    "        row[-1].yaxis.set_label_position('right')\n",
    "        row[-1].set_ylabel(slits[i], rotation=-90, labelpad=10)\n",
    "\n",
    "    ax_stack[0,6].yaxis.set_label_position('right')\n",
    "    ax_stack[0,6].set_ylabel('NL', rotation=-90, labelpad=10)\n",
    "\n",
    "    ax_stack[1,6].yaxis.set_label_position('right')\n",
    "    ax_stack[1,6].set_ylabel('L', rotation=-90, labelpad=10)\n",
    "\n",
    "    ax[3,0].set_title(r'$v_{\\rm{sep}}$')\n",
    "    ax[3,1].set_title('FWHM (blue)')\n",
    "    ax[0,2].set_title('FWHM (center)')\n",
    "    ax[0,3].set_title('FWHM (red)')\n",
    "    ax[0,4].set_title(r'$f_{\\rm{min}}/f_{\\rm{cont}}$')\n",
    "    ax[0,5].set_title('EW')\n",
    "    ax[0,6].set_title(r'$f_{\\rm{cen}}$')\n",
    "    ax[0,7].set_title('Luminosity')\n",
    "    \n",
    "    ax_stack[1,0].set_title(r'$v_{\\rm{sep}}$')\n",
    "    ax_stack[1,1].set_title('FWHM (blue)')\n",
    "    ax_stack[0,2].set_title('FWHM (center)')\n",
    "    ax_stack[0,3].set_title('FWHM (red)')\n",
    "    ax_stack[0,4].set_title(r'$f_{\\rm{min}}/f_{\\rm{cont}}$')\n",
    "    ax_stack[0,5].set_title('EW')\n",
    "    ax_stack[0,6].set_title(r'$f_{\\rm{cen}}$')\n",
    "\n",
    "    xlabels = np.array(['(km s$^{-1}$)','(km s$^{-1}$)','(km s$^{-1}$)',\n",
    "        '(km s$^{-1}$)','',r'($\\rm{\\AA}$)','(%)','(10$^{42}$ erg s$^{-1}$)'], dtype=str)\n",
    "\n",
    "    for i, column in enumerate(ax.T):\n",
    "\n",
    "        ax[8,i].set_xlabel(xlabels[i])\n",
    "\n",
    "    for i, column in enumerate(ax_stack.T):\n",
    "\n",
    "        ax_stack[1,i].set_xlabel(xlabels[i])\n",
    "\n",
    "    fig.supxlabel(r'$x-\\mu$')\n",
    "    fig.supylabel('Count')\n",
    "\n",
    "    fig_stack.supxlabel(r'$x-\\mu$')\n",
    "    fig_stack.supylabel('Count')\n",
    "\n",
    "def set_ticks(ax, ax_stack):\n",
    "\n",
    "    for i, row in enumerate(ax):\n",
    "\n",
    "        for j, column in enumerate(row):\n",
    "\n",
    "            if j == 7:\n",
    "                ax[i,j].tick_params(bottom=True, left=True, right=False)\n",
    "\n",
    "            else:\n",
    "                ax[i,j].tick_params(bottom=True, left=True, right=True)\n",
    "\n",
    "    ax[0,2].tick_params(labelleft=True)\n",
    "    ax[1,2].tick_params(labelleft=True)\n",
    "    ax[2,2].tick_params(labelleft=True)\n",
    "\n",
    "    for i, row in enumerate(ax_stack):\n",
    "\n",
    "        for j, column in enumerate(row):\n",
    "\n",
    "            if j == 6:\n",
    "                ax_stack[i,j].tick_params(bottom=True, left=True, right=False)\n",
    "\n",
    "            else:\n",
    "                ax_stack[i,j].tick_params(bottom=True, left=True, right=True)\n",
    "\n",
    "    ax_stack[0,2].tick_params(labelleft=True)\n",
    "\n",
    "def disable_plots(ax, ax_stack):\n",
    "\n",
    "    ax_stack[0,0].axis('off')    \n",
    "    ax_stack[0,1].axis('off')    \n",
    "\n",
    "    for i in [0,1,2]:\n",
    "\n",
    "        ax[i,0].axis('off')\n",
    "        ax[i,1].axis('off')\n",
    "\n",
    "def make_corner_plot():\n",
    "\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    fig, ax = plt.subplots(8,8, figsize=(16,16), sharex='col', sharey='row', constrained_layout=True)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for i, row in enumerate(ax):\n",
    "\n",
    "        for j, subplot in enumerate(row):\n",
    "\n",
    "            subplot.set_aspect(1 / subplot.get_data_ratio(), adjustable='box')\n",
    "\n",
    "            if j > i:\n",
    "                ax[i,j].axis('off')\n",
    "    \n",
    "    #ax[0,0].set_aspect(1 / subplot.get_data_ratio(), adjustable='datalim')\n",
    "    #ax_dummy = fig.add_axes(ax[0,0].get_position())\n",
    "\n",
    "    #ax_dummy = ax[0,0].twinx()    \n",
    "    #ax_dummy.set_aspect('equal')\n",
    "\n",
    "    #row_titles = np.array(['FWHM(center)','FWHM (red)',r'$f_{\\rm{min}}/f_{\\rm{cont}}$','EW',r'$f_{\\rm{cen}}$','Luminosity',r'$f_{\\rm{esc}}^{\\rm{LyC}}$'], dtype=str)\n",
    "\n",
    "    row_titles = np.array(['FWHM (blue)', 'FWHM (center)', 'FWHM (red)', \n",
    "        r'$f_{\\rm{min}}/f_{\\rm{cont}}$', 'EW', r'$f_{\\rm{cen}}$',\n",
    "        'Luminosity', r'$f_{\\rm{esc}}^{\\rm{LyC}}$'], dtype=str)\n",
    "    column_titles = np.array([r'$v_{\\rm{sep}}$', 'FWHM (blue)', 'FWHM (center)',\n",
    "        'FWHM (red)', r'$f_{\\rm{min}}/f_{\\rm{cont}}$', 'EW', \n",
    "        r'$f_{\\rm{cen}}$', 'Luminosity'], dtype=str)\n",
    "    column_labels = np.array(['(km s$^{-1}$)', '(km s$^{-1}$)', '(km s$^{-1}$)',\n",
    "        '(km s$^{-1}$)', '', r'($\\rm{\\AA}$)',\n",
    "        '(%)', '(10$^{42}$ erg s$^{-1}$)'], dtype=str)\n",
    "    row_labels = np.array(['(km s$^{-1}$)', '(km s$^{-1}$)', '(km s$^{-1}$)',\n",
    "        '', r'($\\rm{\\AA}$)', '(%)', '(10$^{42}$ erg s$^{-1}$)', '(%)'], dtype=str)\n",
    "\n",
    "    for i, row in enumerate(ax):\n",
    "\n",
    "        ax[i,0].set_ylabel(row_titles[i], fontsize='large')\n",
    "        ax[i,i].set_title(column_titles[i])\n",
    "\n",
    "        ax[7,i].set_xlabel(column_labels[i], fontsize='large')\n",
    "\n",
    "        if i > 0:\n",
    "            ax[i,i].yaxis.set_label_position('right')\n",
    "            ax[i,i].set_ylabel(row_labels[i], fontsize='large')\n",
    "\n",
    "    for i, row in enumerate(ax):\n",
    "\n",
    "        for j, subplot in enumerate(row):\n",
    "\n",
    "            # Subplots on the interior\n",
    "            if ((i != j) & (i != 7) & (j != 0)):\n",
    "                subplot.tick_params(left=True, right=True, bottom=True, top=True,\n",
    "                    labelleft=False, labelright=False, labelbottom=False, labeltop=False, direction='in')\n",
    "\n",
    "            # Subplots on the main diagonal but not the corners\n",
    "            elif ((i == j) & (i != 7) & (j != 0)):\n",
    "                subplot.tick_params(left=True, right=True, bottom=True, top=True,\n",
    "                    labelleft=False, labelright=True, labelbottom=False, labeltop=False, direction='in')\n",
    "            \n",
    "            # Subplots on the left column but not the corners\n",
    "            elif ((j == 0) & (i != 0) & (i != 7)):\n",
    "                subplot.tick_params(left=True, right=True, bottom=True, top=True,\n",
    "                    labelleft=False, labelright=False, labelbottom=False, labeltop=False, direction='in')\n",
    "\n",
    "            # Subplots on the bottom row but not the corners\n",
    "            elif ((i == 7) & (j != 0) & (j != 7)):\n",
    "                subplot.tick_params(left=True, right=True, bottom=True, top=True,\n",
    "                    labelleft=False, labelright=False, labelbottom=True, labeltop=False, direction='in')\n",
    "\n",
    "    ax[0,0].tick_params(left=True, right=True, bottom=True, top=True,\n",
    "        labelleft=False, labelright=False, labelbottom=False, labeltop=False, direction='in')\n",
    "    ax[7,0].tick_params(left=True, right=True, bottom=True, top=True,\n",
    "        labelleft=False, labelright=False, labelbottom=True, labeltop=False, direction='in')\n",
    "    ax[7,7].tick_params(left=True, right=True, bottom=True, top=True,\n",
    "        labelleft=False, labelright=True, labelbottom=True, labeltop=False, direction='in')\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "def gaussian(x, amp, cen, width):\n",
    "    return amp * np.exp(-((x - cen) / width)**2 / 2)\n",
    "\n",
    "def skewed_gaussian(x, amp, cen, width, skew):\n",
    "    return amp * np.exp(-((x - cen) / width)**2 / 2) * (1 + erf(skew * ((x - cen) / width) / np.sqrt(2)))\n",
    "\n",
    "def measure():\n",
    "\n",
    "    # Establish directories\n",
    "    home = os.getcwd()\n",
    "    data = home + '/data'\n",
    "    figs = home + '/figs'\n",
    "\n",
    "    #files = glob.glob(data + '/spectra/mage/*MWdr.txt')\n",
    "\n",
    "    files = np.array(['rest_sba-nonleaker-no_m3_MWdr.txt','rest_sba-leaker-no_m0_MWdr.txt',\n",
    "                      'psz-arcslit-h3-comb1_MWdr.txt', 'psz-arcslit-h1-comb1_MWdr.txt',\n",
    "                      'sunburst_M-6-comb1_MWdr.txt', 'psz-arcslit-h4-comb1_MWdr.txt',\n",
    "                      'planckarc_pos1-comb1_MWdr.txt', 'psz-arcslit-h6-comb1_MWdr.txt',\n",
    "                      'psz-arcslit-h9-comb1_MWdr.txt', 'psz-arcslit-f-comb1_MWdr.txt',\n",
    "                      'psz-arcslit-h2-comb1_MWdr.txt'], dtype=object)\n",
    "    names = np.array(['NL', 'L', 'M5', 'M4', 'M6', 'M3', 'M0', 'M2', 'M7', 'M8', 'M9'], dtype=str)\n",
    "\n",
    "    f_esc = [2.3, -0.6, 3, 2.3, 17, 18, 12, 15, 14]\n",
    "    ne_esc = [0.8, 0.2, 1, 0.8, 6, 7, 5, 6, 5]\n",
    "\n",
    "    # For each file\n",
    "    for i, file in enumerate(files):\n",
    "\n",
    "        # Join its folder path to the file name\n",
    "        file = ''.join([data, '/spectra/mage/', file])\n",
    "        files[i] = file\n",
    "\n",
    "    z = np.array([0, 0, 2.37086, 2.37073, 2.37021, 2.37025, 2.37014, 2.37017, 2.37044, \n",
    "                  2.37024, 2.37030], dtype=np.float64)\n",
    "    c_peak_range = np.array([[60,140],[20,130],[0,100],[0,85],[10,130],[35,120],[10,130],\n",
    "                             [45,125],[15,100],[25,125],[15,125]], dtype=np.float64)\n",
    "\n",
    "    fig_mc, ax_mc = plt.subplots(9,8, figsize=(16,18), sharey='row', constrained_layout=True)\n",
    "    fig_mc_stack, ax_mc_stack = plt.subplots(2,7, figsize=(21,6), sharey='row', constrained_layout=True)\n",
    "\n",
    "    fig_lya, ax_lya = plt.subplots(3,3, figsize=(12,12), sharex=True, constrained_layout=True)\n",
    "    ax_lya_array = np.array(ax_lya).reshape(-1)\n",
    "\n",
    "    fig_lya_stack, ax_lya_stack = plt.subplots(3,1, figsize=(3,9), sharex=True)#, constrained_layout=True)\n",
    "    #fig_lya_stack.subplots_adjust(hspace=0, wspace=0)\n",
    "\n",
    "    fig_c, ax_c = make_corner_plot()\n",
    "\n",
    "    total_result = np.array([np.empty((9,1000))])\n",
    "\n",
    "    mag = np.array([1.0, 1.0, 50.7, 14.6, 147.0, 36.1, 10.4, 31.6, 34.6, 29.4, 30.9], dtype=np.float64)\n",
    "\n",
    "    R = np.array([\n",
    "        299792.458 / 5400,\n",
    "        299792.458 / 5300,\n",
    "        299792.458 / 5500,\n",
    "        299792.458 / 5400,\n",
    "        299792.458 / 5300,\n",
    "        299792.458 / 5500,\n",
    "        299792.458 / 4700,\n",
    "        299792.458 / 5300,\n",
    "        299792.458 / 5200,\n",
    "        299792.458 / 5200,\n",
    "        299792.458 / 5500\n",
    "    ])\n",
    "\n",
    "    R_error = np.array([\n",
    "        299792.458 / 5400**2 * 200,\n",
    "        299792.458 / 5300**2 * 200,\n",
    "        299792.458 / 5500**2 * 400,\n",
    "        299792.458 / 5400**2 * 300,\n",
    "        299792.458 / 5300**2 * 300,\n",
    "        299792.458 / 5500**2 * 400,\n",
    "        299792.458 / 4700**2 * 200,\n",
    "        299792.458 / 5300**2 * 300,\n",
    "        299792.458 / 5200**2 * 200,\n",
    "        299792.458 / 5200**2 * 300,\n",
    "        299792.458 / 5500**2 * 400\n",
    "    ])\n",
    "\n",
    "    # For each spectrum\n",
    "    for i, file in enumerate(files):\n",
    "\n",
    "        print(names[i])\n",
    "\n",
    "        # Extract the data from the .txt file\n",
    "        w, f, n = extract_data(file)\n",
    "\n",
    "        # Place the data in the rest frame\n",
    "        w, f, n = rest_frame(w, f, n, z[i])\n",
    "\n",
    "        f = f[(w >= 1195) & (w <= 1235)]\n",
    "        n = n[(w >= 1195) & (w <= 1235)]\n",
    "        w = w[(w >= 1195) & (w <= 1235)]\n",
    "\n",
    "        v = 299792.458 * (w / 1215.67 - 1)\n",
    "\n",
    "        #cosmology = FlatLambdaCDM(70, 0.3)\n",
    "        #l_distance = cosmology.luminosity_distance(z[i]).value * 3.086e24\n",
    "\n",
    "        fit_results, e_results, fcen_results, ratio_results, l_results = mc(w, v, f, n, z[i], c_peak_range[i], i)    \n",
    "\n",
    "        median_fit = [np.median(e) for e in fit_results.T]\n",
    "\n",
    "        # Take the transpose of the results so that the rows\n",
    "        # are by parameter, not sample number\n",
    "        #fit_results = fit_results.T\n",
    "\n",
    "        if i in [0,2,3,4]:\n",
    "            stdv_c_results = fit_results.T[6]\n",
    "            fwhm_c_results = 2 * np.sqrt(2 * np.log(2)) * stdv_c_results\n",
    "\n",
    "            fwhm_b_results = np.array([], dtype=np.float64)\n",
    "            fwhm_c_results = np.array([], dtype=np.float64)\n",
    "            fwhm_r_results = np.array([], dtype=np.float64)\n",
    "            vsep_results = np.array([], dtype=np.float64)\n",
    "\n",
    "            for j, sample in enumerate(fit_results):\n",
    "\n",
    "                fwhm_r, _ = compute_fwhm_and_peak(sample[0], sample[1], sample[2], sample[3], R[i], R_error[i])\n",
    "                fwhm_r_results = np.append(fwhm_r_results, fwhm_r)\n",
    "\n",
    "                fwhm_c, _ = compute_fwhm_and_peak(sample[4], sample[5], sample[6], 0, R[i], R_error[i])\n",
    "                fwhm_c_results = np.append(fwhm_c_results, fwhm_c)\n",
    "\n",
    "                if np.isnan(fwhm_c):\n",
    "                    print(i, fwhm_c)\n",
    "\n",
    "        else:\n",
    "            stdv_c_results = fit_results.T[10]\n",
    "            fwhm_c_results = 2 * np.sqrt(2 * np.log(2)) * stdv_c_results\n",
    "\n",
    "            fwhm_b_results = np.array([], dtype=np.float64)\n",
    "            fwhm_c_results = np.array([], dtype=np.float64)\n",
    "            fwhm_r_results = np.array([], dtype=np.float64)\n",
    "            vsep_results = np.array([], dtype=np.float64)\n",
    "\n",
    "            for j, sample in enumerate(fit_results):\n",
    "\n",
    "                fwhm_b, loc_b = compute_fwhm_and_peak(sample[0], sample[1], sample[2], sample[3], R[i], R_error[i])\n",
    "                fwhm_b_results = np.append(fwhm_b_results, fwhm_b)\n",
    "\n",
    "                fwhm_c, _ = compute_fwhm_and_peak(sample[8], sample[9], sample[10], 0, R[i], R_error[i])\n",
    "                fwhm_c_results = np.append(fwhm_c_results, fwhm_c)\n",
    "\n",
    "                if np.isnan(fwhm_c):\n",
    "                    print(i, fwhm_c)\n",
    "\n",
    "                fwhm_r, loc_r = compute_fwhm_and_peak(sample[4], sample[5], sample[6], sample[7], R[i], R_error[i])\n",
    "                fwhm_r_results = np.append(fwhm_r_results, fwhm_r)\n",
    "\n",
    "                vsep = abs(loc_r - loc_b)\n",
    "                vsep_results = np.append(vsep_results, vsep)\n",
    "\n",
    "        fwhm_c_results = fwhm_c_results[~np.isnan(fwhm_c_results)]\n",
    "\n",
    "        # If the spectrum is one of the stacked spectra\n",
    "        if i < 2:\n",
    "\n",
    "            if i == 0:\n",
    "                color='#DC3220'\n",
    "                markersize = 8\n",
    "            else:\n",
    "                color='#005AB5'\n",
    "                markersize = 8\n",
    "\n",
    "            ax_mc_stack[i,0].hist(vsep_results - np.median(vsep_results), bins=30, color=color)\n",
    "            ax_mc_stack[i,1].hist(fwhm_b_results - np.median(fwhm_b_results), bins=30, color=color)\n",
    "            ax_mc_stack[i,2].hist(fwhm_c_results - np.median(fwhm_c_results), bins=30, color=color)\n",
    "            ax_mc_stack[i,3].hist(fwhm_r_results - np.median(fwhm_r_results), bins=30, color=color)\n",
    "            ax_mc_stack[i,4].hist(ratio_results - np.median(ratio_results), bins=30, color=color)\n",
    "            ax_mc_stack[i,5].hist(e_results - np.median(e_results), bins=30, color=color)\n",
    "            ax_mc_stack[i,6].hist(fcen_results - np.median(fcen_results), bins=30, color=color)\n",
    "\n",
    "            results = [vsep_results, fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results]\n",
    "            r = np.array([None,None,None,None,1,1,1])\n",
    "            tau = np.array([None,None,None,None,1,1,1])\n",
    "            rho = np.array([None,None,None,None,1,1,1])\n",
    "\n",
    "            # For each subplot in a row\n",
    "            for j, subplot in enumerate(ax_mc_stack[i]):\n",
    "\n",
    "                try:\n",
    "\n",
    "                    median = np.median(results[j][~np.isnan(results[j])])\n",
    "                    lower = sigfig.round(str(median - np.percentile(results[j][~np.isnan(results[j])], 16)), 2)\n",
    "                    upper = sigfig.round(str(np.percentile(results[j][~np.isnan(results[j])], 84) - median), 2)\n",
    "\n",
    "                    # If both percentiles are greater than 10, they will not have\n",
    "                    # any significant figures past the decimal place\n",
    "                    if abs(float(lower)) >= 10 and abs(float(upper)) >= 10:\n",
    "                        # Find the smallest value between the two percentiles\n",
    "                        string = str(np.amin([int(lower), int(upper)]))\n",
    "\n",
    "                        # Count the number of zeros in the string\n",
    "                        zeros = string.count('0')\n",
    "\n",
    "                        # If the length of the string differs by only one from\n",
    "                        # the number of zeros, this indicates one of the 2 \n",
    "                        # significant figures is a zero, so to prevent the median\n",
    "                        # from being rounded prematurely (to the significant figure\n",
    "                        # before the zero), subtract the number of zeros by one\n",
    "                        if len(string) == zeros + 1:\n",
    "                            zeros = zeros - 1\n",
    "\n",
    "                        zeros = -1 * zeros\n",
    "\n",
    "                        median = str(int(round(median, zeros)))\n",
    "\n",
    "                    elif (abs(float(lower)) >= 10 and abs(float(upper)) < 10) or (abs(float(lower)) < 10 and abs(float(upper)) >= 10):\n",
    "                        print('check')\n",
    "\n",
    "                        if abs(float(lower)) >= 10:\n",
    "                            median = str(round(median, len(upper.split('.')[1])))\n",
    "                        if abs(float(upper)) >= 10:\n",
    "                            median = str(round(median, len(lower.split('.')[1])))\n",
    "                        # Round the median to the smallest digit place between the upper and lower percentiles   \n",
    "                        #median = str(round(median, np.amax([len(lower.split('.')[1]), len(upper.split('.')[1])])))\n",
    "\n",
    "                    # Otherwise, since the percentiles are rounded to 2 significant\n",
    "                    # figures, they will have at least one significant digit past\n",
    "                    # the decimal place\n",
    "                    else:\n",
    "                        # Round the median to the smallest digit place between the upper and lower percentiles   \n",
    "                        median = str(round(median, np.amax([len(lower.split('.')[1]), len(upper.split('.')[1])])))\n",
    "\n",
    "                    at = AnchoredText('$' + median + '_{-' + lower + '}^{+' + upper + '}$',\n",
    "                        loc='upper right', frameon=False)\n",
    "                    subplot.add_artist(at)\n",
    "\n",
    "                # Except if there are no results for the parameter\n",
    "                # (because it was inapplicable to the profile)\n",
    "                except IndexError:\n",
    "                    pass\n",
    "\n",
    "            ax_lya_stack[0].fill_between(v[(v >= -1100) & (v <= 1100)], (f - 3 * n)[(v >= -1100) & (v <= 1100)], (f + 3 * n)[(v >= -1100) & (v <= 1100)], step='mid', facecolor=color, alpha=0.3)\n",
    "            ax_lya_stack[0].plot(v[(v >= -1100) & (v <= 1100)], f[(v >= -1100) & (v <= 1100)], ds='steps-mid', c=color)\n",
    "\n",
    "            for j, result_row in enumerate([fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results]):\n",
    "                \n",
    "                # If this is the stacked nonleaker spectrum\n",
    "                if i == 0:\n",
    "                    if j == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        for k, result_col in enumerate([fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results]):\n",
    "                            \n",
    "                            if k + 2 > j:\n",
    "                                pass\n",
    "                            else:\n",
    "                                ax_c[j,k+2].errorbar(np.median(result_col), np.median(result_row), \n",
    "                                    xerr=[[np.median(result_col) - np.percentile(result_col, 16)], [np.percentile(result_col, 84) - np.median(result_col)]], \n",
    "                                    yerr=[[np.median(result_row) - np.percentile(result_row, 16)], [np.percentile(result_row, 84) - np.median(result_row)]],\n",
    "                                    lw=1, marker='s', markersize=markersize, mec=color, mfc=color, ecolor=color)\n",
    "                else:\n",
    "                    for k, col in enumerate([vsep_results, fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results]):\n",
    "\n",
    "                        if k > j:\n",
    "                            pass\n",
    "                        else:\n",
    "                            ax_c[j,k].errorbar(np.median(col), np.median(result_row),\n",
    "                                xerr=[[np.median(col) - np.percentile(col, 16)], [np.percentile(col, 84) - np.median(col)]], \n",
    "                                yerr=[[np.median(result_row) - np.percentile(result_row, 16)], [np.percentile(result_row, 84) - np.median(result_row)]],\n",
    "                                lw=1, marker='o', markersize=markersize, mec=color, mfc=color, ecolor=color)\n",
    "\n",
    "            if i == 0:\n",
    "\n",
    "                ax_lya_stack[1].fill_between(v[(v >= -1100) & (v <= 1100)], (f - 3 * n)[(v >= -1100) & (v <= 1100)], (f + 3 * n)[(v >= -1100) & (v <= 1100)], step='mid', facecolor=color, alpha=0.3)\n",
    "                ax_lya_stack[1].plot(v[(v >= -1100) & (v <= 1100)], f[(v >= -1100) & (v <= 1100)], ds='steps-mid', c=color)\n",
    "                ax_lya_stack[1].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (gaussian(v, median_fit[4], median_fit[5], median_fit[6]) + median_fit[7])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_stack[1].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (skewed_gaussian(v, median_fit[0], median_fit[1], median_fit[2], median_fit[3]) + median_fit[7])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_stack[1].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (gaussian(v, median_fit[4], median_fit[5], median_fit[6]) + skewed_gaussian(v, median_fit[0], median_fit[1], median_fit[2], median_fit[3]) + median_fit[7])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dashed')\n",
    "\n",
    "            else:\n",
    "\n",
    "                ax_lya_stack[2].fill_between(v[(v >= -1100) & (v <= 1100)], (f - 3 * n)[(v >= -1100) & (v <= 1100)], (f + 3 * n)[(v >= -1100) & (v <= 1100)], step='mid', facecolor=color, alpha=0.3)\n",
    "                ax_lya_stack[2].plot(v[(v >= -1100) & (v <= 1100)], f[(v >= -1100) & (v <= 1100)], ds='steps-mid', c=color)\n",
    "                ax_lya_stack[2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (gaussian(v, median_fit[8], median_fit[9], median_fit[10]) + median_fit[11])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_stack[2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (skewed_gaussian(v, median_fit[0], median_fit[1], median_fit[2], median_fit[3]) + median_fit[11])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_stack[2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (skewed_gaussian(v, median_fit[4], median_fit[5], median_fit[6], median_fit[7]) + median_fit[11])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_stack[2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (gaussian(v, median_fit[8], median_fit[9], median_fit[10]) + skewed_gaussian(v, median_fit[4], median_fit[5], median_fit[6], median_fit[7]) + skewed_gaussian(v, median_fit[0], median_fit[1], median_fit[2], median_fit[3]) + median_fit[11])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dashed')\n",
    "\n",
    "        # If the spectrum is not one of the stacked spectra\n",
    "        elif i > 1:\n",
    "\n",
    "            if i in [2,3,4]:\n",
    "                color='#DC3220'\n",
    "                marker = 's'\n",
    "                markersize = 8\n",
    "\n",
    "            elif i == 5:\n",
    "                color='#D35FB7'\n",
    "                marker = '*'\n",
    "                markersize = 10\n",
    "\n",
    "            else:\n",
    "                color='#005AB5'\n",
    "                marker = 'o'\n",
    "                markersize = 8\n",
    "\n",
    "            ax_mc[i-2,0].hist(vsep_results - np.median(vsep_results), bins=30, color=color)\n",
    "            ax_mc[i-2,1].hist(fwhm_b_results - np.median(fwhm_b_results), bins=30, color=color)\n",
    "            ax_mc[i-2,2].hist(fwhm_c_results - np.median(fwhm_c_results), bins=30, color=color)\n",
    "            ax_mc[i-2,3].hist(fwhm_r_results - np.median(fwhm_r_results), bins=30, color=color)\n",
    "            ax_mc[i-2,4].hist(ratio_results - np.median(ratio_results), bins=30, color=color)\n",
    "            ax_mc[i-2,5].hist(e_results - np.median(e_results), bins=30, color=color)\n",
    "            ax_mc[i-2,6].hist(fcen_results - np.median(fcen_results), bins=30, color=color)\n",
    "            ax_mc[i-2,7].hist((l_results - np.median(l_results)) * 1e-42, bins=30, color=color)\n",
    "            \n",
    "            results = [vsep_results, fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results, l_results * 1e-42]\n",
    "            r = np.array([None,None,None,None,1,1,1,1])\n",
    "            tau = np.array([None,None,None,None,1,1,1,1])\n",
    "            rho = np.array([None,None,None,None,1,1,1,1])\n",
    "\n",
    "            # For each subplot in a row\n",
    "            for j, subplot in enumerate(ax_mc[i-2]):\n",
    "\n",
    "                try:\n",
    "\n",
    "                    #median = np.median(results[j])\n",
    "                    #lower = median - np.percentile(results[j], 16)\n",
    "                    #upper = np.percentile(results[j], 84) - median\n",
    "\n",
    "                    median = np.median(results[j][~np.isnan(results[j])])\n",
    "                    lower = sigfig.round(str(median - np.percentile(results[j][~np.isnan(results[j])], 16)), 2)\n",
    "                    upper = sigfig.round(str(np.percentile(results[j][~np.isnan(results[j])], 84) - median), 2)\n",
    "\n",
    "                    #print(median, lower, upper)\n",
    "\n",
    "                    # If both percentiles are greater than 10, they will not have\n",
    "                    # any significant figures past the decimal place\n",
    "                    if abs(float(lower)) >= 10 and abs(float(upper)) >= 10:\n",
    "                        # Find the smallest value between the two percentiles\n",
    "                        string = str(np.amin([int(lower), int(upper)]))\n",
    "\n",
    "                        # Count the number of zeros in the string\n",
    "                        zeros = string.count('0')\n",
    "\n",
    "                        # If the length of the string differs by only one from\n",
    "                        # the number of zeros, this indicates one of the 2 \n",
    "                        # significant figures is a zero, so to prevent the median\n",
    "                        # from being rounded prematurely (to the significant figure\n",
    "                        # before the zero), subtract the number of zeros by one\n",
    "                        if len(string) == zeros + 1:\n",
    "                            zeros = zeros - 1\n",
    "\n",
    "                        zeros = -1 * zeros\n",
    "\n",
    "                        median = str(int(round(median, zeros)))\n",
    "                    \n",
    "                    elif (abs(float(lower)) >= 10 and abs(float(upper)) < 10) or (abs(float(lower)) < 10 and abs(float(upper)) >= 10):\n",
    "                        print('check')\n",
    "                        \n",
    "                        if abs(float(lower)) >= 10:\n",
    "                            formatter = '{:.' + str(len(upper.split('.')[1])) + 'f}'\n",
    "                            median = formatter.format(round(median, len(upper.split('.')[1])))\n",
    "                        if abs(float(upper)) >= 10:\n",
    "                            formatter = '{:.' + str(len(lower.split('.')[1])) + 'f}'\n",
    "                            median = formatter.format(round(median, len(lower.split('.')[1])))\n",
    "                        # Round the median to the smallest digit place between the upper and lower percentiles   \n",
    "                        #median = str(round(median, np.amax([len(lower.split('.')[1]), len(upper.split('.')[1])])))\n",
    "\n",
    "                    # Otherwise, since the percentiles are rounded to 2 significant\n",
    "                    # figures, they will have at least one significant digit past\n",
    "                    # the decimal place\n",
    "                    else:\n",
    "                        # Round the median to the smallest digit place between the upper and lower percentiles   \n",
    "                        formatter = '{:.' + str(np.amax([len(lower.split('.')[1]), len(upper.split('.')[1])])) + 'f}'\n",
    "                        median = formatter.format(round(median, np.amax([len(lower.split('.')[1]), len(upper.split('.')[1])])))\n",
    "\n",
    "                    #at = AnchoredText('$' + str(round(median, r[j])) + '_{-' + str(round(lower, r[j])) + '}^{+' + str(round(upper, r[j])) + '}$',\n",
    "                    #    loc='upper right', frameon=False)\n",
    "                    at = AnchoredText('$' + median + '_{-' + lower + '}^{+' + upper + '}$',\n",
    "                        loc='upper right', frameon=False)\n",
    "                    subplot.add_artist(at)\n",
    "\n",
    "                # Except if there are no results for the parameter\n",
    "                # (because it was inapplicable to the profile)\n",
    "                except IndexError:\n",
    "                    pass\n",
    "\n",
    "            ax_lya_array[i-2].fill_between(v[(v >= -1100) & (v <= 1100)], (f / mag[i] - 2 * n / mag[i])[(v >= -1100) & (v <= 1100)] * 1e17, (f / mag[i] + 2 * n / mag[i])[(v >= -1100) & (v <= 1100)] * 1e17, step='mid', facecolor=color, alpha=0.3)\n",
    "            ax_lya_array[i-2].plot(v[(v >= -1100) & (v <= 1100)], (f / mag[i])[(v >= -1100) & (v <= 1100)] * 1e17, ds='steps-mid', c=color)\n",
    "\n",
    "            if i in [2,3,4]:\n",
    "\n",
    "                ax_lya_array[i-2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (gaussian(v, median_fit[4] / mag[i], median_fit[5], median_fit[6]) + median_fit[7] / mag[i])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_array[i-2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (skewed_gaussian(v, median_fit[0] / mag[i], median_fit[1], median_fit[2], median_fit[3]) + median_fit[7] / mag[i])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_array[i-2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (gaussian(v, median_fit[4] / mag[i], median_fit[5], median_fit[6]) + skewed_gaussian(v, median_fit[0] / mag[i], median_fit[1], median_fit[2], median_fit[3]) + median_fit[7] / mag[i])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dashed')\n",
    "\n",
    "            else:\n",
    "\n",
    "                ax_lya_array[i-2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (gaussian(v, median_fit[8] / mag[i], median_fit[9], median_fit[10]) + median_fit[11] / mag[i])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_array[i-2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (skewed_gaussian(v, median_fit[0] / mag[i], median_fit[1], median_fit[2], median_fit[3]) + median_fit[11] / mag[i])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_array[i-2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (skewed_gaussian(v, median_fit[4] / mag[i], median_fit[5], median_fit[6], median_fit[7]) + median_fit[11] / mag[i])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_array[i-2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (gaussian(v, median_fit[8] / mag[i], median_fit[9], median_fit[10]) + skewed_gaussian(v, median_fit[4] / mag[i], median_fit[5], median_fit[6], median_fit[7]) + skewed_gaussian(v, median_fit[0] / mag[i], median_fit[1], median_fit[2], median_fit[3]) + median_fit[11] / mag[i])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dashed')\n",
    "\n",
    "            at = AnchoredText(names[i], loc='upper right', frameon=False, prop=dict(fontsize='large'))\n",
    "            ax_lya_array[i-2].add_artist(at)\n",
    "\n",
    "            ax_lya_array[i-2].set_xlim(-1000,1000)\n",
    "            ax_lya_array[i-2].set_ylim(bottom=0)\n",
    "\n",
    "            # For each parameter row\n",
    "            for j, row in enumerate([fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results, l_results * 1e-42, f_esc[i-2]]):\n",
    "                \n",
    "                # If the spectrum does not have a triple peak\n",
    "                if i in [2,3,4]:\n",
    "                    \n",
    "                    # For each parameter column\n",
    "                    for k, col in enumerate([fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results, l_results * 1e-42]):\n",
    "                        if k + 2 > j:\n",
    "                            pass\n",
    "                        else:\n",
    "                            if j == 0:\n",
    "                                pass\n",
    "                            elif j == 7:\n",
    "                                ax_c[j,k+2].errorbar(np.median(col), f_esc[i-2],\n",
    "                                    xerr=[[np.median(col) - np.percentile(col, 16)], [np.percentile(col, 84) - np.median(col)]], \n",
    "                                    yerr=ne_esc[i-2],\n",
    "                                    lw=1, marker=marker, markersize=markersize, mec=color, mfc='none', ecolor=color)\n",
    "                            else:\n",
    "                                ax_c[j,k+2].errorbar(np.median(col), np.median(row),\n",
    "                                    xerr=[[np.median(col) - np.percentile(col, 16)], [np.percentile(col, 84) - np.median(col)]], \n",
    "                                    yerr=[[np.median(row) - np.percentile(row, 16)], [np.percentile(row, 84) - np.median(row)]],\n",
    "                                    lw=1, marker=marker, markersize=markersize, mec=color, mfc='none', ecolor=color)\n",
    "                else:\n",
    "                    for k, col in enumerate([vsep_results, fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results, l_results * 1e-42]):\n",
    "                        \n",
    "                        # If the row / column combination is above the main diagonal\n",
    "                        if k > j:\n",
    "                            pass\n",
    "\n",
    "                        else:\n",
    "                            if (j == 7) and (i != 6):\n",
    "                                ax_c[j,k].errorbar(np.median(col), f_esc[i-2],\n",
    "                                    xerr=[[np.median(col) - np.percentile(col, 16)], [np.percentile(col, 84) - np.median(col)]], \n",
    "                                    yerr=ne_esc[i-2],\n",
    "                                    lw=1, marker=marker, markersize=markersize, mec=color, mfc='none', ecolor=color)\n",
    "                            elif (j == 7) and (k != 7) and (i == 6):\n",
    "                                ax_c[j,k].errorbar(np.median(col), f_esc[i-2],\n",
    "                                    xerr=[[np.median(col) - np.percentile(col, 16)], [np.percentile(col, 84) - np.median(col)]], \n",
    "                                    yerr=ne_esc[i-2],\n",
    "                                    lw=1, marker=marker, markersize=markersize, mec=color, mfc='none', ecolor=color)\n",
    "                            elif (j == 6 or k == 7) and (i == 6):\n",
    "                                pass\n",
    "                            else:\n",
    "                                ax_c[j,k].errorbar(np.median(col), np.median(row),\n",
    "                                    xerr=[[np.median(col) - np.percentile(col, 16)], [np.percentile(col, 84) - np.median(col)]], \n",
    "                                    yerr=[[np.median(row) - np.percentile(row, 16)], [np.percentile(row, 84) - np.median(row)]],\n",
    "                                    lw=1, marker=marker, markersize=markersize, mec=color, mfc='none', ecolor=color)\n",
    "\n",
    "        # If the spectrum is not a stacked one\n",
    "        if i > 1:\n",
    "            # Create pseudo-measurements of the LyC escape fraction assuming the calculated value and standard deviation\n",
    "            # correspond to a Gaussian mean and standard deviation\n",
    "            fesc_results = np.random.normal(f_esc[i-2], ne_esc[i-2], 1000)\n",
    "\n",
    "        else:\n",
    "            fesc_results = np.empty(1000)\n",
    "            fesc_results[:] = np.nan\n",
    "\n",
    "        slit_result = np.array([np.empty(1000)])\n",
    "\n",
    "        for j, result in enumerate([vsep_results, fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results, l_results * 1e-42, fesc_results]):\n",
    "            \n",
    "            # Try to append any missing values as NaNs\n",
    "            try:\n",
    "                if len(result) < 1000:\n",
    "                    a = np.empty(1000 - len(result))\n",
    "                    a[:] = np.nan\n",
    "\n",
    "                    result = np.append(result, a)\n",
    "\n",
    "            # Unless the array has not been instantiated\n",
    "            except TypeError:\n",
    "                result = np.empty(1000)\n",
    "                result[:] = np.nan\n",
    "            \n",
    "            if (i == 6) and (j == 7):\n",
    "                result = np.empty(1000)\n",
    "                result[:] = np.nan\n",
    "\n",
    "            result = np.where(result != 0.0, result, np.nan)\n",
    "\n",
    "            slit_result = np.append(slit_result, np.array([result]), axis=0)\n",
    "\n",
    "        # Drop the first row since it is empty\n",
    "        slit_result = slit_result[1:]\n",
    "\n",
    "        total_result = np.append(total_result, np.array([slit_result]), axis=0)\n",
    "\n",
    "    total_result = total_result[1:]\n",
    "\n",
    "    r_locs = [[3,0,0,0,0,0,0,0],\n",
    "        [2,4,0,0,0,0,0,0],\n",
    "        [1,1,2,0,0,0,0,0],\n",
    "        [3,3,2,7,0,0,0,0],\n",
    "        [3,3,2,7,2,0,0,0],\n",
    "        [3,3,2,7,2,2,1,1],\n",
    "        [3,3,2,1,2,2,2,1],\n",
    "        [2,'center left',2,4,2,2,2,2]]\n",
    "\n",
    "    # For each row in the corner plot\n",
    "    for i, row in enumerate(ax_c):\n",
    "\n",
    "        # For each column in the corner plot\n",
    "        for j, subplot in enumerate(row):\n",
    "\n",
    "            r_results = np.array([], dtype=np.float64)\n",
    "            tau_results = np.array([], dtype=np.float64)\n",
    "            rho_results = np.array([], dtype=np.float64)\n",
    "\n",
    "            # If the row / column pair is above the main diagonal\n",
    "            if j > i:\n",
    "                pass\n",
    "            else:\n",
    "\n",
    "                x_data = np.empty(1000)\n",
    "                x_data[:] = np.nan\n",
    "                x_data = np.array([x_data])\n",
    "                y_data = np.empty(1000)\n",
    "                y_data[:] = np.nan\n",
    "                y_data = np.array([y_data])\n",
    "\n",
    "                # For each spectrum\n",
    "                for k, slit in enumerate(total_result):\n",
    "                    \n",
    "                    if all(np.isnan(slit[j])) or all(np.isnan(slit[i + 1])):\n",
    "                        pass\n",
    "                    else:\n",
    "                        #if ~all(np.isnan(slit[j])) and ~all(np.isnan(slit[j + 1])):\n",
    "                        x_data = np.append(x_data, np.array([slit[j]]), axis=0)\n",
    "                        y_data = np.append(y_data, np.array([slit[i + 1]]), axis=0)\n",
    "                        #pass\n",
    "                    '''\n",
    "                    else:\n",
    "                        x_data = np.append(x_data, np.array([slit[j]]), axis=0)\n",
    "                        y_data = np.append(x_data, np.array([slit[j + 1]]), axis=0)\n",
    "                    '''\n",
    "\n",
    "                x_data = x_data[1:]\n",
    "                y_data = y_data[1:]\n",
    "\n",
    "                #r_results = np.array([], dtype=np.float64)\n",
    "                #tau_results = np.array([], dtype=np.float64)\n",
    "                #rho_results = np.array([], dtype=np.float64)\n",
    "\n",
    "                if (i==6) and (j==6):\n",
    "                    pass\n",
    "                    #print(x_data)\n",
    "                    #print(y_data)\n",
    "\n",
    "                for k, sample in enumerate(x_data[0]):\n",
    "\n",
    "                    x = x_data[:,k]\n",
    "                    y = y_data[:,k]\n",
    "\n",
    "                    if any(np.isnan(x)) or any(np.isnan(y)):\n",
    "                        pass\n",
    "                    else:\n",
    "                        x = x[x != 0]\n",
    "                        y = y[y != 0]\n",
    "\n",
    "                        r = np.corrcoef(x, y)[0,1]\n",
    "                        tau = kendalltau(x, y).statistic\n",
    "                        rho = spearmanr(x, y).statistic\n",
    "\n",
    "                        r_results = np.append(r_results, r)\n",
    "                        tau_results = np.append(tau_results, tau)\n",
    "                        rho_results = np.append(rho_results, rho)\n",
    "\n",
    "                print(i, j)\n",
    "                print(len(r_results), np.median(r_results), \n",
    "                    sf_round(abs(np.median(r_results) - np.percentile(r_results, 16)),1), \n",
    "                    sf_round(abs(np.percentile(r_results, 84) - np.median(r_results)),1))\n",
    "                print(len(tau_results), np.median(tau_results), \n",
    "                    sf_round(abs(np.median(tau_results) - np.percentile(tau_results, 16)),1), \n",
    "                    sf_round(abs(np.percentile(tau_results, 84) - np.median(tau_results)),1))\n",
    "                print(len(rho_results), np.median(rho_results), \n",
    "                    sf_round(abs(np.median(rho_results) - np.percentile(rho_results, 16)),1), \n",
    "                    sf_round(abs(np.percentile(rho_results, 84) - np.median(rho_results)),1))\n",
    "                \n",
    "                '''\n",
    "                for k in [r_results, tau_results]:\n",
    "\n",
    "                    median = np.median(k)\n",
    "                    p16 = abs(np.median(k) - np.percentile(k, 16))\n",
    "                    p84 = abs(np.percentile(k, 84) - np.median(k))\n",
    "\n",
    "                    print(median, p16, p84)\n",
    "\n",
    "                    print(len(k), np.median(k), abs(np.median(r_results) - np.percentile(r_results, 16)), abs(np.percentile(r_results, 84) - np.median(r_results)))   \n",
    "\n",
    "                    plt.hist(k, bins=20)\n",
    "                    plt.show()\n",
    "                '''\n",
    "\n",
    "                lower = sigfig.round(abs(np.median(r_results) - np.percentile(r_results, 16)), 2)\n",
    "                upper = sigfig.round(abs(np.percentile(r_results, 84) - np.median(r_results)), 2)\n",
    "\n",
    "                d = [decimal.Decimal(lower).as_tuple().exponent, decimal.Decimal(upper).as_tuple().exponent]\n",
    "                d = abs(np.argmax(d))\n",
    "\n",
    "                at = AnchoredText('$' + '{:.2f}'.format(round(np.median(r_results), 2)) + '_{-' + '{:.2f}'.format(round(np.median(r_results) - np.percentile(r_results, 16), 2)) + '}^{+' + '{:.2f}'.format(round(np.percentile(r_results, 84) - np.median(r_results), 2))  + '}$',\n",
    "                    loc=r_locs[i][j], frameon=False, prop=dict(fontsize='small'))\n",
    "                ax_c[i,j].add_artist(at)\n",
    "\n",
    "    #ax_lya[2,1].set_xlabel('Velocity (km s$^{-1}$)', fontsize='large')\n",
    "    #ax_lya[1,0].set_ylabel(r'Flux density (10$^{-16}$ erg s$^{-1}$ cm$^{-2}$ $\\rm{\\AA}^{-1}$)', fontsize='large')\n",
    "\n",
    "    #for ax in [ax_top, ax_left, ax_right]:\n",
    "    #    ax.set_xlim(-1000,1000)\n",
    "    #    ax.set_ylim(bottom=0)\n",
    "\n",
    "    for i, ax in enumerate(ax_lya_stack):\n",
    "        ax.set_xlim(-1000,1000)\n",
    "        ax.set_ylim(bottom=0)\n",
    "\n",
    "        ax.tick_params(top=True, bottom=True, left=True, right=True, labelleft=True)\n",
    "\n",
    "    ax_lya_stack[2].set_xlabel('Velocity (km s$^{-1}$)')\n",
    "    ax_lya_stack[1].set_ylabel('Flux density (arb. scale)')\n",
    "\n",
    "    #ax_left.tick_params(left=False, labelleft=False)\n",
    "    #ax_right.tick_params(left=False, labelleft=False)\n",
    "\n",
    "    for i, ax_list in enumerate(ax_mc):\n",
    "\n",
    "        for j, subplot in enumerate(ax_list):\n",
    "\n",
    "            subplot.set_aspect(1 / subplot.get_data_ratio(), adjustable='box')\n",
    "\n",
    "    for i, ax_list in enumerate(ax_mc_stack):\n",
    "\n",
    "        for j, subplot in enumerate(ax_list):\n",
    "\n",
    "            subplot.set_aspect(1 / subplot.get_data_ratio(), adjustable='box')\n",
    "\n",
    "    '''\n",
    "    for i in ['top', 'left', 'right']:\n",
    "\n",
    "        ax_lya_stack[i].set_aspect(1 / ax_lya_stack[i].get_data_ratio(), adjustable='box')\n",
    "    '''\n",
    "\n",
    "    for i, ax in enumerate(ax_lya_stack):\n",
    "\n",
    "        #ax.set_aspect(1 / ax.get_data_ratio(), adjustable='box')\n",
    "        ax.tick_params(direction='in')\n",
    "\n",
    "    ax_lya_stack[1].set_yticks([0,3,6,9,12])\n",
    "\n",
    "    #for ax in [ax_top, ax_left, ax_right]:\n",
    "\n",
    "    #    ax.set_aspect(1 / ax.get_data_ratio(), adjustable='box')\n",
    "\n",
    "    for i, ax in enumerate(ax_lya_array):\n",
    "\n",
    "        ax.set_aspect(1 / ax.get_data_ratio(), adjustable='box')\n",
    "        ax.tick_params(top=True, bottom=True, left=True, right=True, labelleft=True, direction='in')\n",
    "\n",
    "    for i in [0,1,2]:\n",
    "        \n",
    "        ax_lya[2,i].set_xticks([-1000,-500,0,500,1000])\n",
    "    \n",
    "    ax_lya[2,1].set_xlabel('Velocity (km s$^{-1}$)', fontsize='large')\n",
    "    ax_lya[1,0].set_ylabel(r'Flux density (10$^{-17}$ erg s$^{-1}$ cm$^{-2}$ $\\rm{\\AA}^{-1}$)', fontsize='large')\n",
    "\n",
    "    #fig_lya_stack.tight_layout()\n",
    "\n",
    "    fig_lya.savefig(figs + '/lya_fits.pdf', bbox_inches='tight')\n",
    "\n",
    "    label(fig_mc, ax_mc, fig_mc_stack, ax_mc_stack)\n",
    "    set_ticks(ax_mc, ax_mc_stack)\n",
    "    disable_plots(ax_mc, ax_mc_stack)\n",
    "\n",
    "    for i, row in enumerate(ax_c):\n",
    "\n",
    "        for j, subplot in enumerate(row):\n",
    "            \n",
    "            if j > i:\n",
    "                subplot.axis('off')\n",
    "            else:\n",
    "                subplot.set_aspect(1 / subplot.get_data_ratio(), adjustable='box')\n",
    "\n",
    "    fig_lya_stack.subplots_adjust(hspace=0, wspace=0)\n",
    "\n",
    "    fig_lya_stack.savefig(figs + '/lya_fits_stack.pdf', bbox_inches='tight')\n",
    "\n",
    "    f = lambda x: x\n",
    "    g = lambda x: x\n",
    "\n",
    "    ax_2 = ax_c[0,0].secondary_yaxis('right', functions=(f,g))\n",
    "    ax_2.set_ylabel('(km s$^{-1}$)', fontsize='large')\n",
    "\n",
    "    fig_c.subplots_adjust(hspace=0.2, wspace=0.05)\n",
    "\n",
    "    fig_c.savefig(figs + '/corner.pdf', bbox_inches='tight')\n",
    "\n",
    "    fig_mc.savefig(figs + '/mc.pdf', bbox_inches='tight')\n",
    "    fig_mc_stack.savefig(figs + '/mc_stack.pdf', bbox_inches='tight')\n",
    "\n",
    "    #make_corner_plot()\n",
    "    \n",
    "    #plt.subplots_adjust(wspace=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edea5d7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16844/2806096226.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmeasure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16844/1970065138.py\u001b[0m in \u001b[0;36mmeasure\u001b[1;34m()\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{results}/lya_fits/{slit_id}/{slit_id}_best_fit_parameters.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mmedian_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfit_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;31m# Take the transpose of the results so that the rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fit_results' is not defined"
     ]
    }
   ],
   "source": [
    "measure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b2b404ef7cfffcb2d9e58206576e0220bed399f08fa92bc6fd125b02b641f3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
