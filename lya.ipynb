{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f1beb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Riley Owens (GitHub: mrileyowens)\n",
    "\n",
    "# This file measures values and errors of \n",
    "# various Lyα profile parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92eb6b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from math import floor, log10\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import erf\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "import sigfig\n",
    "\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnchoredText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4643e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "\n",
    "    '''\n",
    "    Directly fit models of the Lya profiles in a Monte Carlo simulation\n",
    "    '''\n",
    "\n",
    "    def extract_data(file):\n",
    "\n",
    "        '''\n",
    "        Extract spectrum from the .txt file\n",
    "\n",
    "        Parameters:\n",
    "            file : str\n",
    "                Name of the file\n",
    "\n",
    "        Returns:\n",
    "            w : numpy.ndarray\n",
    "                Observed wavelength bins\n",
    "            f : numpy.ndarray\n",
    "                Observed flux densities\n",
    "            n : numpy.ndarray\n",
    "                Observed Gaussian standard deviation of observed flux densities\n",
    "        '''\n",
    "\n",
    "        # Retrieve the data columns\n",
    "        w, f, n = np.loadtxt(file, delimiter='\\t', comments=('#', 'w'), usecols=(0,1,2), unpack=True)\n",
    "    \n",
    "        # If the file is not the stacked leaker spectrum. This step is necessary because \n",
    "        if 'leaker' not in file:\n",
    "\n",
    "            # Remove bins of extreme outliers\n",
    "            w = w[f < 1e-20]\n",
    "            n = n[f < 1e-20]\n",
    "            f = f[f < 1e-20]\n",
    "\n",
    "            # Convert from units of erg/s/cm^2/Hz to erg/s/cm^2/Å\n",
    "            f = f * 2.998e18 / np.square(w)\n",
    "            n = n * 2.998e18 / np.square(w)\n",
    "\n",
    "        return w, f, n    \n",
    "\n",
    "    def compute_fwhm_and_peak(parameters, Rs):\n",
    "\n",
    "        '''\n",
    "        Compute the FWHMs and locations of the maxima of a set of fits of a Lya peak\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            parameters : numpy.ndarray\n",
    "                The best-fit model parameters from the Monte Carlo simulation\n",
    "            Rs : numpy.ndarray\n",
    "                Spectral resolutions of the sets of parameters\n",
    "\n",
    "        Returns:\n",
    "            \n",
    "            fwhms : numpy.ndarray\n",
    "                The FWHMs of the Lya peaks corresponding to the input parameters\n",
    "            locs : numpy.ndarray\n",
    "                The location of the maxima of the Lya peaks corresponding to the input parameters\n",
    "        '''\n",
    "\n",
    "        def lin_interp(x, y, i):\n",
    "            return x[i] + (x[i+1] - x[i]) * y[i] / (y[i+1] - y[i])\n",
    "\n",
    "        def skew_gaussian(x, amp, cen, width, skew):\n",
    "\n",
    "            fit = amp * np.exp(-((x - cen) / width)**2 / 2) * (1 + erf(skew * ((x - cen) / width) / np.sqrt(2)))\n",
    "\n",
    "            return fit\n",
    "\n",
    "        # Make a an array of velocity bins; we are not using the data's bins \n",
    "        # because they are a large fraction of the FWHMs of the peaks\n",
    "        v = np.arange(-500,500,0.1)\n",
    "\n",
    "        # Make an empty list to append the best-fit curves to\n",
    "        fits = []\n",
    "\n",
    "        # For each set of best-fit parameters of the Lya peak\n",
    "        for i, fit_params in enumerate(parameters):\n",
    "\n",
    "            # Create the model curve\n",
    "            fit = skew_gaussian(v, *fit_params)\n",
    "\n",
    "            # Append the curve to the aggregate list\n",
    "            fits.append([*fit])\n",
    "\n",
    "        # Calculate the velocity locations of the maxima of the peaks\n",
    "        fits_max_indices = np.argmax(fits, axis=1)\n",
    "        locs = v[fits_max_indices]\n",
    "\n",
    "        # Calculate the fits halved by the maximum of each peak\n",
    "        fits_halved = fits - np.amax(fits, axis=1, keepdims=True) / 2\n",
    "\n",
    "        # Find where the halved fits cross the x-axis\n",
    "        crossings = np.diff(np.sign(fits_halved), axis=1) != 0\n",
    "\n",
    "        # Get the indices of the x-axis crossings\n",
    "        indices = np.array(np.where(crossings)).T\n",
    "\n",
    "        # Make an empty list to append the FWHMs to\n",
    "        fwhms = []\n",
    "\n",
    "        # For each set of parameters\n",
    "        for i in range(len(parameters)):\n",
    "\n",
    "            # Calculate the FWHM of the peak by linearly interpolating the location of the zero-crossing points of the halved fits\n",
    "            fwhm = abs(lin_interp(v, fits_halved[i], indices[2 * i][1]) - lin_interp(v, fits_halved[i], indices[2 * i + 1][1]))\n",
    "\n",
    "            # Append the FWHM to the aggregate list\n",
    "            fwhms.append(fwhm)\n",
    "\n",
    "        # Correct the FWHMs for the instrumental FWHM\n",
    "        fwhms = np.sqrt((np.array(fwhms))**2 - (Rs)**2)\n",
    "\n",
    "        # Replace any NaNs (where the instrumental FWHM was greater than the measured FWHM) with just the instrumental FWHMs\n",
    "        fwhms = np.where(np.isnan(fwhms), Rs, fwhms)\n",
    "\n",
    "        return fwhms, locs\n",
    "\n",
    "    def two_peaks(x, amp_r, cen_r, width_r, skew_r, amp_c, cen_c, width_c, cntm):\n",
    "\n",
    "        return amp_r * np.exp(-((x - cen_r) / width_r)**2 / 2) * (1 + erf(skew_r * ((x - cen_r) / width_r) / np.sqrt(2))) \\\n",
    "            + amp_c * np.exp(-((x - cen_c) / width_c)**2 / 2) \\\n",
    "            + cntm\n",
    "\n",
    "    def three_peaks(x, amp_b, cen_b, width_b, skew_b, amp_r, cen_r, width_r, skew_r, amp_c, cen_c, width_c, cntm):\n",
    "\n",
    "        return amp_b * np.exp(-((x - cen_b) / width_b)**2 / 2) * (1 + erf(skew_b * ((x - cen_b) / width_b) / np.sqrt(2)))  \\\n",
    "            + amp_r * np.exp(-((x - cen_r) / width_r)**2 / 2) * (1 + erf(skew_r * ((x - cen_r) / width_r) / np.sqrt(2))) \\\n",
    "            + amp_c * np.exp(-((x - cen_c) / width_c)**2 / 2) \\\n",
    "            + cntm\n",
    "\n",
    "    # Dictionary containing information about the MagE slit spectra, including, from left to right:\n",
    "    # the name of the file containing the data, redshift (taken from Mainali et al. (2022) (ApJ, 940, 160)),\n",
    "    # magnification of the slit, a tuple specifying a range of Lya peculiar velocities used to set \n",
    "    # initial parameter estimates of the central Lya peak, spectral resolution, and uncertainty in the spectral resolution\n",
    "    slits = {\n",
    "        'NL' : ['sunburst_arc_nonleaker_stack_mage.txt', 0, 1, [60,140], 5400, 200],\n",
    "        'L' : ['sunburst_arc_leaker_stack_mage.txt', 0, 1, [20,130], 5300, 200],\n",
    "        'M5' : ['sunburst_M-5-comb1_MWdr.txt', 2.37086, 51, [0,100], 5500, 400],\n",
    "        'M4' : ['sunburst_M-4-comb1_MWdr.txt', 2.37073, 14.6, [0,85], 5400, 300],\n",
    "        'M6' : ['sunburst_M-6-comb1_MWdr.txt', 2.37021, 147, [10,130], 5300, 300],\n",
    "        'M3' : ['sunburst_M-3-comb1_MWdr.txt', 2.37025, 36, [35,120], 5500, 400],\n",
    "        'M0' : ['sunburst_M-0-comb1_MWdr.txt', 2.37014, 10, [10,130], 4700, 200],\n",
    "        'M2' : ['sunburst_M-2-comb1_MWdr.txt', 2.37017, 32, [45,125], 5300, 300],\n",
    "        'M7' : ['sunburst_M-7-comb1_MWdr.txt', 2.37044, 35, [15,100], 5200, 200],\n",
    "        'M8' : ['sunburst_M-8-comb1_MWdr.txt', 2.37024, 29, [25,125], 5200, 300],\n",
    "        'M9' : ['sunburst_M-9-comb1_MWdr.txt', 2.37030, 31, [15,125], 5500, 400]\n",
    "    }\n",
    "\n",
    "    # Establish directories\n",
    "    home = os.getcwd()\n",
    "    data = f'{home}/data'\n",
    "    results = f'{home}/results'\n",
    "\n",
    "    # Instantiate a cosmology with 30% matter-based energy density and an expansion rate of 70 km/s/Mpc\n",
    "    cosmology = FlatLambdaCDM(70,0.3)\n",
    "\n",
    "    # For each slit ID\n",
    "    for i, slit_id in enumerate(slits):\n",
    "\n",
    "        # Unpack the dictionary values for this slit\n",
    "        file_name, z, mag, c_peak_range, R, R_error = slits[slit_id]\n",
    "\n",
    "        # Convert the spectral resolution to units of km/s, which is \n",
    "        # necessary when correcting the FWHMs (which has units of km/s)\n",
    "        # for the instrumental resolution\n",
    "        R_error = 299792.458 / R**2 * R_error\n",
    "        R = 299792.458 / R\n",
    "\n",
    "        # Set a scaling factor which will be applied to the data. This is necesary\n",
    "        # because for the observed data (so not the stacked spectra), the fits are \n",
    "        # very poor (the fitting does not explore the parameter space) without rescaling \n",
    "        # them, perhaps related to float precision because given their flux density units, \n",
    "        # the flux density values are very small (~10^16 - 10^17 erg/s/cm^2/Å)\n",
    "        factor = 1e16 if 'M' in slit_id else 1\n",
    "\n",
    "        # Extract the data from the .txt file\n",
    "        w, f, n = extract_data(f'{data}/mage/{file_name}')\n",
    "\n",
    "        # Cut the data to just the relevant portion of the spectrum\n",
    "        f = f[(w >= 1195 * (1 + z)) & (w <= 1235 * (1 + z))]\n",
    "        n = n[(w >= 1195 * (1 + z)) & (w <= 1235 * (1 + z))]\n",
    "        w = w[(w >= 1195 * (1 + z)) & (w <= 1235 * (1 + z))]\n",
    "        \n",
    "        # Place the data in the rest frame, and apply the scaling factor\n",
    "        w = w / (1 + z)\n",
    "        f = f * (1 + z) * factor\n",
    "        n = n * (1 + z) * factor\n",
    "\n",
    "        # Calculate the Lya peculiar velocities\n",
    "        v = 299792.458 * (w / 1215.67 - 1)\n",
    "\n",
    "        # Determine the luminosity distance to the redshift in units of centimeters (to match the flux density unit)\n",
    "        l_dist = cosmology.luminosity_distance(z).value * 3.086e24\n",
    "\n",
    "        # Create a boolean mask designating the wavelength range used to compute the local continuum\n",
    "        cntm_mask = (w >= 1221) & (w <= 1225)\n",
    "\n",
    "        # Create a boolean mask designating the integration range when computing the Lya equivalent width and luminosity\n",
    "        lya_profile_mask = (w >= 1212) & (w <= 1221)\n",
    "        \n",
    "        # Create boolean masks for the integration ranges about the rest velocity of the Lya profile\n",
    "        # for computing the central escape fraction of the Lya profile\n",
    "        v100_mask = (v >= -100) & (v <= 100)\n",
    "        v1000_mask = (v >= -1000) & (v <= 1000)\n",
    "\n",
    "        # Set a string for the flux density unit of the data\n",
    "        flux_density_unit = 'erg/s/cm^2/Å' if 'M' in slit_id else 'normalized wavelength-space flux density'\n",
    "\n",
    "        # If there is no blue peak and the slit ID is not M5\n",
    "        if i in [0,3,4]:\n",
    "        \n",
    "            # Set the model function and the model parameter labels\n",
    "            model = two_peaks\n",
    "            model_parameter_labels = f'redshifted Lyα peak amplitude ({flux_density_unit}), centroid (km/s), width (km/s), and skew, \\n' \\\n",
    "                + f'central Lyα peak amplitude ({flux_density_unit}), centroid (km/s), and width (km/s), ' \\\n",
    "                + f'and local continuum ({flux_density_unit})'\n",
    "        \n",
    "            # Set the initial parameter estimates and bounds\n",
    "            p0 = (np.amax(f[(v >= 150) & (v <= 1000)]),200,20,1,np.amax(f[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]),80,40,np.median(f[cntm_mask]))\n",
    "            bounds = ([0,0,0,0,0,0,0,0],[np.amax(f),1000,np.inf,np.inf,np.amax(f),200,85,np.amax(f)])\n",
    "    \n",
    "        # If the slit ID is M5; this elif statement is necessary for improved initial parameter estimates of the central Lya peak in slit M5. The previous\n",
    "        # if statement was not satisfactory\n",
    "        elif i in [2]:\n",
    "\n",
    "            # Set the model function and the model parameter labels\n",
    "            model = two_peaks\n",
    "            model_parameter_labels = f'redshifted Lyα peak amplitude ({flux_density_unit}), centroid (km/s), width (km/s), and skew, \\n' \\\n",
    "                + f'central Lyα peak amplitude ({flux_density_unit}), centroid (km/s), and width (km/s), ' \\\n",
    "                + f'and local continuum ({flux_density_unit})'\n",
    "\n",
    "            # Set the initial parameter estimates and bounds\n",
    "            p0 = (np.amax(f[(v >= 150) & (v <= 1000)]),200,20,1,np.amax(f[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]) - np.median(f[cntm_mask]),50,20,np.median(f[cntm_mask]))\n",
    "            bounds = ([0,0,0,0,0,0,0,0],[np.amax(f),500,np.inf,np.inf,np.amax(f[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]) - np.median(f[cntm_mask]),100,40,np.amax(f)])\n",
    "\n",
    "        # Otherwise\n",
    "        else:\n",
    "\n",
    "            # Set the model function and the model parameter labels\n",
    "            model = three_peaks\n",
    "            model_parameter_labels = f'blueshifted Lyα peak amplitude ({flux_density_unit}), centroid (km/s), width (km/s), and skew, \\n' \\\n",
    "                + f'redshifted Lyα peak amplitude ({flux_density_unit}), centroid (km/s), width (km/s), and skew, \\n' \\\n",
    "                + f'central Lyα peak amplitude ({flux_density_unit}), centroid (km/s), and width (km/s), ' \\\n",
    "                + f'and local continuum ({flux_density_unit})'\n",
    "\n",
    "            # Set the initial parameter estimates and bounds\n",
    "            p0 = (np.amax(f[(v >= -1000) & (v <= 0)]),-150,20,-1,np.amax(f[(v >= 150) & (v <= 1000)]),200,20,1,np.amax(f[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]),80,40,np.median(f[cntm_mask]))\n",
    "            bounds = ([0,-1000,0,-np.inf,0,0,0,0,0,0,0,0],[np.amax(f[(v >= -1000) & (v <= 0)]),0,np.inf,0,np.amax(f),1000,np.inf,np.inf,np.amax(f),200,85,np.amax(f)])\n",
    "\n",
    "        # Establish empty lists to append the results of the burn-in Monte Carlo simulation to\n",
    "        mc_spectra_burn_in = []\n",
    "        parameters_burn_in = []\n",
    "\n",
    "        # For each iteration in the burn-in Monte Carlo simulation\n",
    "        for j in range(1000):\n",
    "\n",
    "            # Draw a randomly sampled spectrum from the original observation, assuming \n",
    "            # that the observed flux densities and associated uncertainties correspond \n",
    "            # to the mean and standard deviation of Gaussian distributions, respectively\n",
    "            f_mc = np.random.normal(f, n)\n",
    "\n",
    "            # Directly fit the Lya profile with the assigned model function and initial parameter estimates and bounds\n",
    "            p, _ = curve_fit(model, v[v >= -500], f_mc[v >= -500], p0=p0, bounds=bounds, maxfev=1e6) \n",
    "\n",
    "            # Append the randomly sampled spectrum and best-fit model parameters to the aggregate lists\n",
    "            mc_spectra_burn_in.append([*f_mc])\n",
    "            parameters_burn_in.append([*p])\n",
    "\n",
    "        # Make an array out of the burn-in Monte Carlo simulation spectra for easier handling\n",
    "        mc_spectra_burn_in = np.array(mc_spectra_burn_in, dtype=np.float64)\n",
    "\n",
    "        # Make arrays out of the initial and best-fit model parameters for easier handling\n",
    "        p0 = np.array(p0, dtype=np.float64)\n",
    "        parameters_burn_in = np.array(parameters_burn_in, dtype=np.float64)\n",
    "\n",
    "        # Unscale the affected intial model parameter estimates by the artificial scale factor\n",
    "        p0[-1], p0[-4], p0[-8] = p0[-1] / factor, p0[-4] / factor, p0[-8] / factor\n",
    "\n",
    "        # Also unscale the initial estimate of the amplitude of the blueshifted Lya peak if the model function had 3 peaks\n",
    "        p0[0] = p0[0] / factor if model==three_peaks else p0[0]\n",
    "\n",
    "        # Set the header for the output file containing the randomly sampled Lya spectra of the burn-in Monte Carlo simulation\n",
    "        header = f'Randomly sampled Lyα spectra of the burn-in Monte Carlo simulation of {f\"slit {slit_id}\" if \"M\" in slit_id else slit_id}.\\n' \\\n",
    "            + f'Created by lya.ipynb on {datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")}.\\n' \\\n",
    "            + '\\n' \\\n",
    "            + f'Number of iterations of the burn-in Monte Carlo simulation: {len(mc_spectra_burn_in)}\\n' \\\n",
    "            + '\\n' \\\n",
    "            + f'Columns, from left to right: peculiar velocity relative to Lyα (km/s), flux density uncertainty ({flux_density_unit}), and randomly sampled flux densities of each iteration of the Monte Carlo simulation ({flux_density_unit})\\n'\n",
    "\n",
    "        # Save the output file containing the randomly sampled Lya spectra of the burn-in Monte Carlo simulation\n",
    "        np.savetxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_burn_in_lya_spectra.txt', np.array([v, n / factor, *(mc_spectra_burn_in / factor)]).T, header=header, delimiter=' ', encoding='utf-8')\n",
    "\n",
    "        # Set the header for the output file containing the best-fit model parameters of the burn-in Monte Carlo simulation\n",
    "        header = f'Best-fit model parameters of the Lyα fits of the burn-in Monte Carlo simulation of {f\"slit {slit_id}\" if \"M\" in slit_id else slit_id}.\\n' \\\n",
    "            + f'Created by lya.ipynb on {datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")}.\\n' \\\n",
    "            + '\\n' \\\n",
    "            + f'Initial parameters used by the burn-in Monte Carlo simulation: {p0}\\n' \\\n",
    "            + f'Number of iterations of the burn-in Monte Carlo simulation: {len(mc_spectra_burn_in)}\\n' \\\n",
    "            + '\\n' \\\n",
    "            + f'Columns, from left to right: {model_parameter_labels}\\n'\n",
    "        \n",
    "        # Make a copy of the burn-in best-fit model parameters to manipulate while preserving the original version\n",
    "        p0_burn_in_save = parameters_burn_in.copy()\n",
    "        \n",
    "        # Unscale the affected intial model parameter estimates by the artificial scale factor\n",
    "        p0_burn_in_save[:,-1], p0_burn_in_save[:,-4], p0_burn_in_save[:,-8] = p0_burn_in_save[:,-1] / factor, p0_burn_in_save[:,-4] / factor, p0_burn_in_save[:,-8] / factor\n",
    "\n",
    "        # Also unscale the initial estimate of the amplitude of the blueshifted Lya peak if the model function had 3 peaks\n",
    "        p0_burn_in_save[:,0] = p0_burn_in_save[:,0] / factor if model==three_peaks else p0_burn_in_save[:,0]\n",
    "\n",
    "        # Save the output file containing the best-fit model parameters\n",
    "        np.savetxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_burn_in_lya_best_fit_model_parameters.txt', p0_burn_in_save, header=header, delimiter=' ', encoding='utf-8')\n",
    "\n",
    "        # Compute the median best-fit model parameters of the burn-in Monte Carlo simulation\n",
    "        p0_burn_in = np.median(np.array(parameters_burn_in, dtype=np.float64).T, axis=1)\n",
    "\n",
    "        # Establish empty lists to append the results of the Monte Carlo simulation to\n",
    "        mc_spectra = []\n",
    "        parameters = []\n",
    "        measurements = []\n",
    "\n",
    "        # For each iteration in the Monte Carlo simulation\n",
    "        for j in range(10000):\n",
    "\n",
    "            # Draw a randomly sampled spectrum from the original observation, assuming \n",
    "            # that the observed flux densities and associated uncertainties correspond \n",
    "            # to the mean and standard deviation of Gaussian distributions, respectively\n",
    "            f_mc = np.random.normal(f, n)\n",
    "\n",
    "            # Compute the local continuum as the median flux density between 1221 - 1225 Å in the rest frame\n",
    "            c = np.median(f_mc[cntm_mask])\n",
    "\n",
    "            # Compute the rest-frame equivalent width of the Lya profile\n",
    "            ew = -1 * np.trapz(1 - (f_mc / c)[lya_profile_mask], w[lya_profile_mask])\n",
    "\n",
    "            # Compute the central escape fraction of the Lya profile\n",
    "            f_cen = np.trapz(f_mc[v100_mask], v[v100_mask]) / np.trapz(f_mc[v1000_mask], v[v1000_mask]) * 100\n",
    "\n",
    "            # Compute the luminosity of the Lya profile, placing the data back into the observed frame, removing the scaling factor, and \n",
    "            # correcting for the magnification\n",
    "            l = 4 * np.pi * np.trapz((f_mc[lya_profile_mask] - c) / (1 + z) / mag / factor, (w * (1 + z))[lya_profile_mask]) * l_dist**2\n",
    "\n",
    "            # Directly fit the Lya profile with the assigned model function and initial parameter estimates and bounds\n",
    "            p, _ = curve_fit(model, v[(v >= -500) & (f_mc >= c)], f_mc[(v >= -500) & (f_mc >= c)], p0=p0_burn_in, bounds=bounds, maxfev=1e6) \n",
    "\n",
    "            # Compute the ratio between the 'minimum' flux density between the redshifted and blueshifted Lya peaks \n",
    "            # (really taken as the amplitude of the central Lya peak; see paper for justifying details) and the local continuum\n",
    "            ratio = p[-4] / p[-1]\n",
    "\n",
    "            # Append the randomly sampled spectrum, best-fit model parameters, and Lya measurements to the aggregate lists\n",
    "            mc_spectra.append([*f_mc])\n",
    "            parameters.append([*p])\n",
    "            measurements.append([ratio, ew, f_cen, l])\n",
    "\n",
    "        # Convert all of the result lists into arrays for easier handling\n",
    "        #p0 = np.array(p0, dtype=np.float64)\n",
    "        mc_spectra = np.array(mc_spectra, dtype=np.float64)\n",
    "        parameters = np.array(parameters, dtype=np.float64)\n",
    "        measurements = np.array(measurements, dtype=np.float64)\n",
    "\n",
    "        # Unscale the affected best-fit model parameters by the artifical scaling factor\n",
    "        p0_burn_in[-1], p0_burn_in[-4], p0_burn_in[-8] = p0_burn_in[-1] / factor, p0_burn_in[-4] / factor, p0_burn_in[-8] / factor\n",
    "\n",
    "        # Also unscale the amplitude of the blueshifted Lya peak if the model function had 3 peaks\n",
    "        p0_burn_in[0] = p0_burn_in[0] / factor if model==three_peaks else p0_burn_in[0]\n",
    "\n",
    "        # Unscale the affected best-fit model parameters by the artifical scaling factor\n",
    "        parameters[:,-1], parameters[:,-4], parameters[:,-8] = parameters[:,-1] / factor, parameters[:,-4] / factor, parameters[:,-8] / factor\n",
    "\n",
    "        # Also unscale the amplitude of the blueshifted Lya peak if the model function had 3 peaks\n",
    "        parameters[:,0] = parameters[:,0] / factor if model==three_peaks else parameters[:,0]\n",
    "\n",
    "        # Replace the Lya luminosity measurements with NaNs if the slit is one of \n",
    "        # the stacked spectra, since they do not have physical flux densities\n",
    "        measurements[:,-1] = np.nan if 'L' in slit_id else measurements[:,-1]\n",
    "\n",
    "        # Randomly sample spectral resolutions for the spectrum from a Gaussian distribution, \n",
    "        # matching the number of iterations of the Monte Carlo simulation, and assuming that \n",
    "        # the reported measurement and uncertainty correspond to the mean and standard deviation \n",
    "        # of the Gaussian distribution. This is necessary to fold in the uncertainty in the \n",
    "        # spectral resolution into the measured FWHMs of the Lya peaks, since they are corrected\n",
    "        # for instrumental resolution\n",
    "        Rs = np.random.normal(R, R_error, len(parameters))\n",
    "\n",
    "        # Calculate the FWHMs of the central Lya peak, correcting for the instrumental resolution\n",
    "        fwhms_c = 2 * np.sqrt(2 * np.log(2)) * parameters.T[-2]\n",
    "        fwhms_c = np.sqrt((np.array(fwhms_c))**2 - (Rs)**2)\n",
    "\n",
    "        fwhms_c = np.where(np.isnan(fwhms_c), Rs, fwhms_c)\n",
    "\n",
    "        # Calculate the FWHMs and locations of the maxima of the redshifted Lya peak\n",
    "        fwhms_r, locs_r = compute_fwhm_and_peak(parameters[:, -8:-4], Rs)\n",
    "\n",
    "        # Insert the central and redshifted Lya peak FWHMs into the Lya measurements array\n",
    "        measurements = np.insert(measurements, 0, fwhms_r, axis=1)\n",
    "        measurements = np.insert(measurements, 0, fwhms_c, axis=1)\n",
    "\n",
    "        # Instantiate the Lya peak separations and blueshifted Lya peak FWHMs as NaNs. This is\n",
    "        # done to keep the output Lya measurements the same shape for all the spectra, even if\n",
    "        # they do not have a blueshifted Lya peak necessary to measure the Lya peak separation\n",
    "        # and blueshifted Lya peak FWHM. If the spectrum does have a third peak, these NaNs\n",
    "        # will be replaced by actual measurements in the following if statement\n",
    "        v_sep, fwhms_b = np.empty(len(mc_spectra)), np.empty(len(mc_spectra))\n",
    "        v_sep[:], fwhms_b[:] = np.nan, np.nan\n",
    "\n",
    "        # If the spectrum has three Lya peaks\n",
    "        if slit_id in ['L', 'M0', 'M2', 'M3', 'M7', 'M8', 'M9']:\n",
    "            \n",
    "            # Calculate the FWHMs and locations of the maxima of the blueshifted Lya peak\n",
    "            fwhms_b, locs_b = compute_fwhm_and_peak(parameters[:, 0:4], Rs)\n",
    "\n",
    "            # Calculate the Lya peak separations\n",
    "            v_sep = np.abs(locs_r - locs_b)\n",
    "\n",
    "        # Insert the Lya peak separations and blueshifted Lya peak FWHMs into the Lya measurements array\n",
    "        measurements = np.insert(measurements, 0, [v_sep, fwhms_b], axis=1)\n",
    "\n",
    "        # Insert the randomly sampled spectral resolutions into the Lya measurements array, since they are \n",
    "        # necessary to measure the instrument-corrected FWHMs\n",
    "        measurements = np.insert(measurements, 0, 299792.458 / Rs, axis=1)\n",
    "\n",
    "        # If the results folder for this slit ID doesn't exist yet, make it\n",
    "        if not os.path.isdir(f'{results}/lya_fits/{slit_id}'):\n",
    "            os.makedirs(f'{results}/lya_fits/{slit_id}')\n",
    "\n",
    "        # Set the header for the output file containing the randomly sampled Lya spectra\n",
    "        header = f'Randomly sampled Lyα spectra of the Monte Carlo simulation of {f\"slit {slit_id}\" if \"M\" in slit_id else slit_id}\\n' \\\n",
    "            + f'Created by lya.ipynb on {datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")}.\\n' \\\n",
    "            + '\\n' \\\n",
    "            + f'Number of iterations of this Monte Carlo simulation: {len(mc_spectra)}\\n' \\\n",
    "            + '\\n' \\\n",
    "            + f'Columns, from left to right: peculiar velocity relative to Lyα (km/s), flux density uncertainty ({flux_density_unit}), and randomly sampled flux densities of each iteration of the Monte Carlo simulation ({flux_density_unit})\\n'\n",
    "\n",
    "        # Save the output file containing the randomly sampled Lya spectra\n",
    "        np.savetxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_spectra.txt', np.array([v, n / factor, *(mc_spectra / factor)]).T, header=header, delimiter=' ', encoding='utf-8')    \n",
    "\n",
    "        # Set the header for the output file containing the best-fit model parameters\n",
    "        header = f'Best-fit model parameters of the Lyα fits of the Monte Carlo simulation of {f\"slit {slit_id}\" if \"M\" in slit_id else slit_id}.\\n' \\\n",
    "            + f'Created by lya.ipynb on {datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")}.\\n' \\\n",
    "            + '\\n' \\\n",
    "            + f'Initial parameters from the burn-in Monte Carlo simulation: {p0_burn_in}\\n' \\\n",
    "            + f'Number of iterations of this Monte Carlo simulation: {len(mc_spectra)}\\n' \\\n",
    "            + '\\n' \\\n",
    "            + f'Columns, from left to right: {model_parameter_labels}\\n'\n",
    "\n",
    "        # Save the output file containing the best-fit model parameters\n",
    "        np.savetxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_best_fit_model_parameters.txt', parameters, header=header, delimiter=' ', encoding='utf-8')\n",
    "\n",
    "        # Set the header for the output file containing the Lya measurements\n",
    "        header = f'Measurements of the Lyα profiles from the best-fit curves of the Monte Carlo simulation of {f\"slit {slit_id}\" if \"M\" in slit_id else slit_id}\\n' \\\n",
    "            + f'Created by lya.ipynb on {datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")}.\\n' \\\n",
    "            + '\\n' \\\n",
    "            + f'Number of iterations of this Monte Carlo simulation: {len(mc_spectra)}\\n' \\\n",
    "            + '\\n' \\\n",
    "            + 'Columns, from left to right: spectral resolution (R), Lyα peak separation (km/s), blueshifted Lyα peak FWHM (km/s), central Lyα peak FWHM (km/s), redshifted Lyα peak FWHM (km/s),\\n' \\\n",
    "            + 'ratio between the \\'minimum\\' flux density between the redshifted and blueshifted Lyα peaks and the continuum flux density (consult the paper for more details),\\n' \\\n",
    "            + 'rest-frame Lyα equivalent width (Å), central fraction of Lyα flux (%), and Lyα luminosity (erg/s)\\n'\n",
    "\n",
    "        # Save the output file containing the Lya measurements\n",
    "        np.savetxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_measurements.txt', measurements, header=header, delimiter=' ', encoding='utf-8')\n",
    "\n",
    "def correlate():\n",
    "\n",
    "    '''\n",
    "    Measure the statistical correlations between the Lya measurements\n",
    "    '''\n",
    "\n",
    "    # Slit ID dictionary containing, from left to right: file name containing the data, redshift, \n",
    "    slits = {\n",
    "        'NL' : [],\n",
    "        'L' : [],\n",
    "        'M5' : [],\n",
    "        'M4' : [],\n",
    "        'M6' : [],\n",
    "        'M3' : [],\n",
    "        'M0' : [],\n",
    "        'M2' : [],\n",
    "        'M7' : [],\n",
    "        'M8' : [],\n",
    "        'M9' : []\n",
    "    }\n",
    "\n",
    "    # Establish directories\n",
    "    home = os.getcwd()\n",
    "    results = f'{home}/results'\n",
    "\n",
    "    # Calculate the number of iterations of the main Monte Carlo simulation\n",
    "    iters = len(np.loadtxt(f'{results}/lya_fits/NL/NL_mc_sim_lya_measurements.txt', usecols=(0)))\n",
    "\n",
    "    # Get the LyC escape fraction measurements and uncertainties from the results file\n",
    "    f_escs, n_f_escs = np.loadtxt(f'{results}/f_esc_lyc_measurements.txt', delimiter=' ', usecols=(4,5), unpack=True)\n",
    "    f_escs, n_f_escs = np.insert(f_escs, 0, [np.nan, np.nan]), np.insert(n_f_escs, 0, [np.nan, np.nan])\n",
    "\n",
    "    # Make an empty list that will contain the statistical correlation results\n",
    "    corr_coefs = []\n",
    "\n",
    "    # For each Lya parameter\n",
    "    for i in range(8):\n",
    "\n",
    "        # For each unique combination with another Lya parameter not covered by the loop yet\n",
    "        for j in range(8 - (i + 1)):\n",
    "\n",
    "            # Make an empty list that will store the aggregate measurements of the parameter pair from all the spectra\n",
    "            measurements = []\n",
    "\n",
    "            # For each slit ID\n",
    "            for k, slit_id in enumerate(slits):\n",
    "\n",
    "                # If the slit ID is M0 and the Lya luminosity is one of the measurements of the pair\n",
    "                if slit_id == 'M0' and i+j+2 == 8:\n",
    "\n",
    "                    # Get the two measurements. Assign the Lya luminosity values as NaNs since they are unphysical for slit M0 due to poor fluxing\n",
    "                    x, y = np.loadtxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_measurements.txt', usecols=(i+1)), np.empty(iters) * np.nan\n",
    "\n",
    "                    # Rearrange the measurements into an acceptable format\n",
    "                    measurements.append([[x[i],y[i]] for i in range(len(x))])\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # Append the measurements of the parameter pair to the measurement list\n",
    "                    measurements.append(np.loadtxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_measurements.txt', usecols=(i+1, i+j+2)))\n",
    "\n",
    "            # Make the measurements list into an array for convenience and easier handling\n",
    "            measurements = np.array(measurements)\n",
    "\n",
    "            # Make an empty list that will store the aggregate statistical correlations between the measurement pairs\n",
    "            measurement_pair_corr_coefs = []\n",
    "            \n",
    "            #for k, _ in enumerate(measurements):\n",
    "            # For each iteration in the Monte Carlo simulation\n",
    "            for k in range(iters):\n",
    "\n",
    "                # Get all of the measurements of the given measurement pair for that iteration\n",
    "                pairs = measurements[:,k,:]\n",
    "\n",
    "                # Drop any pairs that contain NaNs (i.e., where that measurement wasn't applicable)\n",
    "                pairs = pairs[~np.any(np.isnan(pairs), axis=1)]\n",
    "\n",
    "                # Take the transpose of the pairs to separate the measurements into an 'x' and 'y' column\n",
    "                pairs = pairs.T\n",
    "\n",
    "                # Compute the Pearson and Kendall correlation coefficients\n",
    "                pearson_corr_coef = np.corrcoef(pairs[0], pairs[1])[0,1]\n",
    "                kendall_corr_coef = kendalltau(pairs[0], pairs[1], variant='b').statistic\n",
    "\n",
    "                # Append the correlation coefficients to the aggregate list\n",
    "                measurement_pair_corr_coefs.append([pearson_corr_coef, kendall_corr_coef])\n",
    "\n",
    "            # Append all the statistical correlation measurements for the parameter pair to the aggregate list\n",
    "            corr_coefs.append(measurement_pair_corr_coefs)        \n",
    "\n",
    "        # Make an empty list that will store the randomly sampled measurements of the LyC escape fractions\n",
    "        f_esc_lyc = []\n",
    "\n",
    "        # For each spectrum\n",
    "        for j, _ in enumerate(slits):\n",
    "\n",
    "            # Append the randomly sampled measurements (following a Gaussian distribution\n",
    "            # and assuming that the measured value and associated uncertainty correspond \n",
    "            # to the mean and standard deviation of the distribution) of the LyC escape\n",
    "            # fraction of the slit to the aggregate list\n",
    "            f_esc_lyc.append(np.random.normal(f_escs[j], n_f_escs[j], iters))\n",
    "\n",
    "        # Convert the list to an array and transpose it\n",
    "        f_esc_lyc = np.array(f_esc_lyc).T\n",
    "\n",
    "        # Make an empty list that will store the aggregate statistical correlations between the measurement pairs\n",
    "        measurement_pair_corr_coefs = []\n",
    "\n",
    "        # For each iteration of the Monte Carlo simulation \n",
    "        for j in range(iters):\n",
    "\n",
    "            # Make an array of the measurements organized into pairs by source spectrum\n",
    "            pairs = np.array([measurements[:,k,0], f_esc_lyc[j]]).T\n",
    "\n",
    "            # Remove any pairs that contain any NaNs (i.e., where that measurement wasn't applicable)\n",
    "            pairs = pairs[~np.any(np.isnan(pairs), axis=1)].T\n",
    "\n",
    "            # Calculate the Pearson and Kendall correlation coefficients\n",
    "            pearson_corr_coef = np.corrcoef(pairs[0], pairs[1])[0,1]\n",
    "            kendall_corr_coef = kendalltau(pairs[0], pairs[1], variant='b').statistic\n",
    "\n",
    "            # Append the correlation coefficients to the aggregate list\n",
    "            measurement_pair_corr_coefs.append([pearson_corr_coef, kendall_corr_coef])\n",
    " \n",
    "        # Append all the statistical correlation measurements for the parameter pair to the aggregate list\n",
    "        corr_coefs.append(measurement_pair_corr_coefs)\n",
    "\n",
    "    # Convert the list of correlation coefficient measurements to an array for easier handling\n",
    "    corr_coefs = np.array(corr_coefs)\n",
    "\n",
    "    # Flatten the array so that each set of measurements of a type of correlation coefficient\n",
    "    # for a parameter pair becomes its own column\n",
    "    corr_coefs = corr_coefs.swapaxes(1, 2)\n",
    "    corr_coefs = corr_coefs.reshape(-1, corr_coefs.shape[-1])\n",
    "\n",
    "    # Transpose the data so that the correlation measurements of a parameter pair are in columns\n",
    "    corr_coefs = corr_coefs.T\n",
    "\n",
    "    # Set the header of the output file of the statistical correlations between the measurement pairs\n",
    "    header = 'Statistical correlations measured between the Lyα measurements each iteration of the Monte Carlo simulation.\\n' \\\n",
    "        + f'Created by lya.ipynb on {datetime.now(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")}.\\n' \\\n",
    "        + '\\n' \\\n",
    "        + f'Number of iterations of the Monte Carlo simulation: {iters}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + 'Columns, from left to right: each consecutive pair of columns, starting from the first two columns, are the Pearson \\n' \\\n",
    "        + 'correlation coefficients (first column of the pair) and type \\'b\\' Kendall correlation coefficient (second column \\n' \\\n",
    "        + 'of the pair) measured for the following order of Lyα parameter pairs from the Monte Carlo simulation: \\n' \\\n",
    "        + '\\n' \\\n",
    "        + 'Lyα peak separation - (blueshifted Lyα peak FWHM, central Lyα peak FWHM, redshifted Lyα peak FWHM, ratio between the \\'minimum\\' flux density between the redshifted and blueshifted Lyα peaks and the continuum flux density, rest-frame Lyα equivalent width, central fraction of Lyα flux, Lyα luminosity), \\n' \\\n",
    "        + 'blueshifted Lyα peak FWHM - (central Lyα peak FWHM, redshifted Lyα peak FWHM, ratio between the \\'minimum\\' flux density between the redshifted and blueshifted Lyα peaks and the continuum flux density, rest-frame Lyα equivalent width, central fraction of Lyα flux, Lyα luminosity), \\n' \\\n",
    "        + 'central Lyα peak FWHM - (redshifted Lyα peak FWHM, ratio between the \\'minimum\\' flux density between the redshifted and blueshifted Lyα peaks and the continuum flux density, rest-frame Lyα equivalent width, central fraction of Lyα flux, Lyα luminosity), \\n' \\\n",
    "        + 'redshifted Lyα peak FWHM - (ratio between the \\'minimum\\' flux density between the redshifted and blueshifted Lyα peaks and the continuum flux density, rest-frame Lyα equivalent width, central fraction of Lyα flux, Lyα luminosity), \\n' \\\n",
    "        + 'ratio between the \\'minimum\\' flux density between the redshifted and blueshifted Lyα peaks and the continuum flux density - (rest-frame Lyα equivalent width, central fraction of Lyα flux, Lyα luminosity), \\n' \\\n",
    "        + 'rest-frame Lyα equivalent width - (central fraction of Lyα flux, Lyα luminosity), and\\n' \\\n",
    "        + 'central fraction of Lyα flux - Lyα luminosity\\n'\n",
    "\n",
    "    # Save the output file containing the statistical correlations between the measurement pairs\n",
    "    np.savetxt(f'{results}/lya_fits/mc_sim_lya_measurements_statistical_correlations.txt', corr_coefs, header=header, delimiter='\\t', encoding='utf-8')\n",
    "\n",
    "def plot():\n",
    "\n",
    "    '''\n",
    "    Makes various Lya figures: the stacked and individual Lya profiles and Lya measurements corner plot\n",
    "    '''\n",
    "\n",
    "    def extract_data(file):\n",
    "\n",
    "        '''\n",
    "        Extract spectrum from the .txt file\n",
    "\n",
    "        Parameters:\n",
    "            file : str\n",
    "                Name of the file\n",
    "\n",
    "        Returns:\n",
    "            w : numpy.ndarray\n",
    "                Observed wavelength bins\n",
    "            f : numpy.ndarray\n",
    "                Observed flux densities\n",
    "            n : numpy.ndarray\n",
    "                Observed Gaussian standard deviation of observed flux densities\n",
    "        '''\n",
    "\n",
    "        # Retrieve the data columns\n",
    "        w, f, n = np.loadtxt(file, delimiter='\\t', comments=('#', 'w'), usecols=(0,1,2), unpack=True)\n",
    "    \n",
    "        # If the file is not the stacked leaker spectrum. This step is necessary because \n",
    "        if 'leaker' not in file:\n",
    "\n",
    "            # Remove bins of extreme outliers\n",
    "            w = w[f < 1e-20]\n",
    "            n = n[f < 1e-20]\n",
    "            f = f[f < 1e-20]\n",
    "\n",
    "            # Convert from units of erg/s/cm^2/Hz to erg/s/cm^2/Å\n",
    "            f = f * 2.998e18 / np.square(w)\n",
    "            n = n * 2.998e18 / np.square(w)\n",
    "\n",
    "        return w, f, n\n",
    "\n",
    "    def two_peaks(x, amp_r, cen_r, width_r, skew_r, amp_c, cen_c, width_c, cntm):\n",
    "\n",
    "        return amp_r * np.exp(-((x - cen_r) / width_r)**2 / 2) * (1 + erf(skew_r * ((x - cen_r) / width_r) / np.sqrt(2))), \\\n",
    "            amp_c * np.exp(-((x - cen_c) / width_c)**2 / 2), \\\n",
    "            cntm\n",
    "\n",
    "    def three_peaks(x, amp_b, cen_b, width_b, skew_b, amp_r, cen_r, width_r, skew_r, amp_c, cen_c, width_c, cntm):\n",
    "\n",
    "        return amp_b * np.exp(-((x - cen_b) / width_b)**2 / 2) * (1 + erf(skew_b * ((x - cen_b) / width_b) / np.sqrt(2))),  \\\n",
    "            amp_r * np.exp(-((x - cen_r) / width_r)**2 / 2) * (1 + erf(skew_r * ((x - cen_r) / width_r) / np.sqrt(2))), \\\n",
    "            amp_c * np.exp(-((x - cen_c) / width_c)**2 / 2), \\\n",
    "            cntm\n",
    "\n",
    "    # Slit dictionary containing, for each slit ID, from left to right: the file name containing the spectrum,\n",
    "    # redshift, magnification, peculiar velocity range used to constrain the central Lya peak for purpose of \n",
    "    slits = {\n",
    "        'NL' : ['sunburst_arc_nonleaker_stack_mage.txt', 0, 1, '#DC3220', two_peaks, '#DC3220', 's', 8],\n",
    "        'L' : ['sunburst_arc_leaker_stack_mage.txt', 0, 1, '#005AB5', three_peaks, '#005AB5', 'o', 8],\n",
    "        'M5' : ['sunburst_M-5-comb1_MWdr.txt', 2.37086, 51, '#DC3220', two_peaks, 'none', 's', 8],\n",
    "        'M4' : ['sunburst_M-4-comb1_MWdr.txt', 2.37073, 14.6, '#DC3220', two_peaks, 'none', 's', 8],\n",
    "        'M6' : ['sunburst_M-6-comb1_MWdr.txt', 2.37021, 147, '#DC3220', two_peaks, 'none', 's', 8],\n",
    "        'M3' : ['sunburst_M-3-comb1_MWdr.txt', 2.37025, 36, '#D35FB7', three_peaks, 'none', '*', 12],\n",
    "        'M0' : ['sunburst_M-0-comb1_MWdr.txt', 2.37014, 10, '#005AB5', three_peaks, 'none', 'o', 8],\n",
    "        'M2' : ['sunburst_M-2-comb1_MWdr.txt', 2.37017, 32, '#005AB5', three_peaks, 'none', 'o', 8],\n",
    "        'M7' : ['sunburst_M-7-comb1_MWdr.txt', 2.37044, 35, '#005AB5', three_peaks, 'none', 'o', 8],\n",
    "        'M8' : ['sunburst_M-8-comb1_MWdr.txt', 2.37024, 29, '#005AB5', three_peaks, 'none', 'o', 8],\n",
    "        'M9' : ['sunburst_M-9-comb1_MWdr.txt', 2.37030, 31, '#005AB5', three_peaks, 'none', 'o', 8]\n",
    "    }\n",
    "\n",
    "    # Array containing the padding between the plot and the row unit labels\n",
    "    labelpads = np.array([27,20,27,20,20,20,20,20], dtype=int)\n",
    "\n",
    "    # Establish common directories\n",
    "    home = os.getcwd()\n",
    "    data = f'{home}/data'\n",
    "    figs = f'{home}/figs'\n",
    "    results = f'{home}/results'\n",
    "\n",
    "    f_escs, n_f_escs = np.loadtxt(f'{results}/f_esc_lyc_measurements.txt', delimiter=' ', usecols=(4,5), unpack=True)\n",
    "    f_escs, n_f_escs = np.insert(f_escs, 0, [np.nan, np.nan]), np.insert(n_f_escs, 0, [np.nan, np.nan])\n",
    "\n",
    "    # Make the figure showing the individual Lya profiles\n",
    "\n",
    "    # Create the figure showing the Lya profiles and best-fit curves\n",
    "    fig_lya, ax_lya = plt.subplots(3,3, figsize=(12,12), sharex=True, constrained_layout=True)\n",
    "    \n",
    "    # Flatten the axes array for easier iteration\n",
    "    ax_lya_array = np.array(ax_lya).reshape(-1)\n",
    "\n",
    "    # Multiplicative scaling factor of the data\n",
    "    factor = 1e17\n",
    "\n",
    "    # For each slit ID apart from the first two (the stacked spectra)\n",
    "    for i, slit_id in enumerate(list(slits.keys())[2:]):\n",
    "\n",
    "        # Get the redshift, color to plot the Lya profile curve, and model function \n",
    "        # of the spectrum from the slit dictionary\n",
    "        z, mag, c, model = slits[slit_id][1], slits[slit_id][2], slits[slit_id][3], slits[slit_id][4]\n",
    "\n",
    "        # Get the data\n",
    "        w, f, n = extract_data(f'{data}/mage/{slits[slit_id][0]}')\n",
    "\n",
    "        # Convert the data to the rest frame\n",
    "        w, f, n = w / (1 + z), f * (1 + z) / mag, n * (1 + z) / mag\n",
    "\n",
    "        # Create the Lya peculiar velocities\n",
    "        v = 299792.458 * (w / 1215.67 - 1)\n",
    "\n",
    "        # Create a mask to apply to the data, masking between +/- 1100 km/s\n",
    "        mask = (v >= -1100) & (v <= 1100)\n",
    "\n",
    "        # Apply the mask to the data\n",
    "        v, f, n = v[mask], f[mask], n[mask]\n",
    "\n",
    "        # Get the median best-fit parameters of the model function\n",
    "        lya_best_fit_model_params = np.median(np.loadtxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_best_fit_model_parameters.txt', \n",
    "            delimiter=' '), axis=0)\n",
    "\n",
    "        # Add shading to indicate the 2σ uncertainties of the data\n",
    "        ax_lya_array[i].fill_between(v, (f - 2 * n) * factor, (f + 2 * n) * factor, step='mid', facecolor=c, alpha=0.3)\n",
    "\n",
    "        # Plot the data\n",
    "        ax_lya_array[i].plot(v, f * factor, c=c, ds='steps-mid')\n",
    "\n",
    "        # Get the best-fit curves of the individual peaks and the continuum level\n",
    "        best_fit_model_comps = model(v, *lya_best_fit_model_params)\n",
    "\n",
    "        # Set a variable for the best-fit continuum level from the best-fit curves\n",
    "        cntm = best_fit_model_comps[-1]\n",
    "\n",
    "        # For each best-fit curves of the individual peaks\n",
    "        for j, curve in enumerate(best_fit_model_comps[:-1]):\n",
    "\n",
    "            # Plot tbest-fit curve to the individual peak\n",
    "            ax_lya_array[i].plot(v, (curve + cntm) * factor / mag, c='black', ls='dotted')\n",
    "\n",
    "        # Plot the total best-fit curve to the profile\n",
    "        ax_lya_array[i].plot(v, (np.sum(best_fit_model_comps[:-1], axis=0) + cntm) * factor / mag, c='black', ls='dashed')\n",
    "\n",
    "        # Add a label to the corner of the plot indicating the slit ID\n",
    "        at = AnchoredText(slit_id, frameon=False, loc='upper right', prop=dict(fontsize='x-large'))       \n",
    "        ax_lya_array[i].add_artist(at)\n",
    "\n",
    "        # Set the x- and y-axis limits of the plot\n",
    "        ax_lya_array[i].set_xlim(-1000,1000)\n",
    "        ax_lya_array[i].set_ylim(bottom=0)\n",
    "\n",
    "        # Set the locations and labels of the x-axis ticks\n",
    "        ax_lya_array[i].set_xticks([-1000,-500,0,500,1000], \n",
    "            ['$-$1000','$-$500','0','$+$500','$+$1000'])\n",
    "\n",
    "        # Add ticks to each side of the plot, and point them inward\n",
    "        ax_lya_array[i].tick_params(left=True, bottom=True, right=True, top=True, direction='in')\n",
    "\n",
    "        # Set the plot to be square\n",
    "        ax_lya_array[i].set_aspect(1 / ax_lya_array[i].get_data_ratio(), adjustable='box')\n",
    "\n",
    "    # Add x- and y-axis labels to the figure\n",
    "    fig_lya.supylabel(f'Flux density ($10^{{-{abs(floor(log10(abs(factor))))}}}$ erg s$^{{-1}}$ cm$^{{-2}}$ Å$^{{-1}}$)', fontsize='large')\n",
    "    ax_lya[2,1].set_xlabel('Velocity (km s$^{-1}$)', fontsize='large')\n",
    "\n",
    "    # Save the figure\n",
    "    fig_lya.savefig(f'{figs}/lya_spectra/lya_fits.pdf', bbox_inches='tight')\n",
    "\n",
    "    # Make the figure showing the stacked Lya spectra\n",
    "\n",
    "    # Instantiate the figure showing the stacked Lya spectra and best-fit models\n",
    "    fig_lya_stack, ax_lya_stack = plt.subplots(3,1, figsize=(3,9), sharex=True)\n",
    "\n",
    "    # For the first two slit IDs (the stacked spectra)\n",
    "    for i, slit_id in enumerate(list(slits.keys())[:2]):\n",
    "\n",
    "        # Get the redshift, color to plot the Lya profile curve, and model function \n",
    "        # of the spectrum from the slit dictionary\n",
    "        z, c, model = slits[slit_id][1], slits[slit_id][3], slits[slit_id][4]\n",
    "\n",
    "        # Get the data\n",
    "        w, f, n = extract_data(f'{data}/mage/{slits[slit_id][0]}')\n",
    "\n",
    "        # Convert the data to the rest frame\n",
    "        w, f, n = w / (1 + z), f * (1 + z), n * (1 + z)\n",
    "\n",
    "        # Create the Lya peculiar velocities\n",
    "        v = 299792.458 * (w / 1215.67 - 1)\n",
    "\n",
    "        # Create a mask to apply to the data, masking between +/- 1100 km/s\n",
    "        mask = (v >= -1100) & (v <= 1100)\n",
    "\n",
    "        # Apply the mask to the data\n",
    "        v, f, n = v[mask], f[mask], n[mask]\n",
    "\n",
    "        # Get the median best-fit parameters of the model function\n",
    "        lya_best_fit_model_params = np.median(np.loadtxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_best_fit_model_parameters.txt', \n",
    "            delimiter=' '), axis=0)\n",
    "\n",
    "        # Get the best-fit curves of the individual peaks and the continuum level\n",
    "        best_fit_model_comps = model(v, *lya_best_fit_model_params)\n",
    "\n",
    "        # Add shading to indicate the 3σ uncertainties of the data\n",
    "        ax_lya_stack[0].fill_between(v, f - 3 * n, f + 3 * n, step='mid', facecolor=c, alpha=0.3)\n",
    "        ax_lya_stack[i+1].fill_between(v, f - 3 * n, f + 3 * n, step='mid', facecolor=c, alpha=0.3)\n",
    "\n",
    "        # Plot the spectrum on the subplot of the shared profile and a standalone subplot for the\n",
    "        # profile\n",
    "        ax_lya_stack[0].plot(v, f, color=c, ds='steps-mid')\n",
    "        ax_lya_stack[i+1].plot(v, f, color=c, ds='steps-mid')\n",
    "\n",
    "        # Set a variable for the best-fit continuum level from the best-fit curves\n",
    "        cntm = best_fit_model_comps[-1]\n",
    "\n",
    "        # For each best-fit curves of the individual peaks\n",
    "        for j, curve in enumerate(best_fit_model_comps[:-1]):\n",
    "\n",
    "            # Plot tbest-fit curve to the individual peak\n",
    "            ax_lya_stack[i+1].plot(v, (curve + cntm), c='black', ls='dotted')\n",
    "\n",
    "        # Plot the total fit against the standalone spectrum\n",
    "        ax_lya_stack[i+1].plot(v, np.sum(best_fit_model_comps[:-1], axis=0) + best_fit_model_comps[-1], color='black', ls='dashed')\n",
    "\n",
    "        # Add an annotation with the slit ID to a corner of the standalone spectrum subplot\n",
    "        at = AnchoredText(slit_id, frameon=False, loc='upper right', prop=dict(fontsize='large'))\n",
    "        ax_lya_stack[i+1].add_artist(at)\n",
    "\n",
    "        # Set the x- and y-axis limits of the standalone spectrum subplot\n",
    "        ax_lya_stack[i+1].set_xlim(-1000,1000)\n",
    "        ax_lya_stack[i+1].set_ylim(bottom=0)\n",
    "\n",
    "        # Add inward-facing ticks to all sides of the standalone spectrum subplot\n",
    "        ax_lya_stack[i+1].tick_params(left=True, bottom=True, right=True, top=True, direction='in')\n",
    "\n",
    "    # Add x- and y-axis labels to the figure\n",
    "    ax_lya_stack[-1].set_xlabel('Velocity (km s$^{-1}$)', fontsize='large')\n",
    "    ax_lya_stack[1].set_ylabel('Flux density (normalized)', fontsize='large')\n",
    "\n",
    "    # Set the y-axis limits of the shared subplot\n",
    "    ax_lya_stack[0].set_ylim(bottom=0)\n",
    "\n",
    "    # Set the y-axis limits of the stacked non-LyC-leaking apertures subplot\n",
    "    ax_lya_stack[1].set_ylim(top=12)\n",
    "\n",
    "    # Set the x-axis tick locations and labels of the shared subplot\n",
    "    ax_lya_stack[0].set_xticks([-1000,-500,0,500,1000], ['$-$1000','$-$500','$0$','$+$500','$+$1000'])\n",
    "\n",
    "    # Set the y-axis tick locations and labels of the stacked nonleaker spectrum subplot\n",
    "    ax_lya_stack[1].set_yticks([0,3,6,9], ['0','3','6','9'])\n",
    "\n",
    "    # Add inward-facing ticks to each side of the top subplot\n",
    "    ax_lya_stack[0].tick_params(left=True, bottom=True, right=True, top=True, direction='in')\n",
    "\n",
    "    # Eliminate space between the subplots\n",
    "    fig_lya_stack.subplots_adjust(hspace=0, wspace=0)\n",
    "\n",
    "    # Save the figure as a PDF\n",
    "    fig_lya_stack.savefig(f'{figs}/lya_spectra/lya_fits_stack.pdf', bbox_inches='tight')\n",
    "\n",
    "    # Make the corner plot figure of the Lya measurements and LyC escape fraction\n",
    "\n",
    "    # Instantiate the figure\n",
    "    fig_corner, ax_corner = plt.subplots(8,8, figsize=(16,16), sharex='col', sharey='row', constrained_layout=True)\n",
    "    fig_corner.tight_layout()\n",
    "\n",
    "    # Make arrays with the titles of the columns and rows\n",
    "    col_titles = np.array([r'$v_{\\rm{sep}}$', 'FWHM (blue)', 'FWHM (center)', 'FWHM (red)', r'$f_{\\rm{min}}/f_{\\rm{cont}}$', 'EW', r'$f_{\\rm{cen}}$', 'Luminosity'], dtype=str)\n",
    "    row_titles = np.array(['FWHM (blue)', 'FWHM (center)', 'FWHM (red)', r'$f_{\\rm{min}}/f_{\\rm{cont}}$', 'EW', r'$f_{\\rm{cen}}$', 'Luminosity', r'$f_{\\rm{esc}}^{\\rm{LyC}}$'], dtype=str)\n",
    "\n",
    "    # Make arrays with the unit labels of the columns and rows\n",
    "    col_labels = np.array(['(km s$^{-1}$)', '(km s$^{-1}$)', '(km s$^{-1}$)', '(km s$^{-1}$)', '', '(Å)', '(%)', '(10$^{41}$ erg s$^{-1}$)'], dtype=str)\n",
    "    row_labels = np.array(['(km s$^{-1}$)', '(km s$^{-1}$)', '(km s$^{-1}$)', '', '(Å)', '(%)', '(10$^{41}$ erg s$^{-1}$)', '(%)'], dtype=str)\n",
    "\n",
    "    # Open the .txt file containing the measurement of the statistical correlations between the different Lya measurements\n",
    "    file = open(f'{results}/tables/lya_measurements_statistical_correlations_table.tex')\n",
    "\n",
    "    # Get just the tabulated values, split apart by file line\n",
    "    lines = file.readlines()[10:46]\n",
    "\n",
    "    # Make an array with the numbered location codes of the location to annotate \n",
    "    # the Pearson correlation coefficient of the measurements on each plot\n",
    "    locs = np.array([\n",
    "        3,\n",
    "        3,4,\n",
    "        2,2,2,\n",
    "        3,3,6,7,\n",
    "        3,4,6,7,4,\n",
    "        3,4,6,7,2,2,\n",
    "        4,4,2,4,2,2,2,\n",
    "        4,1,2,1,2,2,2,2], \n",
    "        dtype=int)\n",
    "\n",
    "    # For each row of subplots in the figure except the last row (the LyC escape fractions row)\n",
    "    for i, row in enumerate(ax_corner[:-1]):\n",
    "\n",
    "        # Set the column title of the last subplot in the row\n",
    "        ax_corner[i,i].set_title(col_titles[i], fontsize='large')\n",
    "\n",
    "        # Set the row title\n",
    "        ax_corner[i,0].set_ylabel(row_titles[i], fontsize='large')\n",
    "\n",
    "        # Set the column unit label of the last column in the row\n",
    "        ax_corner[-1,i].set_xlabel(col_labels[i], fontsize='large')\n",
    "\n",
    "        # Make a twin set of axes of the last subplot in the row\n",
    "        subplot_twin = ax_corner[i,i].twinx()\n",
    "\n",
    "        # Set the row unit label of the last subplot in the row\n",
    "        subplot_twin.set_ylabel(row_labels[i], fontsize='large', rotation=-90, labelpad=labelpads[i])\n",
    "\n",
    "        # Make an empty set of ticks and labels on the twin axes (otherwise, \n",
    "        # automatic ticks in the axes coordinate system will appear)\n",
    "        subplot_twin.set_yticks([0.5], ['   '])\n",
    "        subplot_twin.tick_params(left=False, bottom=False, right=False, top=False)\n",
    "\n",
    "        # For each subplot in the row\n",
    "        for j, subplot in enumerate(row):\n",
    "\n",
    "            # If the subplot is above the main diagonal\n",
    "            if j > i:\n",
    "\n",
    "                # Hide the subplot\n",
    "                subplot.axis('off')\n",
    "\n",
    "            else:\n",
    "\n",
    "                # For each slit ID\n",
    "                for k, slit_id in enumerate(slits):\n",
    "\n",
    "                    # If the row is the Lya luminosity and the slit ID is M0, pass, since the Lya luminosity \n",
    "                    # of slit M0 is unphysically large due to a poor fluxing of the observations\n",
    "                    if i == 6 and slit_id == 'M0':\n",
    "                        pass\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        # Get the color, marker, and marker size properties of the slit ID\n",
    "                        ec, fc, marker, size = slits[slit_id][3], slits[slit_id][5], slits[slit_id][6], slits[slit_id][7]\n",
    "\n",
    "                        # Get the measurements of the plotted measurement pair for the given slit ID\n",
    "                        x, y = np.loadtxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_measurements.txt', \n",
    "                            delimiter=' ', comments='#', usecols=(j+1,i+2), unpack=True)\n",
    "\n",
    "                        # Calculate the medians of the measurement distributions\n",
    "                        medians = [np.median(x), np.median(y)]\n",
    "\n",
    "                        # Make arrays of the uncertainties of the two measurement distributions\n",
    "                        x_error = np.array([[medians[0] - np.percentile(x, 16)], [np.percentile(x, 84) - medians[0]]], dtype=np.float64)\n",
    "                        y_error = np.array([[medians[1] - np.percentile(y, 16)], [np.percentile(y, 84) - medians[1]]], dtype=np.float64)\n",
    "\n",
    "                        # If the row is the Lya luminosity, rescale the data by a multiplicative factor to prevent an annotated scientific notation label\n",
    "                        medians, y_error = ([medians[0], medians[1] * 1e-41], y_error * 1e-41) if i==6 else (medians, y_error)\n",
    "\n",
    "                        # Plot the medians with uncertainties\n",
    "                        subplot.errorbar(*medians, xerr=x_error, yerr=y_error, lw=1, mec=ec, mfc=fc, ecolor=ec, marker=marker, markersize=size)\n",
    "\n",
    "                # Hide labels on the left and top sides of the subplot\n",
    "                subplot.tick_params(labelleft=False, labeltop=False)\n",
    "\n",
    "                # If the subplot is on the main diagonal\n",
    "                if i == j:\n",
    "\n",
    "                    # Add tick labels to the right side of the subplot\n",
    "                    subplot.tick_params(labelright=True)\n",
    "\n",
    "                # Add inward-facing ticks to each side of the subplot\n",
    "                subplot.tick_params(left=True, bottom=True, right=True, top=True, direction='in')\n",
    "\n",
    "                # Add the Pearson correlation coefficient measurement for the measurement pair to the subplot\n",
    "                at = AnchoredText(lines[int(i + (7 * (7 + 1) / 2) - ((7 - i) * (7 - i + 1) / 2))].split('& ')[1], loc=locs[int(i * (i + 1) / 2 + j)], frameon=False)\n",
    "                subplot.add_artist(at)\n",
    "\n",
    "    # For each subplot in the last row\n",
    "    for i in range(len(ax_corner[-1])):\n",
    "\n",
    "        # For each slit ID\n",
    "        for j, slit_id in enumerate(slits):\n",
    "\n",
    "            # If the subplot is the last in the row (i.e., the column is Lya luminosity) and the slit ID is M0, \n",
    "            # pass, since the Lya luminosity in that instance is unphysically large due to a poor fluxing\n",
    "            if i == len(ax_corner[0]) - 1 and slit_id == 'M0':\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Get the color, marker, and marker size properties of the slit ID\n",
    "                ec, fc, marker, size = slits[slit_id][3], slits[slit_id][5], slits[slit_id][6], slits[slit_id][7]\n",
    "\n",
    "                # Get the measurements of the plotted measurement pair for the given slit ID\n",
    "                x = np.loadtxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_measurements.txt', \n",
    "                    delimiter=' ', comments='#', usecols=(i+1), unpack=True)\n",
    "                y = f_escs[j]\n",
    "\n",
    "                # Make an array of the x-axis uncertainties\n",
    "                x_error = np.array([[np.median(x) - np.percentile(x, 16)], [np.percentile(x, 84) - np.median(x)]], dtype=np.float64)\n",
    "\n",
    "                # Rescale the data if the subplot is the last in the row (i.e., if it is the Lya luminosity column) for \n",
    "                # better aesthetics to prevent an automatic scientific notation factor\n",
    "                x, x_error = (x * 1e-41, x_error * 1e-41) if i==len(ax_corner[0]) - 1 else (x, x_error)\n",
    "\n",
    "                # Plot the measurements with uncertainties\n",
    "                ax_corner[-1,i].errorbar(np.median(x), y, xerr=x_error, yerr=n_f_escs[j], lw=1, mec=ec, mfc=fc, ecolor=ec, marker=marker, markersize=size)\n",
    "\n",
    "        # Add inward-facing ticks to each side of the subplot and tick labels to the bottom of the subplot\n",
    "        ax_corner[-1,i].tick_params(left=True, bottom=True, right=True, top=True,\n",
    "            labelleft=False, labelbottom=True, labeltop=False, direction='in')\n",
    "\n",
    "        # Add the Pearson correlation coefficient measurement for the measurement pair to the subplot\n",
    "        at = AnchoredText(lines[int(i + (7 * (7 + 1) / 2) - ((7 - i) * (7 - i + 1) / 2))].split('& ')[1], loc=locs[int(7 * (7 + 1) / 2 + i)], frameon=False)\n",
    "        ax_corner[-1,i].add_artist(at)\n",
    "\n",
    "    # Set the column title of the bottom right corner subplot\n",
    "    ax_corner[-1,-1].set_title(col_titles[-1], fontsize='large')\n",
    "\n",
    "    # Set the row title of the bottom row\n",
    "    ax_corner[-1,0].set_ylabel(row_titles[-1], fontsize='large')\n",
    "\n",
    "    # Add the column unit label of the last column\n",
    "    ax_corner[-1,-1].set_xlabel(col_labels[-1], fontsize='large')\n",
    "\n",
    "    # Make a twin axes of the bottom right subplot to add the y-axis label\n",
    "    ax_twin = ax_corner[-1,-1].twinx()\n",
    "\n",
    "    # Add the row unit label \n",
    "    ax_twin.set_ylabel(row_labels[-1], fontsize='large', rotation=-90, labelpad=labelpads[-1])\n",
    "\n",
    "    # Make an empty set of ticks and labels on the twin axes (otherwise, \n",
    "    # automatic ticks in the axes coordinate system will appear)\n",
    "    ax_twin.set_yticks([0.5], ['   '])\n",
    "    ax_twin.tick_params(left=False, bottom=False, right=False, top=False)\n",
    "\n",
    "    # Add inward-facing ticks to all sides and tick labels to the bottom and right sides of \n",
    "    # the bottom right corner subplot\n",
    "    ax_corner[-1,-1].tick_params(left=True, bottom=True, right=True, top=True, \n",
    "        labelleft=False, labelbottom=True, labelright=True, labeltop=False, direction='in')\n",
    "\n",
    "    # Save the figure\n",
    "    fig_corner.savefig(f'{figs}/lya_measurements_corner.pdf', bbox_inches='tight')\n",
    "\n",
    "def tabulate():\n",
    "\n",
    "    '''\n",
    "    Saves the LaTeX-formatted tables (the Lya measurements, statistical correlations, \n",
    "    and model parameters) as .txt files\n",
    "    '''\n",
    "\n",
    "    def round_to_uncertainties(median, lower, upper):\n",
    "\n",
    "        '''\n",
    "        Round the uncertainties to one significant figure and the median to\n",
    "        the most precise significant digit of the two uncertainties\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "            median : numpy.float64\n",
    "                Median value of the measurement distribution\n",
    "\n",
    "            lower : numpy.float64\n",
    "                Absolute difference between the median and 16th percentile of \n",
    "                the measurement distribution\n",
    "\n",
    "            upper : numpy.float64\n",
    "                Absolute difference between the median and 84th percentile of\n",
    "                the measurement distribution\n",
    "\n",
    "        Returns:\n",
    "\n",
    "            median : str\n",
    "                Median value of the measurement distribution, rounded to the most precise\n",
    "                significant digit of the one significant figure-rounded uncertainties\n",
    "\n",
    "            lower : str\n",
    "                Absolute difference between the median and 16th percentile of the \n",
    "                measurement distribution, rounded to one significant figure\n",
    "\n",
    "            upper : str\n",
    "                Absolute difference between the median and 84th percentile of the \n",
    "                measurement distribution, rounded to one significant figure\n",
    "        '''\n",
    "\n",
    "        # Round the lower and upper uncertainties to one significant figure\n",
    "        lower = sigfig.round(lower, sigfigs=1, type=str)\n",
    "        upper = sigfig.round(upper, sigfigs=1, type=str)\n",
    "\n",
    "        # Get whichever bound is smaller, in order to round the median to\n",
    "        # the digit of that bound's significant figure\n",
    "        ref = min(np.array([lower, upper], dtype=str), key=float)\n",
    "\n",
    "        # If the smaller bound is less than 1\n",
    "        if '.' in ref:\n",
    "\n",
    "            # Round the median to the same digit as the smaller bound's only significant figure\n",
    "            median = sigfig.round(median, decimals=len(ref.split('.')[1]), type=str)\n",
    "\n",
    "        # If the smaller bound is greater than 1\n",
    "        elif '.' not in ref:\n",
    "\n",
    "            # Round the median to the same digit as the smaller bound's only significant figure\n",
    "            median = sigfig.round(median, len(str(median).replace('-','').split('.')[0]) - len(ref) + 1, type=str)\n",
    "\n",
    "        return median, lower, upper\n",
    "            \n",
    "    # Dictionary containing the spectral resolutions of the slits\n",
    "    slits = {\n",
    "        'NL' : [5400],\n",
    "        'L' : [5300],\n",
    "        'M5' : [5500],\n",
    "        'M4' : [5400],\n",
    "        'M6' : [5300],\n",
    "        'M3' : [5500],\n",
    "        'M0' : [4700],\n",
    "        'M2' : [5300],\n",
    "        'M7' : [5200],\n",
    "        'M8' : [5200],\n",
    "        'M9' : [5500]\n",
    "    }\n",
    "\n",
    "    # Establish common directories\n",
    "    home = os.getcwd()\n",
    "    results = f'{home}/results'\n",
    "\n",
    "    # Make the Lya measurements table    \n",
    "\n",
    "    # Header of the table containing the Lya measurements\n",
    "    table = '\\\\begin{deluxetable*}{cllllllll}[ht!]\\n\\n' \\\n",
    "        + '\\\\tablecaption{Ly$\\\\alpha$ measurements \\label{tab:lya_params}}\\n\\n' \\\n",
    "        + '\\\\tablehead{\\n' \\\n",
    "        + '\\t\\colhead{Slit} & \\colhead{$v_{\\\\rm{sep}}$} & \\colhead{FWHM (blue)} & \\colhead{FWHM (center)} & \\colhead{FWHM (red)} & \\colhead{$f_{\\\\rm{min}}/f_{\\\\rm{cont}}$} & \\colhead{EW} & \\colhead{$f_{\\\\rm{cen}}$} & \\colhead{Luminosity}\\n' \\\n",
    "        + '\\t\\\\\\\\\\n' \\\n",
    "        + '\\t\\colhead{} &\\n' \\\n",
    "        + '\\t\\colhead{[km s$^{-1}$]} &\\n' \\\n",
    "        + '\\t\\colhead{[km s$^{-1}$]} &\\n' \\\n",
    "        + '\\t\\colhead{[km s$^{-1}$]} &\\n' \\\n",
    "        + '\\t\\colhead{[km s$^{-1}$]} &\\n' \\\n",
    "        + '\\t\\colhead{} &\\n' \\\n",
    "        + '\\t\\colhead{[\\AA]} &\\n' \\\n",
    "        + '\\t\\colhead{[\\%]} &\\n' \\\n",
    "        + '\\t\\colhead{[$10^{41}$ erg s$^{-1}$]}\\n' \\\n",
    "        + '}\\n\\n' \\\n",
    "        + '\\startdata\\n'\n",
    "\n",
    "    # For each slit ID\n",
    "    for i, slit_id in enumerate(slits):\n",
    "\n",
    "        # Load the Lya measurements of that slit\n",
    "        slit_lya_measurements = np.loadtxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_measurements.txt', delimiter=' ', comments='#').T\n",
    "        \n",
    "        # Remove the spectral resolution measurements\n",
    "        slit_lya_measurements = slit_lya_measurements[1:]\n",
    "\n",
    "        # Add a separating horizontal line between the stacked spectra, the individual LyC-leaking spectra, and the individual non-LyC-leaking spectra\n",
    "        table = table + '\\hline\\n' if slit_id in ['M5', 'M0'] else table + ''\n",
    "\n",
    "        # Add the slit ID to the table row\n",
    "        table = table + f'{slit_id} '\n",
    "\n",
    "        # For each set of Lya measurements for the slit\n",
    "        for j, measurements in enumerate(slit_lya_measurements):\n",
    "\n",
    "            # Determine the median and lower and upper uncertainties \n",
    "            median = np.percentile(measurements, 50)\n",
    "            lower = median - np.percentile(measurements, 16)\n",
    "            upper = np.percentile(measurements, 84) - median\n",
    "\n",
    "            # Rescale the median and lower and upper measurements if the measurement is the Lya luminosity\n",
    "            median, lower, upper = (median / 1e41, lower / 1e41, upper / 1e41) if j+1 == len(slit_lya_measurements) else (median, lower, upper)\n",
    "\n",
    "            # If the measurement was not invalid for the given slit (e.g., Lya peak \n",
    "            # separations for profiles missing one of the two peaks would be invalid)\n",
    "            if not np.isnan(median):\n",
    "\n",
    "                # Round the uncertainties to one significant figure and the median to match \n",
    "                # the most significant digit between the two uncertainties\n",
    "                median, lower, upper = round_to_uncertainties(median, lower, upper)\n",
    "\n",
    "                # If the measurement is the central Lya peak FWHM\n",
    "                if j==2:\n",
    "\n",
    "                    # Get the spectral resolution of the spectrum in units of km/s\n",
    "                    R = 299792.458 / slits[slit_id][0]\n",
    "\n",
    "                    # If the 16th percentile of the measurement distribution is less than\n",
    "                    # the instrumental FWHM\n",
    "                    if eval(f'{median} - {lower}') < R:\n",
    "\n",
    "                        # Report the measurement by listing the 84th percentile of the \n",
    "                        # measurement distribution as an upper bound\n",
    "                        table = table + f'& \\lesssim {eval(f\"{median} + {upper}\")}'\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        # Add the formatted measurement to the table\n",
    "                        table = table + f'& ${median}_{{-{lower}}}^{{+{upper}}}$ '           \n",
    "                else:\n",
    "\n",
    "                    # Add the formatted measurement to the table\n",
    "                    table = table + f'& ${median}_{{-{lower}}}^{{+{upper}}}$ '\n",
    "            \n",
    "            else:\n",
    "\n",
    "                # Add a dash to the table to represent the measurement is invalid\n",
    "                table = table + '& $---$ '\n",
    "        \n",
    "        # Add a LaTeX row break character to the end of the two, unless the row is the\n",
    "        # last row, and add a line break to the .txt file\n",
    "        table = table + '\\\\\\\\' + '\\n' if i != len(slits) - 1 else table + '\\n'\n",
    "\n",
    "    # Add a footer to the table\n",
    "    table = table + '\\\\enddata\\n\\n' \\\n",
    "        + '\\\\tablecomments{From left to right: slit label, peak separation between the redshifted and blueshifted Ly$\\\\alpha$ peaks ' \\\n",
    "        + '(km s$^{-1}$), FWHM of the blueshifted, central, and redshifted Ly$\\\\alpha$ peaks (km s$^{-1}$), respectively, ratio between ' \\\n",
    "        + 'the `minimum\\' flux density between the redshifted and blueshifted Ly$\\\\alpha$ peaks and the local continuum flux density, ' \\\n",
    "        + 'rest-frame Ly$\\\\alpha$ equivalent width ({\\AA}), central escape fraction (\\%), and magnification-corrected Ly$\\\\alpha$ luminosity ' \\\n",
    "        + '(10$^{41}$ erg s$^{-1}$). Because the deconvolved FWHMs of the central Ly$\\\\alpha$ peaks of slits M4 and M5 were not significantly ' \\\n",
    "        + 'greater than the instrumental line spread function FWHM ($\\sim55$ km s$^{-1}$), we quote the 84th percentiles of those measurements ' \\\n",
    "        + 'as an upper bound on the intrinsic FWHM of their central Ly$\\\\alpha$ peaks.}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\tablenotetext{a}{Slit M0\\'s observation was taken through thin cloud cover that prevented an accurate fluxing, so its ' \\\n",
    "        + 'significantly larger luminosity is not an accurate estimate. We do not include this data point in any figures or when ' \\\n",
    "        + 'estimating any correlations involving the Ly$\\\\alpha$ luminosity. See Table \\\\ref{tab:mage_log} for more information about ' \\\n",
    "        + 'the observation.}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\end{deluxetable*}'\n",
    "\n",
    "    # Save the table\n",
    "    f = open(f'{results}/tables/lya_measurements_table.tex', 'w', encoding='utf-8')\n",
    "    f.write(table)\n",
    "    f.close()\n",
    "\n",
    "    # Make the statistical correlation measurements table\n",
    "\n",
    "    # Create the header for the statistical correlation measurements table\n",
    "    table = '\\\\begin{deluxetable}{lrr}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\tablecaption{Correlations between Ly$\\\\alpha$ and LyC parameters \\label{tab:param_correlations}}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\tablehead{\\n' \\\n",
    "        + '\\t\\colhead{} & \\colhead{$r$} & \\colhead{$\\\\tau$}\\n' \\\n",
    "        + '}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\startdata\\n'\n",
    "\n",
    "    # Make the array containing the row labels of the table\n",
    "    row_labels = np.array([\n",
    "        'FWHM (b) - $v_{\\\\rm{sep}}$',\n",
    "        'FWHM (c) - $v_{\\\\rm{sep}}$',\n",
    "        'FWHM (r) - $v_{\\\\rm{sep}}$',\n",
    "        '$f_{\\\\rm{min}}/f_{\\\\rm{cont}} - $v_{\\\\rm{sep}}$',\n",
    "        'Ly$\\\\alpha$ EW - $v_{\\\\rm{sep}}$',\n",
    "        '$f_{\\\\rm{cen}}$ - $v_{\\\\rm{sep}}$',\n",
    "        'Ly$\\\\alpha$ $L$ - $v_{\\\\rm{sep}}$',\n",
    "        '$f_{\\\\rm{esc}}^{\\\\rm{LyC}}$ - $v_{\\\\rm{sep}}$',\n",
    "        'FWHM (c) - FWHM (b)',\n",
    "        'FWHM (r) - FWHM (b)',\n",
    "        '$f_{\\\\rm{min}}/f_{\\\\rm{cont}} - FWHM (b)',\n",
    "        'Ly$\\\\alpha$ EW - FWHM (b)',\n",
    "        '$f_{\\\\rm{cen}}$ - FWHM (b)',\n",
    "        'Ly$\\\\alpha$ $L$ - FWHM (b)',\n",
    "        '$f_{\\\\rm{esc}}^{\\\\rm{LyC}}$ - FWHM (b)',\n",
    "        'FWHM (r) - FWHM (c)',\n",
    "        '$f_{\\\\rm{min}}/f_{\\\\rm{cont}} - FWHM (c)',\n",
    "        'Ly$\\\\alpha$ EW - FWHM (c)',\n",
    "        '$f_{\\\\rm{cen}}$ - FWHM (c)',\n",
    "        'Ly$\\\\alpha$ $L$ - FWHM (c)',\n",
    "        '$f_{\\\\rm{esc}}^{\\\\rm{LyC}}$ - FWHM (c)',\n",
    "        '$f_{\\\\rm{min}}/f_{\\\\rm{cont}} - FWHM (r)',\n",
    "        'Ly$\\\\alpha$ EW - FWHM (r)',\n",
    "        '$f_{\\\\rm{cen}}$ - FWHM (r)',\n",
    "        'Ly$\\\\alpha$ $L$ - FWHM (r)',\n",
    "        '$f_{\\\\rm{esc}}^{\\\\rm{LyC}}$ - FWHM (r)',\n",
    "        'Ly$\\\\alpha$ EW - f_{\\\\rm{min}}/f_{\\\\rm{cont}}',\n",
    "        '$f_{\\\\rm{cen}}$ - f_{\\\\rm{min}}/f_{\\\\rm{cont}}',\n",
    "        'Ly$\\\\alpha$ $L$ - f_{\\\\rm{min}}/f_{\\\\rm{cont}}',\n",
    "        '$f_{\\\\rm{esc}}^{\\\\rm{LyC}}$ - f_{\\\\rm{min}}/f_{\\\\rm{cont}}',\n",
    "        '$f_{\\\\rm{cen}}$ - Ly$\\\\alpha$ EW',\n",
    "        'Ly$\\\\alpha$ $L$ - Ly$\\\\alpha$ EW',\n",
    "        '$f_{\\\\rm{esc}}^{\\\\rm{LyC}}$ - Ly$\\\\alpha$ EW',\n",
    "        'Ly$\\\\alpha$ $L$ - $f_{\\\\rm{cen}}',\n",
    "        '$f_{\\\\rm{esc}}^{\\\\rm{LyC}}$ - $f_{\\\\rm{cen}}',\n",
    "        '$f_{\\\\rm{esc}}^{\\\\rm{LyC}}$ - Ly$\\\\alpha$ $L$',\n",
    "    ], dtype=str)\n",
    "\n",
    "    # Get the statistical correlation measurements\n",
    "    corr_coefs = np.loadtxt(f'{results}/lya_fits/mc_sim_lya_measurements_statistical_correlations.txt', delimiter='\\t', comments='#')\n",
    "\n",
    "    # For each measurement pair\n",
    "    for i in range(len(corr_coefs.T) // 2):\n",
    "\n",
    "        # Add the row label to the row\n",
    "        table = table + f'{row_labels[i]} '\n",
    "\n",
    "        # Get the measurements of the two statistical correlation coefficients for the measurement pair\n",
    "        corr_coef_pair = corr_coefs[:,2*i:2*i+2]\n",
    "\n",
    "        # Calculate the medians of the two statistical correlation coefficients\n",
    "        medians = np.median(corr_coef_pair, axis=0)\n",
    "\n",
    "        # Calculate the lower and upper uncertainties of the two statistical correlation coefficients\n",
    "        lowers = medians - np.percentile(corr_coef_pair, 16, axis=0)\n",
    "        uppers = np.percentile(corr_coef_pair, 84, axis=0) - medians\n",
    "\n",
    "        # For each statistical correlation coefficient type\n",
    "        for j in range(len(medians)):\n",
    "\n",
    "            # Round the lower and upper uncertainties to one significant figure and the median \n",
    "            # to the most significant digit between the uncertainties\n",
    "            median, lower, upper = round_to_uncertainties(medians[j], lowers[j], uppers[j])\n",
    "\n",
    "            # Add the formatted measurement to the table row\n",
    "            table = table + f'& ${median}_{{-{lower}}}^{{+{upper}}}$ '\n",
    "\n",
    "        # Add a LaTeX row break character to the end of the row, unless the row is the\n",
    "        # last row, and add a line break to the .txt file\n",
    "        table = table + '\\\\\\\\\\n' if i != len(corr_coefs.T) / 2 - 1 else table + '\\n'\n",
    "    \n",
    "    # Add a footer to the table\n",
    "    table = table + '\\enddata\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\tablecomments{Statistical correlations between the Ly$\\\\alpha$ parameters and $f_{\\\\rm{esc}}^{\\\\rm{LyC}}$. ' \\\n",
    "        + 'From left to right: the parameter pair, Pearson correlation coefficient $r$, and type `b\\' Kendall rank ' \\\n",
    "        + 'correlation coefficient $\\\\tau$. The minimal number of data points (no more than 11 for any pair of parameters) ' \\\n",
    "        + 'means there are not many unique values of $\\\\tau$, which causes many of the listed values and uncertainties ' \\\n",
    "        + 'to be similar, or in extreme cases for high SNR parameter measurements, for the 16th and 84th percentiles ' \\\n",
    "        + 'listed to be the same value as the median.}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\end{deluxetable}'\n",
    "    \n",
    "    # Save the table\n",
    "    f = open(f'{results}/tables/lya_measurements_statistical_correlations_table.tex', 'w', encoding='utf-8')\n",
    "    f.write(table)\n",
    "    f.close()\n",
    "\n",
    "    # Make the table of the best-fit Lya model parameters    \n",
    "\n",
    "    # Make the header\n",
    "    table = '\\\\begin{deluxetable}{lcccc}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\rotate\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\tablecaption{Ly$\\\\alpha$ modeling best-fit parameters \\label{tab:fit_results}}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\tablehead{\\n' \\\n",
    "        + '\\t\\colhead{Slit} & \\colhead{Blue peak} & \\colhead{Red peak} & \\colhead{Central peak} & \\colhead{Continuum}\\n' \\\n",
    "        + '}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\startdata\\n'\n",
    "\n",
    "    # For each slit ID\n",
    "    for i, slit_id in enumerate(slits):\n",
    "\n",
    "        # Add a separating horizontal line between the LyC-leaking and non-LyC-leaking spectra\n",
    "        table = table + '\\hline\\n' if slit_id == 'M0' else table + ''\n",
    "\n",
    "        # Add the slit ID to the row\n",
    "        table = table + f'{slit_id} & '\n",
    "\n",
    "        # Get the best-fit Lya model parameters of the spectrum\n",
    "        params = np.loadtxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_best_fit_model_parameters.txt', delimiter=' ', comments='#').T\n",
    "\n",
    "        # If the slit has a triple-peaked Lya profile\n",
    "        if slit_id in ['L', 'M3', 'M0', 'M2', 'M7', 'M8', 'M9']:\n",
    "\n",
    "            # For the best-fit parameter distributions of the blueshifted Lya peak\n",
    "            for j, param in enumerate(params[0:4]):\n",
    "\n",
    "                # Rescale the parameter distribution if it is the amplitude and the spectrum is \n",
    "                # not one of the stacked spectra (because the stacked spectra are normalized)\n",
    "                param = param * 1e16 if slit_id not in ['NL','L'] and j==0 else param\n",
    "\n",
    "                # Calculate the median of the measured parameter distribution\n",
    "                median = np.median(param)\n",
    "\n",
    "                # Calculate the lower and upper uncertainties of the parameter distribution\n",
    "                lower, upper = median - np.percentile(param, 16), np.percentile(param, 84) - median\n",
    "\n",
    "                # Round the lower and upper uncertainties to one significant figure and \n",
    "                # the median to the most significant digit between the two uncertainties\n",
    "                median, lower, upper = round_to_uncertainties(median, lower, upper)\n",
    "\n",
    "                # Add the formatted parameter measurement to the table row\n",
    "                table = table + f'${median}_{{-{lower}}}^{{+{upper}}}$ '\n",
    "\n",
    "                # Add an aesthetic, dividing '/' after the formatted parameter \n",
    "                # measurement, unless it is the last parameter in the loop\n",
    "                table = table + '/ ' if j != len(params[0:4]) - 1 else table\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Add an empty set of parameter measurements, since the spectrum's \n",
    "            # model did not include a blueshifted Lya peak\n",
    "            table = table + '$-$ / $-$ / $-$ / $-$ '\n",
    "\n",
    "        # Add a LaTeX table column break character\n",
    "        table = table + '& '\n",
    "\n",
    "        # For the best-fit parameter distributions of the redshifted Lya peak\n",
    "        for j, param in enumerate(params[-8:-4]):\n",
    "\n",
    "            # Rescale the parameter distribution if it is the amplitude and the spectrum is \n",
    "            # not one of the stacked spectra (because the stacked spectra are normalized)\n",
    "            param = param * 1e16 if slit_id not in ['NL','L'] and j==0 else param\n",
    "\n",
    "            # Calculate the median of the measured parameter distribution\n",
    "            median = np.median(param)\n",
    "\n",
    "            # Calculate the lower and upper uncertainties of the parameter distribution\n",
    "            lower, upper = median - np.percentile(param, 16), np.percentile(param, 84) - median\n",
    "\n",
    "            # Round the lower and upper uncertainties to one significant figure and \n",
    "            # the median to the most significant digit between the two uncertainties\n",
    "            median, lower, upper = round_to_uncertainties(median, lower, upper)\n",
    "\n",
    "            # Add the formatted parameter measurement to the table row\n",
    "            table = table + f'${median}_{{-{lower}}}^{{+{upper}}}$ '\n",
    "\n",
    "            # Add an aesthetic, dividing '/' after the formatted parameter \n",
    "            # measurement, unless it is the last parameter in the loop\n",
    "            table = table + '/ ' if j != len(params[-8:-4]) - 1 else table\n",
    "\n",
    "        # Add a LaTeX table column break character\n",
    "        table = table + '& '    \n",
    "\n",
    "        # For the best-fit parameter distributions of the central Lya peak\n",
    "        for j, param in enumerate(params[-4:-1]):\n",
    "\n",
    "            # Rescale the parameter distribution if it is the amplitude and the spectrum is \n",
    "            # not one of the stacked spectra (because the stacked spectra are normalized)\n",
    "            param = param * 1e16 if slit_id not in ['NL','L'] and j==0 else param\n",
    "\n",
    "            # Calculate the median of the measured parameter distribution\n",
    "            median = np.median(param)\n",
    "\n",
    "            # Calculate the lower and upper uncertainties of the parameter distribution\n",
    "            lower, upper = median - np.percentile(param, 16), np.percentile(param, 84) - median\n",
    "\n",
    "            # Round the lower and upper uncertainties to one significant figure and \n",
    "            # the median to the most significant digit between the two uncertainties\n",
    "            median, lower, upper = round_to_uncertainties(median, lower, upper)\n",
    "\n",
    "            # Add the formatted parameter measurement to the table row\n",
    "            table = table + f'${median}_{{-{lower}}}^{{+{upper}}}$ '\n",
    "\n",
    "            # Add an aesthetic, dividing '/' after the formatted parameter \n",
    "            # measurement, unless it is the last parameter in the loop\n",
    "            table = table + '/ ' if j != len(params[-4:-1]) - 1 else table\n",
    "\n",
    "        # Add a final empty measurement to the group of central Lya peak measurements for consistency\n",
    "        table = table + '/ $-$ '\n",
    "\n",
    "        # Get the parameter distribution of the local continuum\n",
    "        param = params[-1]\n",
    "\n",
    "        # Rescale the parameter distribution if the spectrum is not one of \n",
    "        # the stacked spectra (because the stacked spectra are normalized)\n",
    "        param = param * 1e16 if slit_id not in ['NL','L'] else param\n",
    "\n",
    "        # Calculate the median of the measured parameter distribution\n",
    "        median = np.median(param)\n",
    "\n",
    "        # Calculate the lower and upper uncertainties of the parameter distribution\n",
    "        lower, upper = median - np.percentile(param, 16), np.percentile(param, 84) - median\n",
    "\n",
    "        # Round the lower and upper uncertainties to one significant figure and \n",
    "        # the median to the most significant digit between the two uncertainties\n",
    "        median, lower, upper = round_to_uncertainties(median, lower, upper)\n",
    "\n",
    "        # Add the formatted parameter measurement to the table row\n",
    "        table = table + f'& ${median}_{{-{lower}}}^{{+{upper}}}$ '\n",
    "\n",
    "        # Add a LaTeX row break character if the row is not the last, and a line break character to the string\n",
    "        table = table + '\\\\\\\\\\n' if i != len(slits) - 1 else table + '\\n'\n",
    "\n",
    "    # Add a footer to the table\n",
    "    table = table + '\\enddata\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\tablecomments{Best-fit Ly$\\\\alpha$ parameters of the magnification-uncorrected, rest-frame Ly$\\\\alpha$ profiles. From left to right: slit label, best-fit parameters for the blue peak, red peak, and central peak, presented as $\\\\alpha$/$\\mu$/$\\sigma$/$\\omega$, as described in \\S\\,\\\\ref{sssec:methods_lya_fwhm_vsep}. Apart from the stacked spectra, $\\\\alpha$ has units of $10^{-16}$ erg s$^{-1}$ cm$^{-2}$ {\\AA}$^{-1}$, and $\\mu$ and $\\sigma$ have units of km s$^{-1}$.}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\tablenotetext{a}{The fitted amplitudes of slit M0 are significantly different from the other observations because this observation was taken through cloud cover that prevented an accurate fluxing. See Table \\\\ref{tab:mage_log} for more information about the observing conditions.}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\end{deluxetable}'\n",
    "\n",
    "    # Save the table\n",
    "    f = open(f'{results}/tables/lya_best_fit_model_parameters_table.tex', 'w', encoding='utf-8')\n",
    "    f.write(table)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9887ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83b584ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b2b404ef7cfffcb2d9e58206576e0220bed399f08fa92bc6fd125b02b641f3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
