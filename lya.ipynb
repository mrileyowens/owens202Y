{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f1beb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Riley Owens (GitHub: mrileyowens)\n",
    "\n",
    "# This file measures values and errors of \n",
    "# various Lyα profile parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92eb6b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.special import erf\n",
    "from scipy.stats import kendalltau, spearmanr\n",
    "\n",
    "import sigfig\n",
    "from sigfig import round as sf_round\n",
    "#from to_precision import std_notation\n",
    "import decimal\n",
    "\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "\n",
    "from numba import njit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "bf4643e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "\n",
    "    def extract_data(file):\n",
    "\n",
    "        '''\n",
    "        Extract spectrum from the .txt file\n",
    "\n",
    "        Parameters:\n",
    "            file : str\n",
    "                Name of the file\n",
    "\n",
    "        Returns:\n",
    "            w : numpy.ndarray\n",
    "                Observed wavelength bins\n",
    "            f : numpy.ndarray\n",
    "                Observed flux densities\n",
    "            n : numpy.ndarray\n",
    "                Observed Gaussian standard deviation of observed flux densities\n",
    "        '''\n",
    "\n",
    "        # Retrieve the data columns\n",
    "        w, f, n = np.loadtxt(file, delimiter='\\t', comments=('#', 'w'), usecols=(0,1,2), unpack=True)\n",
    "    \n",
    "        # If the file is not the stacked leaker spectrum. This step is necessary because \n",
    "        if 'leaker' not in file:\n",
    "\n",
    "            # Remove bins of extreme outliers\n",
    "            w = w[f < 1e-20]\n",
    "            n = n[f < 1e-20]\n",
    "            f = f[f < 1e-20]\n",
    "\n",
    "            # Convert from units of erg/s/cm^2/Hz to erg/s/cm^2/Å\n",
    "            f = f * 2.998e18 / np.square(w)\n",
    "            n = n * 2.998e18 / np.square(w)\n",
    "\n",
    "        return w, f, n    \n",
    "\n",
    "    def compute_fwhm_and_peak(parameters, Rs):\n",
    "\n",
    "        '''\n",
    "        Returns:\n",
    "            fwhm : numpy.float64\n",
    "            loc : numpy.float64\n",
    "        '''\n",
    "        def lin_interp(x, y, i):\n",
    "            return x[i] + (x[i+1] - x[i]) * y[i] / (y[i+1] - y[i])\n",
    "\n",
    "        def skew_gaussian(x, amp, cen, width, skew):\n",
    "\n",
    "            fit = amp * np.exp(-((x - cen) / width)**2 / 2) * (1 + erf(skew * ((x - cen) / width) / np.sqrt(2)))\n",
    "\n",
    "            return fit\n",
    "\n",
    "        v = np.arange(-500,500,0.1)\n",
    "\n",
    "        fits = []\n",
    "\n",
    "        for i, fit_params in enumerate(parameters):\n",
    "\n",
    "            fit = skew_gaussian(v, *fit_params)\n",
    "\n",
    "            fits.append([*fit])\n",
    "\n",
    "        fits_max_indices = np.argmax(fits, axis=1)\n",
    "\n",
    "        locs = v[fits_max_indices]\n",
    "\n",
    "        fits_halved = fits - np.amax(fits, axis=1, keepdims=True) / 2\n",
    "\n",
    "        crossings = np.diff(np.sign(fits_halved), axis=1) != 0\n",
    "\n",
    "        indices = np.array(np.where(crossings))\n",
    "\n",
    "        indices = indices.T\n",
    "\n",
    "        fwhms = []\n",
    "\n",
    "        for i in range(len(parameters)):\n",
    "\n",
    "            fwhm = abs(lin_interp(v, fits_halved[i], indices[2 * i][1]) - lin_interp(v, fits_halved[i], indices[2 * i + 1][1]))\n",
    "\n",
    "            fwhms.append(fwhm)\n",
    "\n",
    "        #res = np.random.normal(R, R_error, len(parameters))\n",
    "\n",
    "        fwhms = np.sqrt((np.array(fwhms))**2 - (Rs)**2)\n",
    "\n",
    "        return fwhms, locs\n",
    "\n",
    "    def two_peaks(x, amp_r, cen_r, width_r, skew_r, amp_c, cen_c, width_c, cntm):\n",
    "\n",
    "        return amp_r * np.exp(-((x - cen_r) / width_r)**2 / 2) * (1 + erf(skew_r * ((x - cen_r) / width_r) / np.sqrt(2))) \\\n",
    "            + amp_c * np.exp(-((x - cen_c) / width_c)**2 / 2) \\\n",
    "            + cntm\n",
    "\n",
    "    def three_peaks(x, amp_b, cen_b, width_b, skew_b, amp_r, cen_r, width_r, skew_r, amp_c, cen_c, width_c, cntm):\n",
    "\n",
    "        return amp_b * np.exp(-((x - cen_b) / width_b)**2 / 2) * (1 + erf(skew_b * ((x - cen_b) / width_b) / np.sqrt(2)))  \\\n",
    "            + amp_r * np.exp(-((x - cen_r) / width_r)**2 / 2) * (1 + erf(skew_r * ((x - cen_r) / width_r) / np.sqrt(2))) \\\n",
    "            + amp_c * np.exp(-((x - cen_c) / width_c)**2 / 2) \\\n",
    "            + cntm\n",
    "\n",
    "    # Dictionary containing information about the MagE slit spectra, including, from left to right:\n",
    "    # the name of the file containing the data, redshift (taken from Mainali et al. (2022) (ApJ, 940, 160)),\n",
    "    # magnification of the slit, a tuple specifying a range of Lya peculiar velocities used to set \n",
    "    # initial parameter estimates of the central Lya peak, spectral resolution, and uncertainty in the spectral resolution\n",
    "    slits = {\n",
    "        'NL' : ['rest_sba-nonleaker-no_m3_MWdr.txt', 0, 1, [60,140], 5400, 200],\n",
    "        'L' : ['rest_sba-leaker-no_m0_MWdr.txt', 0, 1, [20,130], 5300, 200],\n",
    "        'M5' : ['psz-arcslit-m5-comb1_MWdr.txt', 2.37086, 51, [0,100], 5500, 400],\n",
    "        'M4' : ['psz-arcslit-m4-comb1_MWdr.txt', 2.37073, 14.6, [0,85], 5400, 300],\n",
    "        'M6' : ['psz-arcslit-m6-comb1_MWdr.txt', 2.37021, 147, [10,130], 5300, 300],\n",
    "        'M3' : ['psz-arcslit-m3-comb1_MWdr.txt', 2.37025, 36, [35,120], 5500, 400],\n",
    "        'M0' : ['planckarc_m0-comb1_MWdr.txt', 2.37014, 10, [10,130], 4700, 200],\n",
    "        'M2' : ['psz-arcslit-m2-comb1_MWdr.txt', 2.37017, 32, [45,125], 5300, 300],\n",
    "        'M7' : ['psz-arcslit-m7-comb1_MWdr.txt', 2.37044, 35, [15,100], 5200, 200],\n",
    "        'M8' : ['psz-arcslit-m8-comb1_MWdr.txt', 2.37024, 29, [25,125], 5200, 300],\n",
    "        'M9' : ['psz-arcslit-m9-comb1_MWdr.txt', 2.37030, 31, [15,125], 5500, 400]\n",
    "    }\n",
    "\n",
    "    # Establish directories\n",
    "    home = os.getcwd()\n",
    "    data = f'{home}/data'\n",
    "    results = f'{home}/results'\n",
    "\n",
    "    #f_esc = [2.3, -0.6, 3, 2.3, 17, 18, 12, 15, 14]\n",
    "    #ne_esc = [0.8, 0.2, 1, 0.8, 6, 7, 5, 6, 5]\n",
    "\n",
    "    #c_peak_range = np.array([[60,140],[20,130],[0,100],[0,85],[10,130],[35,120],[10,130],\n",
    "    #                         [45,125],[15,100],[25,125],[15,125]], dtype=np.float64)\n",
    "\n",
    "    #total_result = np.array([np.empty((9,1000))])\n",
    "\n",
    "    '''\n",
    "    R = np.array([\n",
    "        299792.458 / 5400,\n",
    "        299792.458 / 5300,\n",
    "        299792.458 / 5500,\n",
    "        299792.458 / 5400,\n",
    "        299792.458 / 5300,\n",
    "        299792.458 / 5500,\n",
    "        299792.458 / 4700,\n",
    "        299792.458 / 5300,\n",
    "        299792.458 / 5200,\n",
    "        299792.458 / 5200,\n",
    "        299792.458 / 5500\n",
    "    ])\n",
    "\n",
    "    R_error = np.array([\n",
    "        299792.458 / 5400**2 * 200,\n",
    "        299792.458 / 5300**2 * 200,\n",
    "        299792.458 / 5500**2 * 400,\n",
    "        299792.458 / 5400**2 * 300,\n",
    "        299792.458 / 5300**2 * 300,\n",
    "        299792.458 / 5500**2 * 400,\n",
    "        299792.458 / 4700**2 * 200,\n",
    "        299792.458 / 5300**2 * 300,\n",
    "        299792.458 / 5200**2 * 200,\n",
    "        299792.458 / 5200**2 * 300,\n",
    "        299792.458 / 5500**2 * 400\n",
    "    ])\n",
    "    '''\n",
    "\n",
    "    # Instantiate a cosmology with 30% matter-based energy density and an expansion rate of 70 km/s/Mpc\n",
    "    cosmology = FlatLambdaCDM(70,0.3)\n",
    "\n",
    "    # For each slit ID\n",
    "    for i, slit_id in enumerate(slits):\n",
    "\n",
    "        # Unpack the dictionary values for this slit\n",
    "        file_name, z, mag, c_peak_range, R, R_error = slits[slit_id]\n",
    "\n",
    "        # Convert the spectral resolution to units of km/s, which is \n",
    "        # necessary when correcting the FWHMs (which has units of km/s)\n",
    "        # for the instrumental resolution\n",
    "        R_error = 299792.458 / R**2 * R_error\n",
    "        R = 299792.458 / R\n",
    "\n",
    "        '''\n",
    "        mag = slits[slit_id][2]\n",
    "        c_peak_range = slits[slit_id][3]\n",
    "        R = slits[slit_id][4]\n",
    "        R_error = slits[slit_id][5]\n",
    "        '''\n",
    "\n",
    "        # Set a scaling factor which will be applied to the data. This is necesary\n",
    "        # because for the observed data (so not the stacked spectra), the fits are \n",
    "        # very poor (the fitting does not explore the parameter space) without rescaling \n",
    "        # them, perhaps related to float precision because their given flux density units, \n",
    "        # the flux density values are very small (~10^16 - 10^17 erg/s/cm^2/Å)\n",
    "        factor = 1e16 if 'M' in slit_id else 1\n",
    "\n",
    "        # Extract the data from the .txt file\n",
    "        w, f, n = extract_data(f'{data}/spectra/mage/{file_name}')\n",
    "\n",
    "        # Cut the data to just the relevant portion of the spectrum\n",
    "        f = f[(w >= 1195 * (1 + z)) & (w <= 1235 * (1 + z))]\n",
    "        n = n[(w >= 1195 * (1 + z)) & (w <= 1235 * (1 + z))]\n",
    "        w = w[(w >= 1195 * (1 + z)) & (w <= 1235 * (1 + z))]\n",
    "        \n",
    "        # Place the data in the rest frame, and apply the scaling factor\n",
    "        w = w / (1 + z)\n",
    "        f = f * (1 + z) * factor\n",
    "        n = n * (1 + z) * factor\n",
    "\n",
    "        # Calculate the Lya peculiar velocities\n",
    "        v = 299792.458 * (w / 1215.67 - 1)\n",
    "\n",
    "        # Instantiate a cosmology with 30% matter-based energy density and an expansion rate of 70 km/s/Mpc\n",
    "        #cosmology = FlatLambdaCDM(70,0.3)\n",
    "\n",
    "        # Determine the luminosity distance to the redshift in units of centimeters (to match the flux density unit)\n",
    "        l_dist = cosmology.luminosity_distance(z).value * 3.086e24\n",
    "\n",
    "        # Create a boolean mask designating the wavelength range used to compute the local continuum\n",
    "        cntm_mask = (w >= 1221) & (w <= 1225)\n",
    "\n",
    "        # Create a boolean mask designating the integration range when computing the equivalent width\n",
    "        #ew_mask = (w >= 1212) & (w <= 1221)\n",
    "\n",
    "        # Create a boolean mask designating the integration range when computing the Lya equivalent width and luminosity\n",
    "        lya_profile_mask = (w >= 1212) & (w <= 1221)\n",
    "        \n",
    "        # Create boolean masks for the integration ranges about the rest velocity of the Lya profile\n",
    "        # for computing the central escape fraction of the Lya profile\n",
    "        v100_mask = (v >= -100) & (v <= 100)\n",
    "        v1000_mask = (v >= -1000) & (v <= 1000)\n",
    "\n",
    "        # Mask for the observed-frame wavelength boundaries when computing the luminosity of the Lya profile\n",
    "        #l_mask = (w * (1 + z) >= 1212 * (1 + z)) & (w * (1 + z) <= 1221 * (1 + z))\n",
    "\n",
    "        # Set a string for the flux density unit of the data\n",
    "        flux_density_unit = 'erg/s/cm^2/Å' if 'M' in slit_id else 'normalized wavelength-space flux density'\n",
    "\n",
    "        # If there is no blue peak and the slit ID is not M5\n",
    "        if i in [0,3,4]:\n",
    "        \n",
    "            # Set the model function and the model parameter labels\n",
    "            model = two_peaks\n",
    "            model_parameter_labels = f'redshifted Lyα peak amplitude ({flux_density_unit}), centroid (km/s), width (km/s), and skew, \\n' \\\n",
    "                + f'central Lyα peak amplitude ({flux_density_unit}), centroid (km/s), and width (km/s), ' \\\n",
    "                + f'and local continuum ({flux_density_unit})'\n",
    "        \n",
    "            # Set the initial parameter estimates and bounds\n",
    "            p0 = (np.amax(f[(v >= 150) & (v <= 1000)]),200,20,1,np.amax(f[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]),80,40,np.median(f[cntm_mask]))\n",
    "            bounds = ([0,0,0,0,0,0,0,np.amin(f[cntm_mask])],[np.amax(f),1000,np.inf,np.inf,np.amax(f),200,85,np.amax(f[cntm_mask])])\n",
    "    \n",
    "        # If the slit ID is M5; this elif statement is necessary for improved initial parameter estimates of the central Lya peak in slit M5. The previous\n",
    "        # if statement was not satisfactory\n",
    "        elif i in [2]:\n",
    "\n",
    "            # Set the model function and the model parameter labels\n",
    "            model = two_peaks\n",
    "            model_parameter_labels = f'redshifted Lyα peak amplitude ({flux_density_unit}), centroid (km/s), width (km/s), and skew, \\n' \\\n",
    "                + f'central Lyα peak amplitude ({flux_density_unit}), centroid (km/s), and width (km/s), ' \\\n",
    "                + f'and local continuum ({flux_density_unit})'\n",
    "\n",
    "            # Set the initial parameter estimates and bounds\n",
    "            p0 = (np.amax(f[(v >= 150) & (v <= 1000)]),200,20,1,np.amax(f[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]) - np.median(f[cntm_mask]),50,20,np.median(f[cntm_mask]))\n",
    "            bounds = ([0,0,0,0,0,0,0,np.amin(f[cntm_mask])],[np.amax(f),500,np.inf,np.inf,np.amax(f[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]) - np.median(f[cntm_mask]),100,40,np.amax(f[cntm_mask])])\n",
    "\n",
    "        # Otherwise\n",
    "        else:\n",
    "\n",
    "            # Set the model function and the model parameter labels\n",
    "            model = three_peaks\n",
    "            model_parameter_labels = f'blueshifted Lyα peak amplitude ({flux_density_unit}), centroid (km/s), width (km/s), and skew, \\n' \\\n",
    "                + f'redshifted Lyα peak amplitude ({flux_density_unit}), centroid (km/s), width (km/s), and skew, \\n' \\\n",
    "                + f'central Lyα peak amplitude ({flux_density_unit}), centroid (km/s), and width (km/s), ' \\\n",
    "                + f'and local continuum ({flux_density_unit})'\n",
    "\n",
    "            # Set the initial parameter estimates and bounds\n",
    "            p0 = (np.amax(f[(v >= -1000) & (v <= 0)]),-150,20,-1,np.amax(f[(v >= 150) & (v <= 1000)]),200,20,1,np.amax(f[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]),80,40,np.median(f[cntm_mask]))\n",
    "            bounds = ([0,-1000,0,-np.inf,0,0,0,0,0,0,0,np.amin(f[cntm_mask])],[np.amax(f[(v >= -1000) & (v <= 0)]),0,np.inf,0,np.amax(f),1000,np.inf,np.inf,np.amax(f),200,85,np.amax(f[cntm_mask])])\n",
    "\n",
    "        p0_burn_in = []\n",
    "\n",
    "        for j in range(10):\n",
    "\n",
    "            # Draw a randomly sampled spectrum from the original observation, assuming \n",
    "            # that the observed flux densities and associated uncertainties correspond \n",
    "            # to the mean and standard deviation of Gaussian distributions, respectively\n",
    "            f_mc = np.random.normal(f, n)\n",
    "\n",
    "            # Directly fit the Lya profile with the assigned model function and initial parameter estimates and bounds\n",
    "            p, _ = curve_fit(model, v[v >= -500], f_mc[v >= -500], p0=p0, bounds=bounds, maxfev=1e6) \n",
    "\n",
    "            p0_burn_in.append([*p])\n",
    "\n",
    "        p0_burn_in = np.median(np.array(p0_burn_in, dtype=np.float64).T, axis=1)\n",
    "\n",
    "        # Establish empty lists to append the results of the Monte Carlo simulation to\n",
    "        mc_spectra = []\n",
    "        parameters = []\n",
    "        measurements = []\n",
    "\n",
    "        # For each iteration in the Monte Carlo simulation\n",
    "        for j in range(10):\n",
    "\n",
    "            # Draw a randomly sampled spectrum from the original observation, assuming \n",
    "            # that the observed flux densities and associated uncertainties correspond \n",
    "            # to the mean and standard deviation of Gaussian distributions, respectively\n",
    "            f_mc = np.random.normal(f, n)\n",
    "\n",
    "            # Compute the local continuum as the median flux density between 1221 - 1225 Å in the rest frame\n",
    "            c = np.median(f_mc[cntm_mask])\n",
    "\n",
    "            # Compute the rest-frame equivalent width of the Lya profile\n",
    "            ew = -1 * np.trapz(1 - (f_mc / c)[lya_profile_mask], w[lya_profile_mask])\n",
    "\n",
    "            # Compute the central escape fraction of the Lya profile\n",
    "            f_cen = np.trapz(f_mc[v100_mask], v[v100_mask]) / np.trapz(f_mc[v1000_mask], v[v1000_mask]) * 100\n",
    "\n",
    "            # Compute the luminosity of the Lya profile, placing the data back into the observed frame, removing the scaling factor, and \n",
    "            # correcting for the magnification\n",
    "            #l = 4 * np.pi * np.trapz((f_mc / (1 + z) / mag)[l_mask] - c / (1 + z) / mag, (w * (1 + z))[l_mask]) * l_dist**2\n",
    "            l = 4 * np.pi * np.trapz((f_mc[lya_profile_mask] - c) / (1 + z) / mag / factor, (w * (1 + z))[lya_profile_mask]) * l_dist**2\n",
    "\n",
    "            # Directly fit the Lya profile with the assigned model function and initial parameter estimates and bounds\n",
    "            p, _ = curve_fit(model, v[v >= -500], f_mc[v >= -500], p0=p0_burn_in, bounds=bounds, maxfev=1e6) \n",
    "\n",
    "            # Compute the ratio between the 'minimum' flux density between the redshifted and blueshifted Lya peaks \n",
    "            # (really taken as the amplitude of the central Lya peak; see paper for justifying details) and the local continuum\n",
    "            ratio = p[-4] / p[-1]\n",
    "\n",
    "            # Append the randomly sampled spectrum, best-fit model parameters, and Lya measurements to the aggregate lists\n",
    "            mc_spectra.append([*f_mc])\n",
    "            parameters.append([*p])\n",
    "            measurements.append([ratio, ew, f_cen, l])\n",
    "\n",
    "        # Convert all of the result lists into arrays for easier handling\n",
    "        p0 = np.array(p0, dtype=np.float64)\n",
    "        mc_spectra = np.array(mc_spectra, dtype=np.float64)\n",
    "        parameters = np.array(parameters, dtype=np.float64)\n",
    "        measurements = np.array(measurements, dtype=np.float64)\n",
    "\n",
    "        # Unscale the affected best-fit model parameters by the artifical scaling factor\n",
    "        parameters[:,-1], parameters[:,-4], parameters[:,-8] = parameters[:,-1] / factor, parameters[:,-4] / factor, parameters[:,-8] / factor\n",
    "\n",
    "        # Also unscale the amplitude of the blueshifted Lya peak if the model function had 3 peaks\n",
    "        parameters[:,0] = parameters[:,0] / factor if model==three_peaks else parameters[:,0]\n",
    "\n",
    "        # Unscale the affected intial model parameter estimates by the artificial scale factor\n",
    "        p0[-1], p0[-4], p0[-8] = p0[-1] / factor, p0[-4] / factor, p0[-8] / factor\n",
    "\n",
    "        # Also unscale the initial estimate of the amplitude of the blueshifted Lya peak if the model function had 3 peaks\n",
    "        p0[0] = p0[0] / factor if model==three_peaks else p0[0]\n",
    "\n",
    "        measurements[:,-1] = np.nan if 'L' in slit_id else measurements[:,-1]\n",
    "\n",
    "        # Randomly sample spectral resolutions for the spectrum from a Gaussian distribution, \n",
    "        # matching the number of iterations of the Monte Carlo simulation, and assuming that \n",
    "        # the reported measurement and uncertainty correspond to the mean and standard deviation \n",
    "        # of the Gaussian distribution. This is necessary to fold in the uncertainty in the \n",
    "        # spectral resolution into the measured FWHMs of the Lya peaks, since they are corrected\n",
    "        # for instrumental resolution\n",
    "        Rs = np.random.normal(R, R_error, len(parameters))\n",
    "\n",
    "        # Calculate the FWHMs of the central Lya peak, correcting for the instrumental resolution\n",
    "        fwhms_c = 2 * np.sqrt(2 * np.log(2)) * parameters.T[-2]\n",
    "        fwhms_c = np.sqrt((np.array(fwhms_c))**2 - (Rs)**2)\n",
    "\n",
    "        # Calculate the FWHMs and locations of the maxima of the redshifted Lya peak\n",
    "        fwhms_r, locs_r = compute_fwhm_and_peak(parameters[:, -8:-4], Rs)\n",
    "\n",
    "        # Insert the central and redshifted Lya peak FWHMs into the Lya measurements array\n",
    "        measurements = np.insert(measurements, 0, fwhms_r, axis=1)\n",
    "        measurements = np.insert(measurements, 0, fwhms_c, axis=1)\n",
    "\n",
    "        # Instantiate the Lya peak separations and blueshifted Lya peak FWHMs as NaNs. This is\n",
    "        # done to keep the output Lya measurements the same shape for all the spectra, even if\n",
    "        # they do not have a blueshifted Lya peak necessary to measure the Lya peak separation\n",
    "        # and blueshifted Lya peak FWHM. If the spectrum does have a third peak, these NaNs\n",
    "        # will be replaced by actual measurements in the following if statement\n",
    "        v_sep, fwhms_b = np.empty(len(mc_spectra)), np.empty(len(mc_spectra))\n",
    "        v_sep[:], fwhms_b[:] = np.nan, np.nan\n",
    "\n",
    "        # If the spectrum has three Lya peaks\n",
    "        if slit_id in ['L', 'M0', 'M2', 'M3', 'M7', 'M8', 'M9']:\n",
    "            \n",
    "            # Calculate the FWHMs and locations of the maxima of the blueshifted Lya peak\n",
    "            fwhms_b, locs_b = compute_fwhm_and_peak(parameters[:, 0:4], Rs)\n",
    "\n",
    "            # Calculate the Lya peak separations\n",
    "            v_sep = np.abs(locs_r - locs_b)\n",
    "\n",
    "        # Insert the Lya peak separations and blueshifted Lya peak FWHMs into the Lya measurements array\n",
    "        measurements = np.insert(measurements, 0, [v_sep, fwhms_b], axis=1)\n",
    "\n",
    "        # If the results folder for this slit ID doesn't exist yet, make it\n",
    "        if not os.path.isdir(f'{results}/lya_fits/{slit_id}'):\n",
    "            os.makedirs(f'{results}/lya_fits/{slit_id}')\n",
    "\n",
    "        # Set the header for the output file containing the randomly sampled Lya spectra\n",
    "        header = f'Randomly sampled Lyα spectra of the Monte Carlo simulation of {f\"slit {slit_id}\" if \"M\" in slit_id else slit_id}\\n' \\\n",
    "            + '\\n' \\\n",
    "            + f'Columns, from left to right: peculiar velocity relative to Lyα (km/s), flux density uncertainty ({flux_density_unit}), and randomly sampled flux densities of each iteration of the Monte Carlo simulation ({flux_density_unit})\\n'\n",
    "\n",
    "        # Save the output file containing the randomly sampled Lya spectra\n",
    "        np.savetxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_spectra.txt', np.array([v, n, *mc_spectra]).T, header=header, delimiter=' ', encoding='utf-8')    \n",
    "\n",
    "        # Set the header for the output file containing the best-fit model parameters\n",
    "        header = f'Best-fit parameters of the Lyα fits of {f\"slit {slit_id}\" if \"M\" in slit_id else slit_id}\\n' \\\n",
    "            + '\\n' \\\n",
    "            + f'Initial parameters: {p0_burn_in}\\n' \\\n",
    "            + '\\n' \\\n",
    "            + f'Columns, from left to right: {model_parameter_labels}\\n'\n",
    "\n",
    "        # Save the output file containing the best-fit model parameters\n",
    "        np.savetxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_best_fit_model_parameters.txt', parameters, header=header, delimiter=' ', encoding='utf-8')\n",
    "\n",
    "        # Set the header for the output file containing the Lya measurements\n",
    "        header = f'Measurements of the Lyα profiles from the best-fit curves of the Monte Carlo simulation of {f\"slit {slit_id}\" if \"M\" in slit_id else slit_id}\\n' \\\n",
    "            + '\\n' \\\n",
    "            + 'Columns, from left to right: Lyα peak separation (km/s), blueshifted Lyα peak FWHM (km/s), central Lyα peak FWHM (km/s), redshifted Lyα peak FWHM (km/s),\\n' \\\n",
    "            + 'ratio between the \\'minimum\\' flux density between the redshifted and blueshifted Lyα peaks and the continuum flux density (consult the paper for more details),\\n' \\\n",
    "            + 'rest-frame Lyα equivalent width (Å), central fraction of Lyα flux (%), and Lyα luminosity (erg/s)\\n'\n",
    "\n",
    "        # Save the output file containing the Lya measurements\n",
    "        np.savetxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_measurements.txt', measurements, header=header, delimiter=' ', encoding='utf-8')\n",
    "\n",
    "def correlate():\n",
    "\n",
    "    slits = {\n",
    "        'NL' : ['rest_sba-nonleaker-no_m3_MWdr.txt', 0, 1, [60,140]],\n",
    "        'L' : ['rest_sba-leaker-no_m0_MWdr.txt', 0, 1, [20,130]],\n",
    "        'M5' : ['psz-arcslit-m5-comb1_MWdr.txt', 2.37086, 51, [0,100]],\n",
    "        'M4' : ['psz-arcslit-m4-comb1_MWdr.txt', 2.37073, 14.6, [0,85]],\n",
    "        'M6' : ['psz-arcslit-m6-comb1_MWdr.txt', 2.37021, 147, [10,130]],\n",
    "        'M3' : ['psz-arcslit-m3-comb1_MWdr.txt', 2.37025, 36, [35,120]],\n",
    "        'M0' : ['planckarc_m0-comb1_MWdr.txt', 2.37014, 10, [10,130]],\n",
    "        'M2' : ['psz-arcslit-m2-comb1_MWdr.txt', 2.37017, 32, [45,125]],\n",
    "        'M7' : ['psz-arcslit-m7-comb1_MWdr.txt', 2.37044, 35, [15,100]],\n",
    "        'M8' : ['psz-arcslit-m8-comb1_MWdr.txt', 2.37024, 29, [25,125]],\n",
    "        'M9' : ['psz-arcslit-m9-comb1_MWdr.txt', 2.37030, 31, [15,125]]\n",
    "    }\n",
    "\n",
    "    # Establish directories\n",
    "    home = os.getcwd()\n",
    "    results = f'{home}/results'\n",
    "\n",
    "    # The LyC escape fractions and associated uncertainties of each spectrum\n",
    "    f_esc = [np.nan, np.nan, 2.3, -0.6, 3, 2.3, 17, 18, 12, 15, 14]\n",
    "    ne_esc = [np.nan, np.nan, 0.8, 0.2, 1, 0.8, 6, 7, 5, 6, 5]\n",
    "\n",
    "    # Make an empty list that will contain the statistical correlation results\n",
    "    corr_coefs = []\n",
    "\n",
    "    # For each Lya parameter\n",
    "    for i in range(8):\n",
    "\n",
    "        # For each unique combination with another Lya parameter not covered by the loop yet\n",
    "        for j in range(8 - (i + 1)):\n",
    "\n",
    "            # Make an empty list that will store the aggregate measurements of the parameter pair from all the spectra\n",
    "            measurements = []\n",
    "\n",
    "            # For each slit ID\n",
    "            for k, slit_id in enumerate(slits):\n",
    "\n",
    "                # Append the measurements of the parameter pair to the measurement list\n",
    "                measurements.append(np.loadtxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_measurements.txt', usecols=(i, i + j + 1)))\n",
    "            \n",
    "            # Make the measurements list into an array for convenience and easier handling\n",
    "            measurements = np.array(measurements)\n",
    "\n",
    "            # Make an empty list that will store the aggregate statistical correlations between the measurement pairs\n",
    "            measurement_pair_corr_coefs = []\n",
    "            \n",
    "            #for k, _ in enumerate(measurements):\n",
    "            # For each iteration in the Monte Carlo simulation\n",
    "            for k in range(1000):\n",
    "\n",
    "                # Get all of the measurements of the given measurement pair for that iteration\n",
    "                pairs = measurements[:,k,:]\n",
    "\n",
    "                # Drop any pairs that contain NaNs (i.e., where that measurement wasn't applicable)\n",
    "                pairs = pairs[~np.any(np.isnan(pairs), axis=1)]\n",
    "\n",
    "                # Take the transpose of the pairs to separate the measurements into an 'x' and 'y' column\n",
    "                pairs = pairs.T\n",
    "\n",
    "                # Compute the Pearson and Kendall correlation coefficients\n",
    "                pearson_corr_coef = np.corrcoef(pairs[0], pairs[1])[0,1]\n",
    "                kendall_corr_coef = kendalltau(pairs[0], pairs[1], variant='b').statistic\n",
    "\n",
    "                # Append the correlation coefficients to the aggregate list\n",
    "                measurement_pair_corr_coefs.append([pearson_corr_coef, kendall_corr_coef])\n",
    "\n",
    "            # Append all the statistical correlation measurements for the parameter pair to the aggregate list\n",
    "            corr_coefs.append(measurement_pair_corr_coefs)        \n",
    "\n",
    "        # Make an empty list that will store the randomly sampled measurements of the LyC escape fractions\n",
    "        f_esc_lyc = []\n",
    "\n",
    "        # For each spectrum\n",
    "        for j, _ in enumerate(slits):\n",
    "\n",
    "            # Append the randomly sampled measurements (following a Gaussian distribution\n",
    "            # and assuming that the measured value and associated uncertainty correspond \n",
    "            # to the mean and standard deviation of the distribution) of the LyC escape\n",
    "            # fraction of the slit to the aggregate list\n",
    "            f_esc_lyc.append(np.random.normal(f_esc[j], ne_esc[j], 1000))\n",
    "\n",
    "        # Convert the list to an array and transpose it\n",
    "        f_esc_lyc = np.array(f_esc_lyc).T\n",
    "\n",
    "        # Make an empty list that will store the aggregate statistical correlations between the measurement pairs\n",
    "        measurement_pair_corr_coefs = []\n",
    "\n",
    "        # For each iteration of the Monte Carlo simulation \n",
    "        for j in range(1000):\n",
    "\n",
    "            # Make an array of the measurements organized into pairs by source spectrum\n",
    "            pairs = np.array([measurements[:,k,0], f_esc_lyc[j]]).T\n",
    "\n",
    "            # Remove any pairs that contain any NaNs (i.e., where that measurement wasn't applicable)\n",
    "            pairs = pairs[~np.any(np.isnan(pairs), axis=1)].T\n",
    "\n",
    "            # Calculate the Pearson and Kendall correlation coefficients\n",
    "            pearson_corr_coef = np.corrcoef(pairs[0], pairs[1])[0,1]\n",
    "            kendall_corr_coef = kendalltau(pairs[0], pairs[1], variant='b').statistic\n",
    "\n",
    "            # Append the correlation coefficients to the aggregate list\n",
    "            measurement_pair_corr_coefs.append([pearson_corr_coef, kendall_corr_coef])\n",
    " \n",
    "        # Append all the statistical correlation measurements for the parameter pair to the aggregate list\n",
    "        corr_coefs.append(measurement_pair_corr_coefs)\n",
    "\n",
    "    # Conver the list of correlation coefficient measurements to an array for easier handling\n",
    "    corr_coefs = np.array(corr_coefs)\n",
    "\n",
    "    # Flatten the array so that each set of measurements of a type of correlation coefficient\n",
    "    # for a parameter pair becomes its own column\n",
    "    corr_coefs = corr_coefs.swapaxes(1, 2)\n",
    "    corr_coefs = corr_coefs.reshape(-1, corr_coefs.shape[-1])\n",
    "\n",
    "    # Set the header of the output file of the statistical correlations between the measurement pairs\n",
    "    header = ''\n",
    "\n",
    "    # Save the output file containing the statistical correlations between the measurement pairs\n",
    "    np.savetxt(f'{results}/lya_fits/mc_sim_lya_measurements_statistical_correlations.txt', corr_coefs, header=header, delimiter='\\t', encoding='utf-8')\n",
    "\n",
    "    '''\n",
    "    fwhm_c_results = fwhm_c_results[~np.isnan(fwhm_c_results)]\n",
    "\n",
    "    # If the spectrum is one of the stacked spectra\n",
    "    if i < 2:\n",
    "\n",
    "        results = [vsep_results, fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results]\n",
    "        r = np.array([None,None,None,None,1,1,1])\n",
    "        tau = np.array([None,None,None,None,1,1,1])\n",
    "        rho = np.array([None,None,None,None,1,1,1])\n",
    "\n",
    "    # If the spectrum is not one of the stacked spectra\n",
    "    elif i > 1:\n",
    "\n",
    "        results = [vsep_results, fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results, l_results * 1e-42]\n",
    "        r = np.array([None,None,None,None,1,1,1,1])\n",
    "        tau = np.array([None,None,None,None,1,1,1,1])\n",
    "        rho = np.array([None,None,None,None,1,1,1,1])\n",
    "\n",
    "    # If the spectrum is not a stacked one\n",
    "    if i > 1:\n",
    "        # Create pseudo-measurements of the LyC escape fraction assuming the calculated value and standard deviation\n",
    "        # correspond to a Gaussian mean and standard deviation\n",
    "        fesc_results = np.random.normal(f_esc[i-2], ne_esc[i-2], 1000)\n",
    "\n",
    "    else:\n",
    "        fesc_results = np.empty(1000)\n",
    "        fesc_results[:] = np.nan\n",
    "\n",
    "    slit_result = np.array([np.empty(1000)])\n",
    "\n",
    "    for j, result in enumerate([vsep_results, fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results, l_results * 1e-42, fesc_results]):\n",
    "            \n",
    "        # Try to append any missing values as NaNs\n",
    "        try:\n",
    "            if len(result) < 1000:\n",
    "                a = np.empty(1000 - len(result))\n",
    "                a[:] = np.nan\n",
    "\n",
    "                result = np.append(result, a)\n",
    "\n",
    "        # Unless the array has not been instantiated\n",
    "        except TypeError:\n",
    "            result = np.empty(1000)\n",
    "            result[:] = np.nan\n",
    "            \n",
    "        if (i == 6) and (j == 7):\n",
    "            result = np.empty(1000)\n",
    "            result[:] = np.nan\n",
    "\n",
    "        result = np.where(result != 0.0, result, np.nan)\n",
    "\n",
    "        slit_result = np.append(slit_result, np.array([result]), axis=0)\n",
    "\n",
    "    # Drop the first row since it is empty\n",
    "    slit_result = slit_result[1:]\n",
    "\n",
    "    total_result = np.append(total_result, np.array([slit_result]), axis=0)\n",
    "\n",
    "    total_result = total_result[1:]\n",
    "\n",
    "    r_locs = [[3,0,0,0,0,0,0,0],\n",
    "        [2,4,0,0,0,0,0,0],\n",
    "        [1,1,2,0,0,0,0,0],\n",
    "        [3,3,2,7,0,0,0,0],\n",
    "        [3,3,2,7,2,0,0,0],\n",
    "        [3,3,2,7,2,2,1,1],\n",
    "        [3,3,2,1,2,2,2,1],\n",
    "        [2,'center left',2,4,2,2,2,2]]\n",
    "\n",
    "    # For each row in the corner plot\n",
    "    for i, row in enumerate(ax_c):\n",
    "\n",
    "        # For each column in the corner plot\n",
    "        for j, subplot in enumerate(row):\n",
    "\n",
    "            r_results = np.array([], dtype=np.float64)\n",
    "            tau_results = np.array([], dtype=np.float64)\n",
    "            rho_results = np.array([], dtype=np.float64)\n",
    "\n",
    "            # If the row / column pair is above the main diagonal\n",
    "            if j > i:\n",
    "                pass\n",
    "            else:\n",
    "\n",
    "                x_data = np.empty(1000)\n",
    "                x_data[:] = np.nan\n",
    "                x_data = np.array([x_data])\n",
    "                y_data = np.empty(1000)\n",
    "                y_data[:] = np.nan\n",
    "                y_data = np.array([y_data])\n",
    "\n",
    "                # For each spectrum\n",
    "                for k, slit in enumerate(total_result):\n",
    "                    \n",
    "                    if all(np.isnan(slit[j])) or all(np.isnan(slit[i + 1])):\n",
    "                        pass\n",
    "                    else:\n",
    "\n",
    "                        x_data = np.append(x_data, np.array([slit[j]]), axis=0)\n",
    "                        y_data = np.append(y_data, np.array([slit[i + 1]]), axis=0)\n",
    "\n",
    "                x_data = x_data[1:]\n",
    "                y_data = y_data[1:]\n",
    "\n",
    "\n",
    "                if (i==6) and (j==6):\n",
    "                    pass\n",
    "\n",
    "                for k, sample in enumerate(x_data[0]):\n",
    "\n",
    "                    x = x_data[:,k]\n",
    "                    y = y_data[:,k]\n",
    "\n",
    "                    if any(np.isnan(x)) or any(np.isnan(y)):\n",
    "                        pass\n",
    "                    else:\n",
    "                        x = x[x != 0]\n",
    "                        y = y[y != 0]\n",
    "\n",
    "                        r = np.corrcoef(x, y)[0,1]\n",
    "                        tau = kendalltau(x, y).statistic\n",
    "                        rho = spearmanr(x, y).statistic\n",
    "\n",
    "                        r_results = np.append(r_results, r)\n",
    "                        tau_results = np.append(tau_results, tau)\n",
    "                        rho_results = np.append(rho_results, rho)\n",
    "    '''\n",
    "\n",
    "def plot():\n",
    "\n",
    "    def extract_data(file):\n",
    "\n",
    "        '''\n",
    "        Extract spectrum from the .txt file\n",
    "\n",
    "        Parameters:\n",
    "            file : str\n",
    "                Name of the file\n",
    "\n",
    "        Returns:\n",
    "            w : numpy.ndarray\n",
    "                Observed wavelength bins\n",
    "            f : numpy.ndarray\n",
    "                Observed flux densities\n",
    "            n : numpy.ndarray\n",
    "                Observed Gaussian standard deviation of observed flux densities\n",
    "        '''\n",
    "\n",
    "        # Retrieve the data columns\n",
    "        w, f, n = np.loadtxt(file, delimiter='\\t', comments=('#', 'w'), usecols=(0,1,2), unpack=True)\n",
    "    \n",
    "        # If the file is not the stacked leaker spectrum. This step is necessary because \n",
    "        if 'leaker' not in file:\n",
    "\n",
    "            # Remove bins of extreme outliers\n",
    "            w = w[f < 1e-20]\n",
    "            n = n[f < 1e-20]\n",
    "            f = f[f < 1e-20]\n",
    "\n",
    "            # Convert from units of erg/s/cm^2/Hz to erg/s/cm^2/Å\n",
    "            f = f * 2.998e18 / np.square(w)\n",
    "            n = n * 2.998e18 / np.square(w)\n",
    "\n",
    "        return w, f, n\n",
    "\n",
    "    def two_peaks(x, amp_r, cen_r, width_r, skew_r, amp_c, cen_c, width_c, cntm):\n",
    "\n",
    "        return amp_r * np.exp(-((x - cen_r) / width_r)**2 / 2) * (1 + erf(skew_r * ((x - cen_r) / width_r) / np.sqrt(2))), \\\n",
    "            amp_c * np.exp(-((x - cen_c) / width_c)**2 / 2), \\\n",
    "            cntm\n",
    "\n",
    "    def three_peaks(x, amp_b, cen_b, width_b, skew_b, amp_r, cen_r, width_r, skew_r, amp_c, cen_c, width_c, cntm):\n",
    "\n",
    "        return amp_b * np.exp(-((x - cen_b) / width_b)**2 / 2) * (1 + erf(skew_b * ((x - cen_b) / width_b) / np.sqrt(2))),  \\\n",
    "            amp_r * np.exp(-((x - cen_r) / width_r)**2 / 2) * (1 + erf(skew_r * ((x - cen_r) / width_r) / np.sqrt(2))), \\\n",
    "            amp_c * np.exp(-((x - cen_c) / width_c)**2 / 2), \\\n",
    "            cntm\n",
    "\n",
    "    slits = {\n",
    "        'NL' : ['rest_sba-nonleaker-no_m3_MWdr.txt', 0, 1, [60,140], '#DC3220', two_peaks],\n",
    "        'L' : ['rest_sba-leaker-no_m0_MWdr.txt', 0, 1, [20,130], '#005AB5', three_peaks],\n",
    "        'M5' : ['psz-arcslit-m5-comb1_MWdr.txt', 2.37086, 51, [0,100], '#DC3220', two_peaks],\n",
    "        'M4' : ['psz-arcslit-m4-comb1_MWdr.txt', 2.37073, 14.6, [0,85], '#DC3220', two_peaks],\n",
    "        'M6' : ['psz-arcslit-m6-comb1_MWdr.txt', 2.37021, 147, [10,130], '#DC3220', two_peaks],\n",
    "        'M3' : ['psz-arcslit-m3-comb1_MWdr.txt', 2.37025, 36, [35,120], '#D35FB7', three_peaks],\n",
    "        'M0' : ['planckarc_m0-comb1_MWdr.txt', 2.37014, 10, [10,130], '#005AB5', three_peaks],\n",
    "        'M2' : ['psz-arcslit-m2-comb1_MWdr.txt', 2.37017, 32, [45,125], '#005AB5', three_peaks],\n",
    "        'M7' : ['psz-arcslit-m7-comb1_MWdr.txt', 2.37044, 35, [15,100], '#005AB5', three_peaks],\n",
    "        'M8' : ['psz-arcslit-m8-comb1_MWdr.txt', 2.37024, 29, [25,125], '#005AB5', three_peaks],\n",
    "        'M9' : ['psz-arcslit-m9-comb1_MWdr.txt', 2.37030, 31, [15,125], '#005AB5', three_peaks]\n",
    "    }\n",
    "\n",
    "    # Establish common directories\n",
    "    home = os.getcwd()\n",
    "    data = f'{home}/data'\n",
    "    figs = f'{home}/figs'\n",
    "    results = f'{home}/results'\n",
    "\n",
    "    # Create the figure showing the Lya profiles and best-fit curves\n",
    "    fig_lya, ax_lya = plt.subplots(3,3, figsize=(12,12), sharex=True, constrained_layout=True)\n",
    "    \n",
    "    # Flatten the axes array for easier iteration\n",
    "    ax_lya_array = np.array(ax_lya).reshape(-1)\n",
    "\n",
    "    # For each slit ID apart from the first two (the stacked spectra)\n",
    "    for i, slit_id in enumerate(list(slits.keys())[2:]):\n",
    "\n",
    "        # Get the redshift, color to plot the Lya profile curve, and model function \n",
    "        # of the spectrum from the slit dictionary\n",
    "        z, c, model = slits[slit_id][1], slits[slit_id][4], slits[slit_id][5]\n",
    "\n",
    "        # Get the data\n",
    "        w, f, n = extract_data(f'{data}/spectra/mage/{slits[slit_id][0]}')\n",
    "\n",
    "        # Convert the data to the rest frame\n",
    "        w, f, n = w / (1 + z), f * (1 + z), n * (1 + z)\n",
    "\n",
    "        # Create the Lya peculiar velocities\n",
    "        v = 299792.458 * (w / 1215.67 - 1)\n",
    "\n",
    "        # Create a mask to apply to the data, masking between +/- 1100 km/s\n",
    "        mask = (v >= -1100) & (v <= 1100)\n",
    "\n",
    "        # Apply the mask to the data\n",
    "        v, f, n = v[mask], f[mask], n[mask]\n",
    "\n",
    "        # Get the median best-fit parameters of the model function\n",
    "        lya_best_fit_model_params = np.median(np.loadtxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_best_fit_model_parameters.txt', \n",
    "            delimiter=' '), axis=0)\n",
    "\n",
    "        # Add shading to indicate the 2σ uncertainties of the data\n",
    "        ax_lya_array[i].fill_between(v, (f - 2 * n) * 1e16, (f + 2 * n) * 1e16, step='mid', facecolor=c, alpha=0.3)\n",
    "\n",
    "        # Plot the data\n",
    "        ax_lya_array[i].plot(v, f * 1e16, c=c, ds='steps-mid')\n",
    "\n",
    "        # Get the best-fit curves of the individual peaks and the continuum level\n",
    "        best_fit_model_comps = model(v, *lya_best_fit_model_params)\n",
    "\n",
    "        # Set a variable for the best-fit continuum level from the best-fit curves\n",
    "        cntm = best_fit_model_comps[-1]\n",
    "\n",
    "        # For each best-fit curves of the individual peaks\n",
    "        for j, curve in enumerate(best_fit_model_comps[:-1]):\n",
    "\n",
    "            # Plot tbest-fit curve to the individual peak\n",
    "            ax_lya_array[i].plot(v, (curve + cntm) * 1e16, c='black', ls='dotted')\n",
    "\n",
    "        # Plot the total best-fit curve to the profile\n",
    "        ax_lya_array[i].plot(v, (np.sum(best_fit_model_comps[:-1], axis=0) + cntm) * 1e16, c='black', ls='dashed')\n",
    "\n",
    "        # Add a label to the corner of the plot indicating the slit ID\n",
    "        at = AnchoredText(slit_id, frameon=False, loc='upper right', prop=dict(fontsize='x-large'))       \n",
    "        ax_lya_array[i].add_artist(at)\n",
    "\n",
    "        # Set the x- and y-axis limits of the plot\n",
    "        ax_lya_array[i].set_xlim(-1000,1000)\n",
    "        ax_lya_array[i].set_ylim(bottom=0)\n",
    "\n",
    "        # Set the locations and labels of the x-axis ticks\n",
    "        ax_lya_array[i].set_xticks([-1000,-500,0,500,1000], \n",
    "            ['$-$1000','$-$500','0','$+$500','$+$1000'])\n",
    "\n",
    "        # Add ticks to each side of the plot, and point them inward\n",
    "        ax_lya_array[i].tick_params(left=True, bottom=True, right=True, top=True, direction='in')\n",
    "\n",
    "        # Set the plot to be square\n",
    "        ax_lya_array[i].set_aspect(1 / ax_lya_array[i].get_data_ratio(), adjustable='box')\n",
    "\n",
    "    # Add x- and y-axis labels to the figure\n",
    "    fig_lya.supylabel('Flux density ($10^{-16}$ erg s$^{-1}$ cm$^{-2}$ Å$^{-1}$)', fontsize='large')\n",
    "    ax_lya[2,1].set_xlabel('Velocity (km s$^{-1}$)', fontsize='large')\n",
    "\n",
    "    # Save the figure\n",
    "    fig_lya.savefig(f'{figs}/lya_fits.pdf', bbox_inches='tight')\n",
    "\n",
    "def tabulate():\n",
    "\n",
    "    slits = {\n",
    "        'NL' : [],\n",
    "        'L' : [],\n",
    "        'M5' : [],\n",
    "        'M4' : [],\n",
    "        'M6' : [],\n",
    "        'M3' : [],\n",
    "        'M0' : [],\n",
    "        'M2' : [],\n",
    "        'M7' : [],\n",
    "        'M8' : [],\n",
    "        'M9' : []\n",
    "    }\n",
    "\n",
    "    home = os.getcwd()\n",
    "    results = f'{home}/results'\n",
    "\n",
    "    table = '\\\\begin{deluxetable*}{cllllllll}[ht!]\\n\\n' \\\n",
    "        + '\\\\tablecaption{Ly$\\\\alpha$ measurements \\label{tab:lya_params}}\\n\\n' \\\n",
    "        + '\\\\tablehead{\\n' \\\n",
    "        + '\\t\\colhead{Slit} & \\colhead{$v_{\\\\rm{sep}}$} & \\colhead{FWHM (blue)} & \\colhead{FWHM (center)} & \\colhead{FWHM (red)} & \\colhead{$f_{\\\\rm{min}}/f_{\\\\rm{cont}}$} & \\colhead{EW} & \\colhead{$f_{\\\\rm{cen}}$} & \\colhead{Luminosity}\\n' \\\n",
    "        + '\\t\\\\\\\\\\n' \\\n",
    "        + '\\t\\colhead{} &\\n' \\\n",
    "        + '\\t\\colhead{[km s$^{-1}$]} &\\n' \\\n",
    "        + '\\t\\colhead{[km s$^{-1}$]} &\\n' \\\n",
    "        + '\\t\\colhead{[km s$^{-1}$]} &\\n' \\\n",
    "        + '\\t\\colhead{[km s$^{-1}$]} &\\n' \\\n",
    "        + '\\t\\colhead{} &\\n' \\\n",
    "        + '\\t\\colhead{[\\AA]} &\\n' \\\n",
    "        + '\\t\\colhead{[\\%]} &\\n' \\\n",
    "        + '\\t\\colhead{[$10^{41}$ erg s$^{-1}$]}\\n' \\\n",
    "        + '}\\n\\n' \\\n",
    "        + '\\startdata\\n\\n'\n",
    "\n",
    "    for i, slit_id in enumerate(slits):\n",
    "\n",
    "        slit_lya_measurements = np.loadtxt(f'{results}/lya_fits/{slit_id}/{slit_id}_mc_sim_lya_measurements.txt', delimiter=' ', comments='#').T\n",
    "\n",
    "        table = table + f'{slit_id} '        \n",
    "\n",
    "        for j, measurements in enumerate(slit_lya_measurements):\n",
    "\n",
    "            median = np.percentile(measurements, 50)\n",
    "            lower = median - np.percentile(measurements, 16)\n",
    "            upper = np.percentile(measurements, 84) - median\n",
    "\n",
    "            median, lower, upper = (median / 1e41, lower / 1e41, upper / 1e41) if j+1 == len(slit_lya_measurements) else (median, lower, upper)\n",
    "\n",
    "            #lower = sigfig.round(lower, sigfigs=1, type=str)\n",
    "            #upper = sigfig.round(upper, sigfigs=1, type=str)\n",
    "\n",
    "            if not np.isnan(median):\n",
    "\n",
    "                lower = sigfig.round(lower, sigfigs=1, type=str)\n",
    "                upper = sigfig.round(upper, sigfigs=1, type=str)\n",
    "\n",
    "                # Get whichever bound is smaller, in order to round the median to\n",
    "                # the digit of that bound's significant figure\n",
    "                ref = min(np.array([lower, upper], dtype=str), key=float)\n",
    "\n",
    "                # If the smaller bound is less than 1\n",
    "                if '.' in ref:\n",
    "\n",
    "                    # Round the median to the same digit as the smaller bound's only significant figure\n",
    "                    median = sigfig.round(median, decimals=len(ref.split('.')[1]), type=str)\n",
    "\n",
    "                # If the smaller bound is greater than 1\n",
    "                elif '.' not in ref:\n",
    "\n",
    "                    # Round the median to the same digit as the smaller bound's only significant figure\n",
    "                    median = sigfig.round(median, len(str(median).split('.')[0]) - len(ref) + 1, type=str)\n",
    "\n",
    "                # If the median or either bound is less than 0.0001 (this is Python's default limit to begin \n",
    "                # printing numbers in scientific notation)\n",
    "                if any('0.0000' in i for i in np.array([median, lower, upper], dtype=str)):\n",
    "\n",
    "                    # Get the smallest quantity of the median and bounds\n",
    "                    ref = min(np.array([median, lower, upper], dtype=str), key=float)\n",
    "\n",
    "                    # Determine the necesessary scientific notation exponent so that the first significant \n",
    "                    # figure of the smallest quantity is the first digit to the left of the decimal place\n",
    "                    exp = -len(ref.split('.')[1])\n",
    "\n",
    "                    # Write the scientific notation factor as a string\n",
    "                    factor = f'\\\\times 10^{{{exp}}}'\n",
    "\n",
    "                    if '.' in median:\n",
    "                        if len(median.split('.')[1]) < abs(exp):\n",
    "                            median += '0' * (abs(exp) - len(median.split('.')[1]))\n",
    "                            median = median.split('.')[0] + median.split('.')[1]\n",
    "                            median = median.lstrip('0')\n",
    "                        elif len(median.split('.')[1]) == abs(exp):\n",
    "                            median = median.split('.')[0] + median.split('.')[1]\n",
    "                            median = median.lstrip('0')\n",
    "                        elif len(median.split('.')[1]) > abs(exp): \n",
    "                            median = median.split('.')[0] + median.split('.')[1][0:abs(exp)] + '.' + median.split('.')[1][abs(exp):]\n",
    "                    elif '.' not in median:\n",
    "                        median += '0' * abs(exp)\n",
    "\n",
    "                    if '.' in lower:\n",
    "                        if len(lower.split('.')[1]) < abs(exp):\n",
    "                            lower += '0' * (abs(exp) - len(lower.split('.')[1]))\n",
    "                            lower = lower.split('.')[0] + lower.split('.')[1]\n",
    "                            lower = lower.lstrip('0')\n",
    "                        elif len(lower.split('.')[1]) == abs(exp):\n",
    "                            lower = lower.split('.')[0] + lower.split('.')[1]\n",
    "                            lower = lower.lstrip('0')\n",
    "                        elif len(lower.split('.')[1]) > abs(exp): \n",
    "                            lower = lower.split('.')[0] + lower.split('.')[1][0:abs(exp)] + '.' + lower.split('.')[1][abs(exp):]\n",
    "                    elif '.' not in lower:\n",
    "                        lower += '0' * abs(exp)\n",
    "                \n",
    "                    if '.' in upper:\n",
    "                        if len(upper.split('.')[1]) < abs(exp):\n",
    "                            upper += '0' * (abs(exp) - len(upper.split('.')[1]))\n",
    "                            upper = upper.split('.')[0] + upper.split('.')[1]\n",
    "                            upper = upper.lstrip('0')\n",
    "                        elif len(upper.split('.')[1]) == abs(exp):\n",
    "                            upper = upper.split('.')[0] + upper.split('.')[1]\n",
    "                            upper = upper.lstrip('0')\n",
    "                        elif len(upper.split('.')[1]) > abs(exp): \n",
    "                            upper = upper.split('.')[0] + upper.split('.')[1][0:abs(exp)] + '.' + upper.split('.')[1][abs(exp):]\n",
    "                    elif '.' not in upper:\n",
    "                        lower += '0' * abs(exp)\n",
    "\n",
    "                table = table + f'& ${median}_{{-{lower}}}^{{+{upper}}}$ '\n",
    "            \n",
    "            else:\n",
    "\n",
    "                table = table + '& $---$ '\n",
    "        \n",
    "        table = table + '\\\\\\\\' + '\\n' if i != len(slits) - 1 else table + '\\n'\n",
    "\n",
    "    table = table + '\\n\\\\enddata\\n\\n' \\\n",
    "        + '\\\\tablecomments{From left to right: slit label, peak separation between the redshifted and blueshifted Ly$\\\\alpha$ peaks ' \\\n",
    "        + '(km s$^{-1}$), FWHM of the blueshifted, central, and redshifted Ly$\\\\alpha$ peaks (km s$^{-1}$), respectively, ratio between ' \\\n",
    "        + 'the `minimum\\' flux density between the redshifted and blueshifted Ly$\\\\alpha$ peaks and the local continuum flux density, ' \\\n",
    "        + 'rest-frame Ly$\\\\alpha$ equivalent width ({\\AA}), central escape fraction (\\%), and magnification-corrected Ly$\\\\alpha$ luminosity ' \\\n",
    "        + '(10$^{41}$ erg s$^{-1}$). Because the deconvolved FWHMs of the central Ly$\\\\alpha$ peaks of slits M4 and M5 were not significantly ' \\\n",
    "        + 'greater than the instrumental line spread function FWHM ($\\sim55$ km s$^{-1}$), we quote the 84th percentiles of those measurements ' \\\n",
    "        + 'as an upper bound on the intrinsic FWHM of their central Ly$\\\\alpha$ peaks.}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\\\tablenotetext{a}{Slit M0\\'s observation was taken through thin cloud cover that prevented an accurate fluxing, so its ' \\\n",
    "        + 'significantly larger luminosity is not an accurate estimate. We do not include this data point in any figures or when ' \\\n",
    "        + 'estimating any correlations involving the Ly$\\\\alpha$ luminosity. See Table \\\\ref{tab:mage_log} for more information about ' \\\n",
    "        + 'the observation.}\\n' \\\n",
    "        + '\\n' \\\n",
    "        + '\\end{deluxetable*}'\n",
    "\n",
    "    f = open(f'{results}/tables/lya_measurements_table.txt', 'w', encoding='utf-8')\n",
    "    f.write(table)\n",
    "    f.close()\n",
    "\n",
    "    correlations_table = ''\n",
    "\n",
    "    lya_model_parameters_table = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "8979a94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\15136\\AppData\\Local\\Temp/ipykernel_11648/389543318.py:361: RuntimeWarning: invalid value encountered in sqrt\n",
      "  fwhms_c = np.sqrt((np.array(fwhms_c))**2 - (Rs)**2)\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9887ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83b584ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a888e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(file):\n",
    "\n",
    "    '''\n",
    "    Extract spectrum from the .txt file\n",
    "\n",
    "    Parameters:\n",
    "        file : str\n",
    "            Name of the file\n",
    "\n",
    "    Returns:\n",
    "        w : numpy.ndarray\n",
    "            Observed wavelength bins\n",
    "        f : numpy.ndarray\n",
    "            Observed flux densities\n",
    "        n : numpy.ndarray\n",
    "            Observed Gaussian standard deviation of observed flux densities\n",
    "    '''\n",
    "\n",
    "    # Retrieve the data columns\n",
    "    w, f, n = np.loadtxt(file, delimiter='\\t', comments=('#', 'w'), usecols=(0,1,2), unpack=True)\n",
    "    \n",
    "    # If the file is not the stacked leaker spectrum. This step is necessary because \n",
    "    if 'leaker' not in file:\n",
    "\n",
    "        # Remove bins of extreme outliers\n",
    "        w = w[f < 1e-20]\n",
    "        n = n[f < 1e-20]\n",
    "        f = f[f < 1e-20]\n",
    "\n",
    "        # Convert from units of erg/s/cm^2/Hz to erg/s/cm^2/Å\n",
    "        f = f * 2.998e18 / np.square(w)\n",
    "        n = n * 2.998e18 / np.square(w)\n",
    "\n",
    "    return w, f, n    \n",
    "\n",
    "@njit\n",
    "def rest_frame(w, f, n, z):\n",
    "\n",
    "    '''\n",
    "    Place the data in the rest frame\n",
    "\n",
    "    Parameters:\n",
    "        w : numpy.ndarray\n",
    "            Observed wavelength bins\n",
    "        f : numpy.ndarray\n",
    "            Observed flux densities\n",
    "        n : numpy.ndarray\n",
    "            Gaussian standard deviation of observed flux densities\n",
    "        z : numpy.float64\n",
    "            Redshift of the spectrum\n",
    "\n",
    "    Returns:\n",
    "        w : numpy.ndarray\n",
    "            Rest wavelength bins\n",
    "        f : numpy.ndarray\n",
    "            Rest flux densities\n",
    "        n : numpy.ndarray\n",
    "            Gaussian standard deviation of rest flux densities\n",
    "    '''\n",
    "\n",
    "    w = w / (1 + z)\n",
    "    f = f * (1 + z)\n",
    "    n = n * (1 + z)\n",
    "\n",
    "    return w, f, n\n",
    "\n",
    "def compute_continuum(w, f):\n",
    "\n",
    "    '''\n",
    "    Compute the local continuum\n",
    "\n",
    "    Parameters:\n",
    "        w : numpy.ndarray\n",
    "            Rest wavelength\n",
    "        f : numpy.ndarray\n",
    "            Rest flux density\n",
    "\n",
    "    Returns:\n",
    "        c : numpy.float64\n",
    "            Local continuum flux density\n",
    "    '''\n",
    "\n",
    "    # Compute the local continuum as the\n",
    "    # median flux density between 1221-1225 Å\n",
    "    f = f[(w >= 1221) & (w <= 1225)]\n",
    "    c = np.median(f)    \n",
    "\n",
    "    return c\n",
    "\n",
    "@njit\n",
    "def compute_ew(w, f, c, results):\n",
    "\n",
    "    '''\n",
    "    Compute the EW of the Lyα profile\n",
    "\n",
    "    Parameters:\n",
    "        w : numpy.ndarray\n",
    "            Rest wavelength\n",
    "        f : numpy.ndarray\n",
    "            Rest flux density\n",
    "        c : numpy.float64\n",
    "            Local continuum flux density\n",
    "        results : numpy.ndarray\n",
    "            Array containing measurements\n",
    "\n",
    "    Returns:\n",
    "        ew : numpy.float64\n",
    "            Equivalent width\n",
    "    '''\n",
    "\n",
    "    # Compute the EW of the Lyα profile\n",
    "    # between 1212-1221 Å\n",
    "    f = f[(w >= 1212) & (w <= 1221)]\n",
    "    w = w[(w >= 1212) & (w <= 1221)]\n",
    "    ew = -1 * np.trapz(1 - f / c, w)\n",
    "\n",
    "    results = np.append(results, ew)\n",
    "\n",
    "    return results\n",
    "\n",
    "@njit\n",
    "def compute_fcen(v, f, results):\n",
    "\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    # Compute the central escape fraction as a percent\n",
    "    f_cen = np.trapz(f[(v >= -100) & (v <= 100)], v[(v >= -100) & (v <= 100)]) / np.trapz(f[(v >= -1000) & (v <= 1000)], v[(v >= -1000) & (v <= 1000)]) * 100\n",
    "    results = np.append(results, f_cen)\n",
    "\n",
    "    return results\n",
    "\n",
    "def compute_l(w, f_mc, c, z, l_distance, l_results):\n",
    "\n",
    "    f_mc = f_mc[(w >= 1212 * (1 + z)) & ((w <= 1221 * (1 + z)))]\n",
    "    w = w[(w >= 1212 * (1 + z)) & ((w <= 1221 * (1 + z)))]\n",
    "\n",
    "    flux = np.trapz(f_mc - c, w)\n",
    "\n",
    "    l = 4 * np.pi * flux * l_distance**2\n",
    "    l_results = np.append(l_results, l)\n",
    "\n",
    "    return l_results\n",
    "\n",
    "\n",
    "def compute_ratio(v, f_mc, c, c_peak_range, ratio_results):\n",
    "\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    ratio = np.amax(f_mc[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]) / c\n",
    "    ratio_results = np.append(ratio_results, ratio)\n",
    "\n",
    "    return ratio_results\n",
    "\n",
    "def fit_peaks(v, f_mc, c, c_peak_range, model, i):\n",
    "\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    # If there is no blue peak\n",
    "    if i in [0,3,4]:\n",
    "        \n",
    "        # Assign the initial guess and parameter bounds; this must be done inside the Monte Carlo loop to get the best initial\n",
    "        # estimates so they can be built from the random sample\n",
    "        p0 = (np.amax(f_mc[(v >= 150) & (v <= 1000)]),200,20,1,np.amax(f_mc[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]),80,40,c)\n",
    "        bounds = ([0,0,0,0,0,0,0,0],[np.amax(f_mc),1000,np.inf,np.inf,np.amax(f_mc),200,85,np.mean(f_mc)])\n",
    "    \n",
    "    elif i in [2]:\n",
    "\n",
    "        # Assign the initial guess and parameter bounds; this must be done inside the Monte Carlo loop to get the best initial\n",
    "        # estimates so they can be built from the random sample\n",
    "        p0 = (np.amax(f_mc[(v >= 150) & (v <= 1000)]),200,20,1,np.amax(f_mc[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]) - c,50,20,c)\n",
    "        bounds = ([0,0,0,0,0,0,0,0],[np.amax(f_mc),500,np.inf,np.inf,np.amax(f_mc[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]) - c,100,40,np.amax(f_mc)])\n",
    "\n",
    "    # Otherwise\n",
    "    else:\n",
    "\n",
    "        # Assign the initial guess and parameter bounds; this must be done inside the Monte Carlo loop to get the best initial\n",
    "        # estimates so they can be built from the random sample\n",
    "        p0 = (np.amax(f_mc[(v >= -1000) & (v <= 0)]),-150,20,-1,np.amax(f_mc[(v >= 150) & (v <= 1000)]),200,20,1,np.amax(f_mc[(v >= c_peak_range[0]) & (v <= c_peak_range[1])]),80,40,c)\n",
    "        bounds = ([0,-1000,0,-np.inf,0,0,0,0,0,0,0,0],[np.amax(f_mc[(v >= -1000) & (v <= 0)]),0,np.inf,0,np.amax(f_mc),1000,np.inf,np.inf,np.amax(f_mc),200,85,np.mean(f_mc)])\n",
    "\n",
    "    p, cov = curve_fit(model, v, f_mc, p0=p0, bounds=bounds, maxfev=2000)\n",
    "\n",
    "    return p, cov\n",
    "\n",
    "def mc(w, v, f, n, z, c_peak_range, i):\n",
    "\n",
    "    '''\n",
    "    Parameters:\n",
    "        w : numpy.ndarray\n",
    "        f : numpy.ndarray\n",
    "        n : numpy.ndarray\n",
    "    '''\n",
    "\n",
    "    def two_peaks(x, amp_r, cen_r, width_r, skew_r, amp_c, cen_c, width_c, cntm):\n",
    "\n",
    "        return amp_r * np.exp(-((x - cen_r) / width_r)**2 / 2) * (1 + erf(skew_r * ((x - cen_r) / width_r) / np.sqrt(2))) \\\n",
    "            + amp_c * np.exp(-((x - cen_c) / width_c)**2 / 2) \\\n",
    "            + cntm\n",
    "\n",
    "    def three_peaks(x, amp_b, cen_b, width_b, skew_b, amp_r, cen_r, width_r, skew_r, amp_c, cen_c, width_c, cntm):\n",
    "\n",
    "        return amp_b * np.exp(-((x - cen_b) / width_b)**2 / 2) * (1 + erf(skew_b * ((x - cen_b) / width_b) / np.sqrt(2)))  \\\n",
    "            + amp_r * np.exp(-((x - cen_r) / width_r)**2 / 2) * (1 + erf(skew_r * ((x - cen_r) / width_r) / np.sqrt(2))) \\\n",
    "            + amp_c * np.exp(-((x - cen_c) / width_c)**2 / 2) \\\n",
    "            + cntm\n",
    "\n",
    "    cosmology=FlatLambdaCDM(70,0.3)\n",
    "    l_distance = cosmology.luminosity_distance(z).value * 3.086e24\n",
    "\n",
    "    # If there is no blue peak\n",
    "    if i in [0,2,3,4]:\n",
    "        model = two_peaks\n",
    "        fit_results = np.empty((1,8), dtype=np.float64)\n",
    "    \n",
    "    # Otherwise\n",
    "    else:\n",
    "        model = three_peaks\n",
    "        fit_results = np.empty((1,12), dtype=np.float64)\n",
    "\n",
    "    if i > 1:\n",
    "        f = f * 1e17\n",
    "        n = n * 1e17\n",
    "\n",
    "    mag = np.array([1.0, 1.0, 50.7, 14.6, 147.0, 36.1, 10.4, 31.6, 34.6, 29.4, 30.9], dtype=np.float64)\n",
    "\n",
    "    e_results = np.array([], dtype=np.float64)\n",
    "    fcen_results = np.array([], dtype=np.float64)\n",
    "    l_results = np.array([], dtype=np.float64)\n",
    "    ratio_results = np.array([], dtype=np.float64)\n",
    "\n",
    "    for s in range(1000):\n",
    "\n",
    "        f_mc = np.random.normal(f, n)\n",
    "\n",
    "        c = compute_continuum(w, f_mc)\n",
    "\n",
    "        e_results = compute_ew(w, f_mc, c, e_results)\n",
    "\n",
    "        fcen_results = compute_fcen(v, f_mc, fcen_results)\n",
    "\n",
    "        l_results = compute_l(w * (1 + z), f_mc * 1e-17 / (1 + z) / mag[i], c * 1e-17 / (1 + z) / mag[i], z, l_distance, l_results)    \n",
    "\n",
    "        #ratio_results = compute_ratio(v, f_mc, c, c_peak_range, ratio_results)\n",
    "\n",
    "        try:\n",
    "\n",
    "            p, _ = fit_peaks(v[(v >= -1000) & (v <= 1000) & (f_mc >= c)], f_mc[(v >= -1000) & (v <= 1000) & (f_mc >= c)], c, c_peak_range, model, i)\n",
    "            \n",
    "            ratio_results = np.append(ratio_results, p[-4] / p[-1])\n",
    "            \n",
    "            fit_results = np.append(fit_results, np.array([p]), axis=0)\n",
    "\n",
    "            '''\n",
    "            if i==0:\n",
    "\n",
    "                fig, ax = plt.subplots()  \n",
    "\n",
    "                ax.plot(v, f_mc, ds='steps-mid', color='black')\n",
    "                ax.plot(v, model(v, *p), ls='dashed', color='red')\n",
    "                ax.set_xlim(-1000,1000)\n",
    "                plt.show()\n",
    "            '''\n",
    "\n",
    "        except RuntimeError:\n",
    "            pass\n",
    "\n",
    "    # Remove the first empty row that was created\n",
    "    # when the array was initialized\n",
    "    fit_results = fit_results[1:]\n",
    "\n",
    "    for i, parameter in enumerate(fit_results.T):\n",
    "\n",
    "        #print(f'$np.median(parameter)_-{abs(np.median(parameter) - np.percentile(parameter, 16))}r}^r{+{abs(np.percentile(parameter, 84) - np.median(parameter))}r}$')\n",
    "        print('$' + str(np.median(parameter)) + \\\n",
    "            '_{-' + str(sf_round(abs(np.median(parameter) - np.percentile(parameter, 16)),1)) + '}' + \\\n",
    "            '^{+' + str(sf_round(abs(np.percentile(parameter, 84) - np.median(parameter)),1)) + '}' + \\\n",
    "            '$')\n",
    "\n",
    "    return fit_results, e_results, fcen_results, ratio_results, l_results\n",
    "\n",
    "def compute_fwhm_and_peak(amp, cen, width, skew, R, R_error):\n",
    "\n",
    "    '''\n",
    "    Returns:\n",
    "        fwhm : numpy.float64\n",
    "        loc : numpy.float64\n",
    "    '''\n",
    "    def lin_interp(x, y, i, half):\n",
    "        return x[i] + (x[i+1] - x[i]) * ((half - y[i]) / (y[i+1] - y[i]))\n",
    "\n",
    "    def model(x, amp, cen, width, skew):\n",
    "\n",
    "        return amp * np.exp(-((x - cen) / width)**2 / 2) * (1 + erf(skew * ((x - cen) / width) / np.sqrt(2)))\n",
    "\n",
    "    v = np.arange(-1000,1000,0.1)\n",
    "\n",
    "    peak = model(v, amp, cen, width, skew)    \n",
    "\n",
    "    loc = v[np.argmax(peak)]\n",
    "\n",
    "    half = np.amax(peak) / 2.0\n",
    "    signs = np.sign(np.add(peak, -half))\n",
    "    zero_crossings = (signs[0:-2] != signs[1:-1])\n",
    "    zero_crossings_i = np.where(zero_crossings)[0]\n",
    "\n",
    "    fwhm = abs(lin_interp(v, peak, zero_crossings_i[0], half) - lin_interp(v, peak, zero_crossings_i[1], half))\n",
    "\n",
    "    res = np.random.normal(R, R_error)\n",
    "\n",
    "    fwhm = np.sqrt((fwhm)**2 - (res)**2)\n",
    "\n",
    "    return fwhm, loc\n",
    "\n",
    "def label(fig, ax, fig_stack, ax_stack):\n",
    "\n",
    "    slits = np.array(['M5','M4','M6','M3','M0','M2','M7','M8','M9'], dtype=str)    \n",
    "\n",
    "    for i, row in enumerate(ax):\n",
    "\n",
    "        row[-1].yaxis.set_label_position('right')\n",
    "        row[-1].set_ylabel(slits[i], rotation=-90, labelpad=10)\n",
    "\n",
    "    ax_stack[0,6].yaxis.set_label_position('right')\n",
    "    ax_stack[0,6].set_ylabel('NL', rotation=-90, labelpad=10)\n",
    "\n",
    "    ax_stack[1,6].yaxis.set_label_position('right')\n",
    "    ax_stack[1,6].set_ylabel('L', rotation=-90, labelpad=10)\n",
    "\n",
    "    ax[3,0].set_title(r'$v_{\\rm{sep}}$')\n",
    "    ax[3,1].set_title('FWHM (blue)')\n",
    "    ax[0,2].set_title('FWHM (center)')\n",
    "    ax[0,3].set_title('FWHM (red)')\n",
    "    ax[0,4].set_title(r'$f_{\\rm{min}}/f_{\\rm{cont}}$')\n",
    "    ax[0,5].set_title('EW')\n",
    "    ax[0,6].set_title(r'$f_{\\rm{cen}}$')\n",
    "    ax[0,7].set_title('Luminosity')\n",
    "    \n",
    "    ax_stack[1,0].set_title(r'$v_{\\rm{sep}}$')\n",
    "    ax_stack[1,1].set_title('FWHM (blue)')\n",
    "    ax_stack[0,2].set_title('FWHM (center)')\n",
    "    ax_stack[0,3].set_title('FWHM (red)')\n",
    "    ax_stack[0,4].set_title(r'$f_{\\rm{min}}/f_{\\rm{cont}}$')\n",
    "    ax_stack[0,5].set_title('EW')\n",
    "    ax_stack[0,6].set_title(r'$f_{\\rm{cen}}$')\n",
    "\n",
    "    xlabels = np.array(['(km s$^{-1}$)','(km s$^{-1}$)','(km s$^{-1}$)',\n",
    "        '(km s$^{-1}$)','',r'($\\rm{\\AA}$)','(%)','(10$^{42}$ erg s$^{-1}$)'], dtype=str)\n",
    "\n",
    "    for i, column in enumerate(ax.T):\n",
    "\n",
    "        ax[8,i].set_xlabel(xlabels[i])\n",
    "\n",
    "    for i, column in enumerate(ax_stack.T):\n",
    "\n",
    "        ax_stack[1,i].set_xlabel(xlabels[i])\n",
    "\n",
    "    fig.supxlabel(r'$x-\\mu$')\n",
    "    fig.supylabel('Count')\n",
    "\n",
    "    fig_stack.supxlabel(r'$x-\\mu$')\n",
    "    fig_stack.supylabel('Count')\n",
    "\n",
    "def set_ticks(ax, ax_stack):\n",
    "\n",
    "    for i, row in enumerate(ax):\n",
    "\n",
    "        for j, column in enumerate(row):\n",
    "\n",
    "            if j == 7:\n",
    "                ax[i,j].tick_params(bottom=True, left=True, right=False)\n",
    "\n",
    "            else:\n",
    "                ax[i,j].tick_params(bottom=True, left=True, right=True)\n",
    "\n",
    "    ax[0,2].tick_params(labelleft=True)\n",
    "    ax[1,2].tick_params(labelleft=True)\n",
    "    ax[2,2].tick_params(labelleft=True)\n",
    "\n",
    "    for i, row in enumerate(ax_stack):\n",
    "\n",
    "        for j, column in enumerate(row):\n",
    "\n",
    "            if j == 6:\n",
    "                ax_stack[i,j].tick_params(bottom=True, left=True, right=False)\n",
    "\n",
    "            else:\n",
    "                ax_stack[i,j].tick_params(bottom=True, left=True, right=True)\n",
    "\n",
    "    ax_stack[0,2].tick_params(labelleft=True)\n",
    "\n",
    "def disable_plots(ax, ax_stack):\n",
    "\n",
    "    ax_stack[0,0].axis('off')    \n",
    "    ax_stack[0,1].axis('off')    \n",
    "\n",
    "    for i in [0,1,2]:\n",
    "\n",
    "        ax[i,0].axis('off')\n",
    "        ax[i,1].axis('off')\n",
    "\n",
    "def make_corner_plot():\n",
    "\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    fig, ax = plt.subplots(8,8, figsize=(16,16), sharex='col', sharey='row', constrained_layout=True)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    for i, row in enumerate(ax):\n",
    "\n",
    "        for j, subplot in enumerate(row):\n",
    "\n",
    "            subplot.set_aspect(1 / subplot.get_data_ratio(), adjustable='box')\n",
    "\n",
    "            if j > i:\n",
    "                ax[i,j].axis('off')\n",
    "    \n",
    "    #ax[0,0].set_aspect(1 / subplot.get_data_ratio(), adjustable='datalim')\n",
    "    #ax_dummy = fig.add_axes(ax[0,0].get_position())\n",
    "\n",
    "    #ax_dummy = ax[0,0].twinx()    \n",
    "    #ax_dummy.set_aspect('equal')\n",
    "\n",
    "    #row_titles = np.array(['FWHM(center)','FWHM (red)',r'$f_{\\rm{min}}/f_{\\rm{cont}}$','EW',r'$f_{\\rm{cen}}$','Luminosity',r'$f_{\\rm{esc}}^{\\rm{LyC}}$'], dtype=str)\n",
    "\n",
    "    row_titles = np.array(['FWHM (blue)', 'FWHM (center)', 'FWHM (red)', \n",
    "        r'$f_{\\rm{min}}/f_{\\rm{cont}}$', 'EW', r'$f_{\\rm{cen}}$',\n",
    "        'Luminosity', r'$f_{\\rm{esc}}^{\\rm{LyC}}$'], dtype=str)\n",
    "    column_titles = np.array([r'$v_{\\rm{sep}}$', 'FWHM (blue)', 'FWHM (center)',\n",
    "        'FWHM (red)', r'$f_{\\rm{min}}/f_{\\rm{cont}}$', 'EW', \n",
    "        r'$f_{\\rm{cen}}$', 'Luminosity'], dtype=str)\n",
    "    column_labels = np.array(['(km s$^{-1}$)', '(km s$^{-1}$)', '(km s$^{-1}$)',\n",
    "        '(km s$^{-1}$)', '', r'($\\rm{\\AA}$)',\n",
    "        '(%)', '(10$^{42}$ erg s$^{-1}$)'], dtype=str)\n",
    "    row_labels = np.array(['(km s$^{-1}$)', '(km s$^{-1}$)', '(km s$^{-1}$)',\n",
    "        '', r'($\\rm{\\AA}$)', '(%)', '(10$^{42}$ erg s$^{-1}$)', '(%)'], dtype=str)\n",
    "\n",
    "    for i, row in enumerate(ax):\n",
    "\n",
    "        ax[i,0].set_ylabel(row_titles[i], fontsize='large')\n",
    "        ax[i,i].set_title(column_titles[i])\n",
    "\n",
    "        ax[7,i].set_xlabel(column_labels[i], fontsize='large')\n",
    "\n",
    "        if i > 0:\n",
    "            ax[i,i].yaxis.set_label_position('right')\n",
    "            ax[i,i].set_ylabel(row_labels[i], fontsize='large')\n",
    "\n",
    "    for i, row in enumerate(ax):\n",
    "\n",
    "        for j, subplot in enumerate(row):\n",
    "\n",
    "            # Subplots on the interior\n",
    "            if ((i != j) & (i != 7) & (j != 0)):\n",
    "                subplot.tick_params(left=True, right=True, bottom=True, top=True,\n",
    "                    labelleft=False, labelright=False, labelbottom=False, labeltop=False, direction='in')\n",
    "\n",
    "            # Subplots on the main diagonal but not the corners\n",
    "            elif ((i == j) & (i != 7) & (j != 0)):\n",
    "                subplot.tick_params(left=True, right=True, bottom=True, top=True,\n",
    "                    labelleft=False, labelright=True, labelbottom=False, labeltop=False, direction='in')\n",
    "            \n",
    "            # Subplots on the left column but not the corners\n",
    "            elif ((j == 0) & (i != 0) & (i != 7)):\n",
    "                subplot.tick_params(left=True, right=True, bottom=True, top=True,\n",
    "                    labelleft=False, labelright=False, labelbottom=False, labeltop=False, direction='in')\n",
    "\n",
    "            # Subplots on the bottom row but not the corners\n",
    "            elif ((i == 7) & (j != 0) & (j != 7)):\n",
    "                subplot.tick_params(left=True, right=True, bottom=True, top=True,\n",
    "                    labelleft=False, labelright=False, labelbottom=True, labeltop=False, direction='in')\n",
    "\n",
    "    ax[0,0].tick_params(left=True, right=True, bottom=True, top=True,\n",
    "        labelleft=False, labelright=False, labelbottom=False, labeltop=False, direction='in')\n",
    "    ax[7,0].tick_params(left=True, right=True, bottom=True, top=True,\n",
    "        labelleft=False, labelright=False, labelbottom=True, labeltop=False, direction='in')\n",
    "    ax[7,7].tick_params(left=True, right=True, bottom=True, top=True,\n",
    "        labelleft=False, labelright=True, labelbottom=True, labeltop=False, direction='in')\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "def gaussian(x, amp, cen, width):\n",
    "    return amp * np.exp(-((x - cen) / width)**2 / 2)\n",
    "\n",
    "def skewed_gaussian(x, amp, cen, width, skew):\n",
    "    return amp * np.exp(-((x - cen) / width)**2 / 2) * (1 + erf(skew * ((x - cen) / width) / np.sqrt(2)))\n",
    "\n",
    "def measure():\n",
    "\n",
    "    # Establish directories\n",
    "    home = os.getcwd()\n",
    "    data = home + '/data'\n",
    "    figs = home + '/figs'\n",
    "\n",
    "    #files = glob.glob(data + '/spectra/mage/*MWdr.txt')\n",
    "\n",
    "    files = np.array(['rest_sba-nonleaker-no_m3_MWdr.txt','rest_sba-leaker-no_m0_MWdr.txt',\n",
    "                      'psz-arcslit-h3-comb1_MWdr.txt', 'psz-arcslit-h1-comb1_MWdr.txt',\n",
    "                      'sunburst_M-6-comb1_MWdr.txt', 'psz-arcslit-h4-comb1_MWdr.txt',\n",
    "                      'planckarc_pos1-comb1_MWdr.txt', 'psz-arcslit-h6-comb1_MWdr.txt',\n",
    "                      'psz-arcslit-h9-comb1_MWdr.txt', 'psz-arcslit-f-comb1_MWdr.txt',\n",
    "                      'psz-arcslit-h2-comb1_MWdr.txt'], dtype=object)\n",
    "    names = np.array(['NL', 'L', 'M5', 'M4', 'M6', 'M3', 'M0', 'M2', 'M7', 'M8', 'M9'], dtype=str)\n",
    "\n",
    "    f_esc = [2.3, -0.6, 3, 2.3, 17, 18, 12, 15, 14]\n",
    "    ne_esc = [0.8, 0.2, 1, 0.8, 6, 7, 5, 6, 5]\n",
    "\n",
    "    # For each file\n",
    "    for i, file in enumerate(files):\n",
    "\n",
    "        # Join its folder path to the file name\n",
    "        file = ''.join([data, '/spectra/mage/', file])\n",
    "        files[i] = file\n",
    "\n",
    "    z = np.array([0, 0, 2.37086, 2.37073, 2.37021, 2.37025, 2.37014, 2.37017, 2.37044, \n",
    "                  2.37024, 2.37030], dtype=np.float64)\n",
    "    c_peak_range = np.array([[60,140],[20,130],[0,100],[0,85],[10,130],[35,120],[10,130],\n",
    "                             [45,125],[15,100],[25,125],[15,125]], dtype=np.float64)\n",
    "\n",
    "    fig_mc, ax_mc = plt.subplots(9,8, figsize=(16,18), sharey='row', constrained_layout=True)\n",
    "    fig_mc_stack, ax_mc_stack = plt.subplots(2,7, figsize=(21,6), sharey='row', constrained_layout=True)\n",
    "\n",
    "    fig_lya, ax_lya = plt.subplots(3,3, figsize=(12,12), sharex=True, constrained_layout=True)\n",
    "    ax_lya_array = np.array(ax_lya).reshape(-1)\n",
    "\n",
    "    fig_lya_stack, ax_lya_stack = plt.subplots(3,1, figsize=(3,9), sharex=True)#, constrained_layout=True)\n",
    "    #fig_lya_stack.subplots_adjust(hspace=0, wspace=0)\n",
    "\n",
    "    fig_c, ax_c = make_corner_plot()\n",
    "\n",
    "    total_result = np.array([np.empty((9,1000))])\n",
    "\n",
    "    mag = np.array([1.0, 1.0, 50.7, 14.6, 147.0, 36.1, 10.4, 31.6, 34.6, 29.4, 30.9], dtype=np.float64)\n",
    "\n",
    "    R = np.array([\n",
    "        299792.458 / 5400,\n",
    "        299792.458 / 5300,\n",
    "        299792.458 / 5500,\n",
    "        299792.458 / 5400,\n",
    "        299792.458 / 5300,\n",
    "        299792.458 / 5500,\n",
    "        299792.458 / 4700,\n",
    "        299792.458 / 5300,\n",
    "        299792.458 / 5200,\n",
    "        299792.458 / 5200,\n",
    "        299792.458 / 5500\n",
    "    ])\n",
    "\n",
    "    R_error = np.array([\n",
    "        299792.458 / 5400**2 * 200,\n",
    "        299792.458 / 5300**2 * 200,\n",
    "        299792.458 / 5500**2 * 400,\n",
    "        299792.458 / 5400**2 * 300,\n",
    "        299792.458 / 5300**2 * 300,\n",
    "        299792.458 / 5500**2 * 400,\n",
    "        299792.458 / 4700**2 * 200,\n",
    "        299792.458 / 5300**2 * 300,\n",
    "        299792.458 / 5200**2 * 200,\n",
    "        299792.458 / 5200**2 * 300,\n",
    "        299792.458 / 5500**2 * 400\n",
    "    ])\n",
    "\n",
    "    # For each spectrum\n",
    "    for i, file in enumerate(files):\n",
    "\n",
    "        print(names[i])\n",
    "\n",
    "        # Extract the data from the .txt file\n",
    "        w, f, n = extract_data(file)\n",
    "\n",
    "        # Place the data in the rest frame\n",
    "        w, f, n = rest_frame(w, f, n, z[i])\n",
    "\n",
    "        f = f[(w >= 1195) & (w <= 1235)]\n",
    "        n = n[(w >= 1195) & (w <= 1235)]\n",
    "        w = w[(w >= 1195) & (w <= 1235)]\n",
    "\n",
    "        v = 299792.458 * (w / 1215.67 - 1)\n",
    "\n",
    "        #cosmology = FlatLambdaCDM(70, 0.3)\n",
    "        #l_distance = cosmology.luminosity_distance(z[i]).value * 3.086e24\n",
    "\n",
    "        fit_results, e_results, fcen_results, ratio_results, l_results = mc(w, v, f, n, z[i], c_peak_range[i], i)    \n",
    "\n",
    "        median_fit = [np.median(e) for e in fit_results.T]\n",
    "\n",
    "        # Take the transpose of the results so that the rows\n",
    "        # are by parameter, not sample number\n",
    "        #fit_results = fit_results.T\n",
    "\n",
    "        if i in [0,2,3,4]:\n",
    "            stdv_c_results = fit_results.T[6]\n",
    "            fwhm_c_results = 2 * np.sqrt(2 * np.log(2)) * stdv_c_results\n",
    "\n",
    "            fwhm_b_results = np.array([], dtype=np.float64)\n",
    "            fwhm_c_results = np.array([], dtype=np.float64)\n",
    "            fwhm_r_results = np.array([], dtype=np.float64)\n",
    "            vsep_results = np.array([], dtype=np.float64)\n",
    "\n",
    "            for j, sample in enumerate(fit_results):\n",
    "\n",
    "                fwhm_r, _ = compute_fwhm_and_peak(sample[0], sample[1], sample[2], sample[3], R[i], R_error[i])\n",
    "                fwhm_r_results = np.append(fwhm_r_results, fwhm_r)\n",
    "\n",
    "                fwhm_c, _ = compute_fwhm_and_peak(sample[4], sample[5], sample[6], 0, R[i], R_error[i])\n",
    "                fwhm_c_results = np.append(fwhm_c_results, fwhm_c)\n",
    "\n",
    "                if np.isnan(fwhm_c):\n",
    "                    print(i, fwhm_c)\n",
    "\n",
    "        else:\n",
    "            stdv_c_results = fit_results.T[10]\n",
    "            fwhm_c_results = 2 * np.sqrt(2 * np.log(2)) * stdv_c_results\n",
    "\n",
    "            fwhm_b_results = np.array([], dtype=np.float64)\n",
    "            fwhm_c_results = np.array([], dtype=np.float64)\n",
    "            fwhm_r_results = np.array([], dtype=np.float64)\n",
    "            vsep_results = np.array([], dtype=np.float64)\n",
    "\n",
    "            for j, sample in enumerate(fit_results):\n",
    "\n",
    "                fwhm_b, loc_b = compute_fwhm_and_peak(sample[0], sample[1], sample[2], sample[3], R[i], R_error[i])\n",
    "                fwhm_b_results = np.append(fwhm_b_results, fwhm_b)\n",
    "\n",
    "                fwhm_c, _ = compute_fwhm_and_peak(sample[8], sample[9], sample[10], 0, R[i], R_error[i])\n",
    "                fwhm_c_results = np.append(fwhm_c_results, fwhm_c)\n",
    "\n",
    "                if np.isnan(fwhm_c):\n",
    "                    print(i, fwhm_c)\n",
    "\n",
    "                fwhm_r, loc_r = compute_fwhm_and_peak(sample[4], sample[5], sample[6], sample[7], R[i], R_error[i])\n",
    "                fwhm_r_results = np.append(fwhm_r_results, fwhm_r)\n",
    "\n",
    "                vsep = abs(loc_r - loc_b)\n",
    "                vsep_results = np.append(vsep_results, vsep)\n",
    "\n",
    "        fwhm_c_results = fwhm_c_results[~np.isnan(fwhm_c_results)]\n",
    "\n",
    "        # If the spectrum is one of the stacked spectra\n",
    "        if i < 2:\n",
    "\n",
    "            if i == 0:\n",
    "                color='#DC3220'\n",
    "                markersize = 8\n",
    "            else:\n",
    "                color='#005AB5'\n",
    "                markersize = 8\n",
    "\n",
    "            ax_mc_stack[i,0].hist(vsep_results - np.median(vsep_results), bins=30, color=color)\n",
    "            ax_mc_stack[i,1].hist(fwhm_b_results - np.median(fwhm_b_results), bins=30, color=color)\n",
    "            ax_mc_stack[i,2].hist(fwhm_c_results - np.median(fwhm_c_results), bins=30, color=color)\n",
    "            ax_mc_stack[i,3].hist(fwhm_r_results - np.median(fwhm_r_results), bins=30, color=color)\n",
    "            ax_mc_stack[i,4].hist(ratio_results - np.median(ratio_results), bins=30, color=color)\n",
    "            ax_mc_stack[i,5].hist(e_results - np.median(e_results), bins=30, color=color)\n",
    "            ax_mc_stack[i,6].hist(fcen_results - np.median(fcen_results), bins=30, color=color)\n",
    "\n",
    "            results = [vsep_results, fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results]\n",
    "            r = np.array([None,None,None,None,1,1,1])\n",
    "            tau = np.array([None,None,None,None,1,1,1])\n",
    "            rho = np.array([None,None,None,None,1,1,1])\n",
    "\n",
    "            # For each subplot in a row\n",
    "            for j, subplot in enumerate(ax_mc_stack[i]):\n",
    "\n",
    "                try:\n",
    "\n",
    "                    median = np.median(results[j][~np.isnan(results[j])])\n",
    "                    lower = sigfig.round(str(median - np.percentile(results[j][~np.isnan(results[j])], 16)), 2)\n",
    "                    upper = sigfig.round(str(np.percentile(results[j][~np.isnan(results[j])], 84) - median), 2)\n",
    "\n",
    "                    # If both percentiles are greater than 10, they will not have\n",
    "                    # any significant figures past the decimal place\n",
    "                    if abs(float(lower)) >= 10 and abs(float(upper)) >= 10:\n",
    "                        # Find the smallest value between the two percentiles\n",
    "                        string = str(np.amin([int(lower), int(upper)]))\n",
    "\n",
    "                        # Count the number of zeros in the string\n",
    "                        zeros = string.count('0')\n",
    "\n",
    "                        # If the length of the string differs by only one from\n",
    "                        # the number of zeros, this indicates one of the 2 \n",
    "                        # significant figures is a zero, so to prevent the median\n",
    "                        # from being rounded prematurely (to the significant figure\n",
    "                        # before the zero), subtract the number of zeros by one\n",
    "                        if len(string) == zeros + 1:\n",
    "                            zeros = zeros - 1\n",
    "\n",
    "                        zeros = -1 * zeros\n",
    "\n",
    "                        median = str(int(round(median, zeros)))\n",
    "\n",
    "                    elif (abs(float(lower)) >= 10 and abs(float(upper)) < 10) or (abs(float(lower)) < 10 and abs(float(upper)) >= 10):\n",
    "                        print('check')\n",
    "\n",
    "                        if abs(float(lower)) >= 10:\n",
    "                            median = str(round(median, len(upper.split('.')[1])))\n",
    "                        if abs(float(upper)) >= 10:\n",
    "                            median = str(round(median, len(lower.split('.')[1])))\n",
    "                        # Round the median to the smallest digit place between the upper and lower percentiles   \n",
    "                        #median = str(round(median, np.amax([len(lower.split('.')[1]), len(upper.split('.')[1])])))\n",
    "\n",
    "                    # Otherwise, since the percentiles are rounded to 2 significant\n",
    "                    # figures, they will have at least one significant digit past\n",
    "                    # the decimal place\n",
    "                    else:\n",
    "                        # Round the median to the smallest digit place between the upper and lower percentiles   \n",
    "                        median = str(round(median, np.amax([len(lower.split('.')[1]), len(upper.split('.')[1])])))\n",
    "\n",
    "                    at = AnchoredText('$' + median + '_{-' + lower + '}^{+' + upper + '}$',\n",
    "                        loc='upper right', frameon=False)\n",
    "                    subplot.add_artist(at)\n",
    "\n",
    "                # Except if there are no results for the parameter\n",
    "                # (because it was inapplicable to the profile)\n",
    "                except IndexError:\n",
    "                    pass\n",
    "\n",
    "            ax_lya_stack[0].fill_between(v[(v >= -1100) & (v <= 1100)], (f - 3 * n)[(v >= -1100) & (v <= 1100)], (f + 3 * n)[(v >= -1100) & (v <= 1100)], step='mid', facecolor=color, alpha=0.3)\n",
    "            ax_lya_stack[0].plot(v[(v >= -1100) & (v <= 1100)], f[(v >= -1100) & (v <= 1100)], ds='steps-mid', c=color)\n",
    "\n",
    "            for j, result_row in enumerate([fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results]):\n",
    "                \n",
    "                # If this is the stacked nonleaker spectrum\n",
    "                if i == 0:\n",
    "                    if j == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        for k, result_col in enumerate([fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results]):\n",
    "                            \n",
    "                            if k + 2 > j:\n",
    "                                pass\n",
    "                            else:\n",
    "                                ax_c[j,k+2].errorbar(np.median(result_col), np.median(result_row), \n",
    "                                    xerr=[[np.median(result_col) - np.percentile(result_col, 16)], [np.percentile(result_col, 84) - np.median(result_col)]], \n",
    "                                    yerr=[[np.median(result_row) - np.percentile(result_row, 16)], [np.percentile(result_row, 84) - np.median(result_row)]],\n",
    "                                    lw=1, marker='s', markersize=markersize, mec=color, mfc=color, ecolor=color)\n",
    "                else:\n",
    "                    for k, col in enumerate([vsep_results, fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results]):\n",
    "\n",
    "                        if k > j:\n",
    "                            pass\n",
    "                        else:\n",
    "                            ax_c[j,k].errorbar(np.median(col), np.median(result_row),\n",
    "                                xerr=[[np.median(col) - np.percentile(col, 16)], [np.percentile(col, 84) - np.median(col)]], \n",
    "                                yerr=[[np.median(result_row) - np.percentile(result_row, 16)], [np.percentile(result_row, 84) - np.median(result_row)]],\n",
    "                                lw=1, marker='o', markersize=markersize, mec=color, mfc=color, ecolor=color)\n",
    "\n",
    "            if i == 0:\n",
    "\n",
    "                ax_lya_stack[1].fill_between(v[(v >= -1100) & (v <= 1100)], (f - 3 * n)[(v >= -1100) & (v <= 1100)], (f + 3 * n)[(v >= -1100) & (v <= 1100)], step='mid', facecolor=color, alpha=0.3)\n",
    "                ax_lya_stack[1].plot(v[(v >= -1100) & (v <= 1100)], f[(v >= -1100) & (v <= 1100)], ds='steps-mid', c=color)\n",
    "                ax_lya_stack[1].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (gaussian(v, median_fit[4], median_fit[5], median_fit[6]) + median_fit[7])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_stack[1].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (skewed_gaussian(v, median_fit[0], median_fit[1], median_fit[2], median_fit[3]) + median_fit[7])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_stack[1].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (gaussian(v, median_fit[4], median_fit[5], median_fit[6]) + skewed_gaussian(v, median_fit[0], median_fit[1], median_fit[2], median_fit[3]) + median_fit[7])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dashed')\n",
    "\n",
    "            else:\n",
    "\n",
    "                ax_lya_stack[2].fill_between(v[(v >= -1100) & (v <= 1100)], (f - 3 * n)[(v >= -1100) & (v <= 1100)], (f + 3 * n)[(v >= -1100) & (v <= 1100)], step='mid', facecolor=color, alpha=0.3)\n",
    "                ax_lya_stack[2].plot(v[(v >= -1100) & (v <= 1100)], f[(v >= -1100) & (v <= 1100)], ds='steps-mid', c=color)\n",
    "                ax_lya_stack[2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (gaussian(v, median_fit[8], median_fit[9], median_fit[10]) + median_fit[11])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_stack[2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (skewed_gaussian(v, median_fit[0], median_fit[1], median_fit[2], median_fit[3]) + median_fit[11])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_stack[2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (skewed_gaussian(v, median_fit[4], median_fit[5], median_fit[6], median_fit[7]) + median_fit[11])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_stack[2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (gaussian(v, median_fit[8], median_fit[9], median_fit[10]) + skewed_gaussian(v, median_fit[4], median_fit[5], median_fit[6], median_fit[7]) + skewed_gaussian(v, median_fit[0], median_fit[1], median_fit[2], median_fit[3]) + median_fit[11])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dashed')\n",
    "\n",
    "        # If the spectrum is not one of the stacked spectra\n",
    "        elif i > 1:\n",
    "\n",
    "            if i in [2,3,4]:\n",
    "                color='#DC3220'\n",
    "                marker = 's'\n",
    "                markersize = 8\n",
    "\n",
    "            elif i == 5:\n",
    "                color='#D35FB7'\n",
    "                marker = '*'\n",
    "                markersize = 10\n",
    "\n",
    "            else:\n",
    "                color='#005AB5'\n",
    "                marker = 'o'\n",
    "                markersize = 8\n",
    "\n",
    "            ax_mc[i-2,0].hist(vsep_results - np.median(vsep_results), bins=30, color=color)\n",
    "            ax_mc[i-2,1].hist(fwhm_b_results - np.median(fwhm_b_results), bins=30, color=color)\n",
    "            ax_mc[i-2,2].hist(fwhm_c_results - np.median(fwhm_c_results), bins=30, color=color)\n",
    "            ax_mc[i-2,3].hist(fwhm_r_results - np.median(fwhm_r_results), bins=30, color=color)\n",
    "            ax_mc[i-2,4].hist(ratio_results - np.median(ratio_results), bins=30, color=color)\n",
    "            ax_mc[i-2,5].hist(e_results - np.median(e_results), bins=30, color=color)\n",
    "            ax_mc[i-2,6].hist(fcen_results - np.median(fcen_results), bins=30, color=color)\n",
    "            ax_mc[i-2,7].hist((l_results - np.median(l_results)) * 1e-42, bins=30, color=color)\n",
    "            \n",
    "            results = [vsep_results, fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results, l_results * 1e-42]\n",
    "            r = np.array([None,None,None,None,1,1,1,1])\n",
    "            tau = np.array([None,None,None,None,1,1,1,1])\n",
    "            rho = np.array([None,None,None,None,1,1,1,1])\n",
    "\n",
    "            # For each subplot in a row\n",
    "            for j, subplot in enumerate(ax_mc[i-2]):\n",
    "\n",
    "                try:\n",
    "\n",
    "                    #median = np.median(results[j])\n",
    "                    #lower = median - np.percentile(results[j], 16)\n",
    "                    #upper = np.percentile(results[j], 84) - median\n",
    "\n",
    "                    median = np.median(results[j][~np.isnan(results[j])])\n",
    "                    lower = sigfig.round(str(median - np.percentile(results[j][~np.isnan(results[j])], 16)), 2)\n",
    "                    upper = sigfig.round(str(np.percentile(results[j][~np.isnan(results[j])], 84) - median), 2)\n",
    "\n",
    "                    #print(median, lower, upper)\n",
    "\n",
    "                    # If both percentiles are greater than 10, they will not have\n",
    "                    # any significant figures past the decimal place\n",
    "                    if abs(float(lower)) >= 10 and abs(float(upper)) >= 10:\n",
    "                        # Find the smallest value between the two percentiles\n",
    "                        string = str(np.amin([int(lower), int(upper)]))\n",
    "\n",
    "                        # Count the number of zeros in the string\n",
    "                        zeros = string.count('0')\n",
    "\n",
    "                        # If the length of the string differs by only one from\n",
    "                        # the number of zeros, this indicates one of the 2 \n",
    "                        # significant figures is a zero, so to prevent the median\n",
    "                        # from being rounded prematurely (to the significant figure\n",
    "                        # before the zero), subtract the number of zeros by one\n",
    "                        if len(string) == zeros + 1:\n",
    "                            zeros = zeros - 1\n",
    "\n",
    "                        zeros = -1 * zeros\n",
    "\n",
    "                        median = str(int(round(median, zeros)))\n",
    "                    \n",
    "                    elif (abs(float(lower)) >= 10 and abs(float(upper)) < 10) or (abs(float(lower)) < 10 and abs(float(upper)) >= 10):\n",
    "                        print('check')\n",
    "                        \n",
    "                        if abs(float(lower)) >= 10:\n",
    "                            formatter = '{:.' + str(len(upper.split('.')[1])) + 'f}'\n",
    "                            median = formatter.format(round(median, len(upper.split('.')[1])))\n",
    "                        if abs(float(upper)) >= 10:\n",
    "                            formatter = '{:.' + str(len(lower.split('.')[1])) + 'f}'\n",
    "                            median = formatter.format(round(median, len(lower.split('.')[1])))\n",
    "                        # Round the median to the smallest digit place between the upper and lower percentiles   \n",
    "                        #median = str(round(median, np.amax([len(lower.split('.')[1]), len(upper.split('.')[1])])))\n",
    "\n",
    "                    # Otherwise, since the percentiles are rounded to 2 significant\n",
    "                    # figures, they will have at least one significant digit past\n",
    "                    # the decimal place\n",
    "                    else:\n",
    "                        # Round the median to the smallest digit place between the upper and lower percentiles   \n",
    "                        formatter = '{:.' + str(np.amax([len(lower.split('.')[1]), len(upper.split('.')[1])])) + 'f}'\n",
    "                        median = formatter.format(round(median, np.amax([len(lower.split('.')[1]), len(upper.split('.')[1])])))\n",
    "\n",
    "                    #at = AnchoredText('$' + str(round(median, r[j])) + '_{-' + str(round(lower, r[j])) + '}^{+' + str(round(upper, r[j])) + '}$',\n",
    "                    #    loc='upper right', frameon=False)\n",
    "                    at = AnchoredText('$' + median + '_{-' + lower + '}^{+' + upper + '}$',\n",
    "                        loc='upper right', frameon=False)\n",
    "                    subplot.add_artist(at)\n",
    "\n",
    "                # Except if there are no results for the parameter\n",
    "                # (because it was inapplicable to the profile)\n",
    "                except IndexError:\n",
    "                    pass\n",
    "\n",
    "            ax_lya_array[i-2].fill_between(v[(v >= -1100) & (v <= 1100)], (f / mag[i] - 2 * n / mag[i])[(v >= -1100) & (v <= 1100)] * 1e17, (f / mag[i] + 2 * n / mag[i])[(v >= -1100) & (v <= 1100)] * 1e17, step='mid', facecolor=color, alpha=0.3)\n",
    "            ax_lya_array[i-2].plot(v[(v >= -1100) & (v <= 1100)], (f / mag[i])[(v >= -1100) & (v <= 1100)] * 1e17, ds='steps-mid', c=color)\n",
    "\n",
    "            if i in [2,3,4]:\n",
    "\n",
    "                ax_lya_array[i-2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (gaussian(v, median_fit[4] / mag[i], median_fit[5], median_fit[6]) + median_fit[7] / mag[i])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_array[i-2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (skewed_gaussian(v, median_fit[0] / mag[i], median_fit[1], median_fit[2], median_fit[3]) + median_fit[7] / mag[i])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_array[i-2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (gaussian(v, median_fit[4] / mag[i], median_fit[5], median_fit[6]) + skewed_gaussian(v, median_fit[0] / mag[i], median_fit[1], median_fit[2], median_fit[3]) + median_fit[7] / mag[i])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dashed')\n",
    "\n",
    "            else:\n",
    "\n",
    "                ax_lya_array[i-2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (gaussian(v, median_fit[8] / mag[i], median_fit[9], median_fit[10]) + median_fit[11] / mag[i])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_array[i-2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (skewed_gaussian(v, median_fit[0] / mag[i], median_fit[1], median_fit[2], median_fit[3]) + median_fit[11] / mag[i])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_array[i-2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (skewed_gaussian(v, median_fit[4] / mag[i], median_fit[5], median_fit[6], median_fit[7]) + median_fit[11] / mag[i])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dotted')\n",
    "                ax_lya_array[i-2].plot(v[(v >= -1100) & (v <= 1100)],\n",
    "                    (gaussian(v, median_fit[8] / mag[i], median_fit[9], median_fit[10]) + skewed_gaussian(v, median_fit[4] / mag[i], median_fit[5], median_fit[6], median_fit[7]) + skewed_gaussian(v, median_fit[0] / mag[i], median_fit[1], median_fit[2], median_fit[3]) + median_fit[11] / mag[i])[(v >= -1100) & (v <= 1100)],\n",
    "                    color='black', ls='dashed')\n",
    "\n",
    "            at = AnchoredText(names[i], loc='upper right', frameon=False, prop=dict(fontsize='large'))\n",
    "            ax_lya_array[i-2].add_artist(at)\n",
    "\n",
    "            ax_lya_array[i-2].set_xlim(-1000,1000)\n",
    "            ax_lya_array[i-2].set_ylim(bottom=0)\n",
    "\n",
    "            # For each parameter row\n",
    "            for j, row in enumerate([fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results, l_results * 1e-42, f_esc[i-2]]):\n",
    "                \n",
    "                # If the spectrum does not have a triple peak\n",
    "                if i in [2,3,4]:\n",
    "                    \n",
    "                    # For each parameter column\n",
    "                    for k, col in enumerate([fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results, l_results * 1e-42]):\n",
    "                        if k + 2 > j:\n",
    "                            pass\n",
    "                        else:\n",
    "                            if j == 0:\n",
    "                                pass\n",
    "                            elif j == 7:\n",
    "                                ax_c[j,k+2].errorbar(np.median(col), f_esc[i-2],\n",
    "                                    xerr=[[np.median(col) - np.percentile(col, 16)], [np.percentile(col, 84) - np.median(col)]], \n",
    "                                    yerr=ne_esc[i-2],\n",
    "                                    lw=1, marker=marker, markersize=markersize, mec=color, mfc='none', ecolor=color)\n",
    "                            else:\n",
    "                                ax_c[j,k+2].errorbar(np.median(col), np.median(row),\n",
    "                                    xerr=[[np.median(col) - np.percentile(col, 16)], [np.percentile(col, 84) - np.median(col)]], \n",
    "                                    yerr=[[np.median(row) - np.percentile(row, 16)], [np.percentile(row, 84) - np.median(row)]],\n",
    "                                    lw=1, marker=marker, markersize=markersize, mec=color, mfc='none', ecolor=color)\n",
    "                else:\n",
    "                    for k, col in enumerate([vsep_results, fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results, l_results * 1e-42]):\n",
    "                        \n",
    "                        # If the row / column combination is above the main diagonal\n",
    "                        if k > j:\n",
    "                            pass\n",
    "\n",
    "                        else:\n",
    "                            if (j == 7) and (i != 6):\n",
    "                                ax_c[j,k].errorbar(np.median(col), f_esc[i-2],\n",
    "                                    xerr=[[np.median(col) - np.percentile(col, 16)], [np.percentile(col, 84) - np.median(col)]], \n",
    "                                    yerr=ne_esc[i-2],\n",
    "                                    lw=1, marker=marker, markersize=markersize, mec=color, mfc='none', ecolor=color)\n",
    "                            elif (j == 7) and (k != 7) and (i == 6):\n",
    "                                ax_c[j,k].errorbar(np.median(col), f_esc[i-2],\n",
    "                                    xerr=[[np.median(col) - np.percentile(col, 16)], [np.percentile(col, 84) - np.median(col)]], \n",
    "                                    yerr=ne_esc[i-2],\n",
    "                                    lw=1, marker=marker, markersize=markersize, mec=color, mfc='none', ecolor=color)\n",
    "                            elif (j == 6 or k == 7) and (i == 6):\n",
    "                                pass\n",
    "                            else:\n",
    "                                ax_c[j,k].errorbar(np.median(col), np.median(row),\n",
    "                                    xerr=[[np.median(col) - np.percentile(col, 16)], [np.percentile(col, 84) - np.median(col)]], \n",
    "                                    yerr=[[np.median(row) - np.percentile(row, 16)], [np.percentile(row, 84) - np.median(row)]],\n",
    "                                    lw=1, marker=marker, markersize=markersize, mec=color, mfc='none', ecolor=color)\n",
    "\n",
    "        # If the spectrum is not a stacked one\n",
    "        if i > 1:\n",
    "            # Create pseudo-measurements of the LyC escape fraction assuming the calculated value and standard deviation\n",
    "            # correspond to a Gaussian mean and standard deviation\n",
    "            fesc_results = np.random.normal(f_esc[i-2], ne_esc[i-2], 1000)\n",
    "\n",
    "        else:\n",
    "            fesc_results = np.empty(1000)\n",
    "            fesc_results[:] = np.nan\n",
    "\n",
    "        slit_result = np.array([np.empty(1000)])\n",
    "\n",
    "        for j, result in enumerate([vsep_results, fwhm_b_results, fwhm_c_results, fwhm_r_results, ratio_results, e_results, fcen_results, l_results * 1e-42, fesc_results]):\n",
    "            \n",
    "            # Try to append any missing values as NaNs\n",
    "            try:\n",
    "                if len(result) < 1000:\n",
    "                    a = np.empty(1000 - len(result))\n",
    "                    a[:] = np.nan\n",
    "\n",
    "                    result = np.append(result, a)\n",
    "\n",
    "            # Unless the array has not been instantiated\n",
    "            except TypeError:\n",
    "                result = np.empty(1000)\n",
    "                result[:] = np.nan\n",
    "            \n",
    "            if (i == 6) and (j == 7):\n",
    "                result = np.empty(1000)\n",
    "                result[:] = np.nan\n",
    "\n",
    "            result = np.where(result != 0.0, result, np.nan)\n",
    "\n",
    "            slit_result = np.append(slit_result, np.array([result]), axis=0)\n",
    "\n",
    "        # Drop the first row since it is empty\n",
    "        slit_result = slit_result[1:]\n",
    "\n",
    "        total_result = np.append(total_result, np.array([slit_result]), axis=0)\n",
    "\n",
    "    total_result = total_result[1:]\n",
    "\n",
    "    r_locs = [[3,0,0,0,0,0,0,0],\n",
    "        [2,4,0,0,0,0,0,0],\n",
    "        [1,1,2,0,0,0,0,0],\n",
    "        [3,3,2,7,0,0,0,0],\n",
    "        [3,3,2,7,2,0,0,0],\n",
    "        [3,3,2,7,2,2,1,1],\n",
    "        [3,3,2,1,2,2,2,1],\n",
    "        [2,'center left',2,4,2,2,2,2]]\n",
    "\n",
    "    # For each row in the corner plot\n",
    "    for i, row in enumerate(ax_c):\n",
    "\n",
    "        # For each column in the corner plot\n",
    "        for j, subplot in enumerate(row):\n",
    "\n",
    "            r_results = np.array([], dtype=np.float64)\n",
    "            tau_results = np.array([], dtype=np.float64)\n",
    "            rho_results = np.array([], dtype=np.float64)\n",
    "\n",
    "            # If the row / column pair is above the main diagonal\n",
    "            if j > i:\n",
    "                pass\n",
    "            else:\n",
    "\n",
    "                x_data = np.empty(1000)\n",
    "                x_data[:] = np.nan\n",
    "                x_data = np.array([x_data])\n",
    "                y_data = np.empty(1000)\n",
    "                y_data[:] = np.nan\n",
    "                y_data = np.array([y_data])\n",
    "\n",
    "                # For each spectrum\n",
    "                for k, slit in enumerate(total_result):\n",
    "                    \n",
    "                    if all(np.isnan(slit[j])) or all(np.isnan(slit[i + 1])):\n",
    "                        pass\n",
    "                    else:\n",
    "                        #if ~all(np.isnan(slit[j])) and ~all(np.isnan(slit[j + 1])):\n",
    "                        x_data = np.append(x_data, np.array([slit[j]]), axis=0)\n",
    "                        y_data = np.append(y_data, np.array([slit[i + 1]]), axis=0)\n",
    "                        #pass\n",
    "                    '''\n",
    "                    else:\n",
    "                        x_data = np.append(x_data, np.array([slit[j]]), axis=0)\n",
    "                        y_data = np.append(x_data, np.array([slit[j + 1]]), axis=0)\n",
    "                    '''\n",
    "\n",
    "                x_data = x_data[1:]\n",
    "                y_data = y_data[1:]\n",
    "\n",
    "                #r_results = np.array([], dtype=np.float64)\n",
    "                #tau_results = np.array([], dtype=np.float64)\n",
    "                #rho_results = np.array([], dtype=np.float64)\n",
    "\n",
    "                if (i==6) and (j==6):\n",
    "                    pass\n",
    "                    #print(x_data)\n",
    "                    #print(y_data)\n",
    "\n",
    "                for k, sample in enumerate(x_data[0]):\n",
    "\n",
    "                    x = x_data[:,k]\n",
    "                    y = y_data[:,k]\n",
    "\n",
    "                    if any(np.isnan(x)) or any(np.isnan(y)):\n",
    "                        pass\n",
    "                    else:\n",
    "                        x = x[x != 0]\n",
    "                        y = y[y != 0]\n",
    "\n",
    "                        r = np.corrcoef(x, y)[0,1]\n",
    "                        tau = kendalltau(x, y).statistic\n",
    "                        rho = spearmanr(x, y).statistic\n",
    "\n",
    "                        r_results = np.append(r_results, r)\n",
    "                        tau_results = np.append(tau_results, tau)\n",
    "                        rho_results = np.append(rho_results, rho)\n",
    "\n",
    "                print(i, j)\n",
    "                print(len(r_results), np.median(r_results), \n",
    "                    sf_round(abs(np.median(r_results) - np.percentile(r_results, 16)),1), \n",
    "                    sf_round(abs(np.percentile(r_results, 84) - np.median(r_results)),1))\n",
    "                print(len(tau_results), np.median(tau_results), \n",
    "                    sf_round(abs(np.median(tau_results) - np.percentile(tau_results, 16)),1), \n",
    "                    sf_round(abs(np.percentile(tau_results, 84) - np.median(tau_results)),1))\n",
    "                print(len(rho_results), np.median(rho_results), \n",
    "                    sf_round(abs(np.median(rho_results) - np.percentile(rho_results, 16)),1), \n",
    "                    sf_round(abs(np.percentile(rho_results, 84) - np.median(rho_results)),1))\n",
    "                \n",
    "                '''\n",
    "                for k in [r_results, tau_results]:\n",
    "\n",
    "                    median = np.median(k)\n",
    "                    p16 = abs(np.median(k) - np.percentile(k, 16))\n",
    "                    p84 = abs(np.percentile(k, 84) - np.median(k))\n",
    "\n",
    "                    print(median, p16, p84)\n",
    "\n",
    "                    print(len(k), np.median(k), abs(np.median(r_results) - np.percentile(r_results, 16)), abs(np.percentile(r_results, 84) - np.median(r_results)))   \n",
    "\n",
    "                    plt.hist(k, bins=20)\n",
    "                    plt.show()\n",
    "                '''\n",
    "\n",
    "                lower = sigfig.round(abs(np.median(r_results) - np.percentile(r_results, 16)), 2)\n",
    "                upper = sigfig.round(abs(np.percentile(r_results, 84) - np.median(r_results)), 2)\n",
    "\n",
    "                d = [decimal.Decimal(lower).as_tuple().exponent, decimal.Decimal(upper).as_tuple().exponent]\n",
    "                d = abs(np.argmax(d))\n",
    "\n",
    "                at = AnchoredText('$' + '{:.2f}'.format(round(np.median(r_results), 2)) + '_{-' + '{:.2f}'.format(round(np.median(r_results) - np.percentile(r_results, 16), 2)) + '}^{+' + '{:.2f}'.format(round(np.percentile(r_results, 84) - np.median(r_results), 2))  + '}$',\n",
    "                    loc=r_locs[i][j], frameon=False, prop=dict(fontsize='small'))\n",
    "                ax_c[i,j].add_artist(at)\n",
    "\n",
    "    #ax_lya[2,1].set_xlabel('Velocity (km s$^{-1}$)', fontsize='large')\n",
    "    #ax_lya[1,0].set_ylabel(r'Flux density (10$^{-16}$ erg s$^{-1}$ cm$^{-2}$ $\\rm{\\AA}^{-1}$)', fontsize='large')\n",
    "\n",
    "    #for ax in [ax_top, ax_left, ax_right]:\n",
    "    #    ax.set_xlim(-1000,1000)\n",
    "    #    ax.set_ylim(bottom=0)\n",
    "\n",
    "    for i, ax in enumerate(ax_lya_stack):\n",
    "        ax.set_xlim(-1000,1000)\n",
    "        ax.set_ylim(bottom=0)\n",
    "\n",
    "        ax.tick_params(top=True, bottom=True, left=True, right=True, labelleft=True)\n",
    "\n",
    "    ax_lya_stack[2].set_xlabel('Velocity (km s$^{-1}$)')\n",
    "    ax_lya_stack[1].set_ylabel('Flux density (arb. scale)')\n",
    "\n",
    "    #ax_left.tick_params(left=False, labelleft=False)\n",
    "    #ax_right.tick_params(left=False, labelleft=False)\n",
    "\n",
    "    for i, ax_list in enumerate(ax_mc):\n",
    "\n",
    "        for j, subplot in enumerate(ax_list):\n",
    "\n",
    "            subplot.set_aspect(1 / subplot.get_data_ratio(), adjustable='box')\n",
    "\n",
    "    for i, ax_list in enumerate(ax_mc_stack):\n",
    "\n",
    "        for j, subplot in enumerate(ax_list):\n",
    "\n",
    "            subplot.set_aspect(1 / subplot.get_data_ratio(), adjustable='box')\n",
    "\n",
    "    '''\n",
    "    for i in ['top', 'left', 'right']:\n",
    "\n",
    "        ax_lya_stack[i].set_aspect(1 / ax_lya_stack[i].get_data_ratio(), adjustable='box')\n",
    "    '''\n",
    "\n",
    "    for i, ax in enumerate(ax_lya_stack):\n",
    "\n",
    "        #ax.set_aspect(1 / ax.get_data_ratio(), adjustable='box')\n",
    "        ax.tick_params(direction='in')\n",
    "\n",
    "    ax_lya_stack[1].set_yticks([0,3,6,9,12])\n",
    "\n",
    "    #for ax in [ax_top, ax_left, ax_right]:\n",
    "\n",
    "    #    ax.set_aspect(1 / ax.get_data_ratio(), adjustable='box')\n",
    "\n",
    "    for i, ax in enumerate(ax_lya_array):\n",
    "\n",
    "        ax.set_aspect(1 / ax.get_data_ratio(), adjustable='box')\n",
    "        ax.tick_params(top=True, bottom=True, left=True, right=True, labelleft=True, direction='in')\n",
    "\n",
    "    for i in [0,1,2]:\n",
    "        \n",
    "        ax_lya[2,i].set_xticks([-1000,-500,0,500,1000])\n",
    "    \n",
    "    ax_lya[2,1].set_xlabel('Velocity (km s$^{-1}$)', fontsize='large')\n",
    "    ax_lya[1,0].set_ylabel(r'Flux density (10$^{-17}$ erg s$^{-1}$ cm$^{-2}$ $\\rm{\\AA}^{-1}$)', fontsize='large')\n",
    "\n",
    "    #fig_lya_stack.tight_layout()\n",
    "\n",
    "    fig_lya.savefig(figs + '/lya_fits.pdf', bbox_inches='tight')\n",
    "\n",
    "    label(fig_mc, ax_mc, fig_mc_stack, ax_mc_stack)\n",
    "    set_ticks(ax_mc, ax_mc_stack)\n",
    "    disable_plots(ax_mc, ax_mc_stack)\n",
    "\n",
    "    for i, row in enumerate(ax_c):\n",
    "\n",
    "        for j, subplot in enumerate(row):\n",
    "            \n",
    "            if j > i:\n",
    "                subplot.axis('off')\n",
    "            else:\n",
    "                subplot.set_aspect(1 / subplot.get_data_ratio(), adjustable='box')\n",
    "\n",
    "    fig_lya_stack.subplots_adjust(hspace=0, wspace=0)\n",
    "\n",
    "    fig_lya_stack.savefig(figs + '/lya_fits_stack.pdf', bbox_inches='tight')\n",
    "\n",
    "    f = lambda x: x\n",
    "    g = lambda x: x\n",
    "\n",
    "    ax_2 = ax_c[0,0].secondary_yaxis('right', functions=(f,g))\n",
    "    ax_2.set_ylabel('(km s$^{-1}$)', fontsize='large')\n",
    "\n",
    "    fig_c.subplots_adjust(hspace=0.2, wspace=0.05)\n",
    "\n",
    "    fig_c.savefig(figs + '/corner.pdf', bbox_inches='tight')\n",
    "\n",
    "    fig_mc.savefig(figs + '/mc.pdf', bbox_inches='tight')\n",
    "    fig_mc_stack.savefig(figs + '/mc_stack.pdf', bbox_inches='tight')\n",
    "\n",
    "    #make_corner_plot()\n",
    "    \n",
    "    #plt.subplots_adjust(wspace=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edea5d7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16844/2806096226.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmeasure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16844/1970065138.py\u001b[0m in \u001b[0;36mmeasure\u001b[1;34m()\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{results}/lya_fits/{slit_id}/{slit_id}_best_fit_parameters.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mmedian_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfit_results\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[1;31m# Take the transpose of the results so that the rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fit_results' is not defined"
     ]
    }
   ],
   "source": [
    "measure()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b2b404ef7cfffcb2d9e58206576e0220bed399f08fa92bc6fd125b02b641f3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
